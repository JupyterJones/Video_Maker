{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8bb9f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/home/jack/Desktop/HDD500/hub/gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/jack/Desktop/HDD500/hub/gpt2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m new_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jack/Desktop/HDD500/hub/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model from the new path\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(new_model_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2088\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2083\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2085\u001b[0m     )\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2089\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2092\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2093\u001b[0m     )\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/home/jack/Desktop/HDD500/hub/gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/jack/Desktop/HDD500/hub/gpt2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Path to the new location of the model\n",
    "new_model_path = '/home/jack/Desktop/HDD500/hub/gpt2'\n",
    "# Load the tokenizer and model from the new path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(new_model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(new_model_path)\n",
    "# Example usage\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905acf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs  refs  snapshots\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jack/Desktop/HDD500/hub/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbe828d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/home/jack/Desktop/HDD500/hub/gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/jack/Desktop/HDD500/hub/gpt2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m new_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jack/Desktop/HDD500/hub/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model from the new path\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(new_model_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#@app.route('/generate', methods=['POST'])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2088\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2083\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2085\u001b[0m     )\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2089\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2092\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2093\u001b[0m     )\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/home/jack/Desktop/HDD500/hub/gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/jack/Desktop/HDD500/hub/gpt2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#app = Flask(__name__)\n",
    "\n",
    "# Path to the new location of the model\n",
    "new_model_path = '/home/jack/Desktop/HDD500/hub/gpt2'\n",
    "\n",
    "# Load the tokenizer and model from the new path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(new_model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(new_model_path)\n",
    "\n",
    "#@app.route('/generate', methods=['POST'])\n",
    "def generate_text(text):\n",
    "    input_text = request.json.get('input_text')\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return jsonify({'generated_text': generated_text})\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "'''\n",
    "text = 'It is a nice day for a white wedding.'\n",
    "generate_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ff1047",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/jack/Desktop/HDD500/hub/gpt2 does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/jack/Desktop/HDD500/hub/gpt2/tree/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[0;32m----> 2\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jack/Desktop/HDD500/hub/gpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m set_seed(\u001b[38;5;241m41\u001b[39m)\n\u001b[1;32m      4\u001b[0m generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat should we talk about?,\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/pipelines/__init__.py:816\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    814\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 816\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    821\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    926\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    930\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/transformers/utils/hub.py:370\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         )\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/jack/Desktop/HDD500/hub/gpt2 does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/jack/Desktop/HDD500/hub/gpt2/tree/None' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='/home/jack/Desktop/HDD500/hub/gpt2')\n",
    "set_seed(41)\n",
    "generator(\"What should we talk about?,\", max_length=300, truncation=True, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d31a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What should we talk about?, I asked.\\n\\n\"The whole thing…it is really quite complicated,\" she said.\\n\\n\"And what is it?\" I asked.\\n\\n\"The concept of a\\'real\\' world,\" she said.\\n\\n\"Why doesn\\'t it?\" I asked.\\n\\n\"It\\'s a new one,\" she replied.\\n\\n\"The idea behind it is, \\'What when does this come from?\"\\n\\n\"Because when we were having what they called \\'babysitters\\',\" I said.\\n\\n\"Why isn\\'t her coming here?\" he asked.\\n\\n\"Because that was not what I wanted,\" she said.\\n\\n\"You\\'re not saying you didn\\'t want her on here?\" I asked.\\n\\n\"It was a good question because you were talking about this. So you are going to tell me why we should all hang out. I will answer your questions if I feel I should give it a go.\"\\n\\n\\nLudwig nodded. I didn\\'t want to leave, I thought, that she had to worry something about my privacy or I would find out something suspicious.\\n\\nI wasn\\'t coming at all.\\n\\n\"Well,\" she said, a little too quietly. \"I\\'ll make it as close as my heart wants you to get for me.\"\\n\\nI stopped to take a cold sip of water.\\n\\n\"What happens if I\\'m going to get arrested?\" I'},\n",
       " {'generated_text': \"What should we talk about?, which in Spanish and in English form. We'll focus on four languages. The first is the Korean, which is like Korean, but is better. The second is Dutch, which is easier though, but Dutch is really what gets you there. The third is Portuguese, which is easy but is not great. The fourth is Brazilian Portuguese, which is only good, but is worse than Portuguese because you have to pay the price of what was not meant to be. And after that, you have to learn your Spanish.\\n\\nYou'll realize that a language is not written on a map. That's why learning a language is extremely important. The purpose of learning is to get out of a bad situation.\\n\\nWhat makes a good language for teaching? Not everything. The key thing is to learn how to give it purpose, that the purpose of learning comes from that. And that's where all the students do. The key is how to ask questions. Those don't matter.\\n\\nWhat's the main difference between English and Korean?\\n\\nWhen I was a student in Tokyo in 1997. After studying the whole set of topics of history, science and language, I became interested in using that vocabulary called English. I thought I could learn everything about grammar because of that vocabulary.\\n\\nWell it is the same as when I was in Korea, but it is different with Korea now. What we still talk about is English.\\n\\nSo, this\"},\n",
       " {'generated_text': 'What should we talk about?, he asked.\\n\\n\"I\\'m not going to talk about a situation that I actually don\\'t know how to talk about.\"\\n\\nThe former NHL defenseman and current NHL vice president of player services spoke about the subject by phone from Canada.\\n\\n\"I\\'m not involved in anything,\" he said. \"I just play.\"\\n\\nLaine told the CBC News he is aware of the problem with players in general.\\n\\n\"It\\'s very frustrating for the teams to have guys with a bad record, so our top goal is to get ourselves out of the playoffs — we know that. Now we\\'re talking about things that don\\'t change our mindset, things that have nothing to do with you in the long-term.\\n\\n\"That\\'s what we\\'re dealing with here in Edmonton so we have to sort of take that stuff on board.\"\\n\\nLaine was asked if he will speak about the issues with players who play in the playoffs.\\n\\n\"We\\'ll do our best to help them out,\" he said. \"I\\'ll say that when you look at the situation, the people we\\'ve already reached out to who\\'s playing in playoff games at the moment, we\\'re in a position where we\\'re at all the way and really need to help them. Once they win those games and their league gets done getting them out of the situation, things can move in a different way.\"'},\n",
       " {'generated_text': 'What should we talk about?, asks the author. \"Why aren\\'t men in politics? Is it time to change what we do?\"\\n\\nThis article was funded by research funded by the National Centre for Equalities, the National Union of Journalists, the National Centre for Women and Journalism and the UK Center For Inquiry Institute.'},\n",
       " {'generated_text': 'What should we talk about?, we asked one man. \"That\\'s a really good question.\" It was really interesting reading the question because then I could see that there was no real conversation outside the camp itself. The first thing I remember from most of the discussions of our discussion was not the talk about the importance of having a safe-and-smooth environment and meeting people who are genuinely human. Instead, it was about how we need to make sure our tents don\\'t become a hive and that safety is the first priority.\\n\\nWe have learned a lot over the short period of time that people are talking about having a safe camp. We started as people who thought they would be able to sleep within five yards of an encampment, but now, we know that even if you\\'re not in it, there are really huge numbers of different tents with different things in them. When we start a camp and people sleep under the same spot for hours and hours, even in camps with only a single person, they\\'re making mistakes from time to time.\\n\\nThe issue is that the camp is a very public one at this time, because that kind of public discussion can get heated up quickly, and that is really damaging and dangerous to people who come to camp.\\n\\nWe started by saying that camp is not an issue for us, and that the camps in this place are not actually unsafe. We were not talking about what the other camp was and I could see we were not talking'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"What should we talk about?,\", max_length=300, truncation=True, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'generated_text': 'What should we talk about?, I asked.\\n\\n\"The whole thing…it is really quite complicated,\" she said.\\n\\n\"And what is it?\" I asked.\\n\\n\"The concept of a\\'real\\' world,\" she said.\\n\\n\"Why doesn\\'t it?\" I asked.\\n\\n\"It\\'s a new one,\" she replied.\\n\\n\"The idea behind it is, \\'What when does this come from?\"\\n\\n\"Because when we were having what they called \\'babysitters\\',\" I said.\\n\\n\"Why isn\\'t her coming here?\" he asked.\\n\\n\"Because that was not what I wanted,\" she said.\\n\\n\"You\\'re not saying you didn\\'t want her on here?\" I asked.\\n\\n\"It was a good question because you were talking about this. So you are going to tell me why we should all hang out. I will answer your questions if I feel I should give it a go.\"\\n\\n\\nLudwig nodded. I didn\\'t want to leave, I thought, that she had to worry something about my privacy or I would find out something suspicious.\\n\\nI wasn\\'t coming at all.\\n\\n\"Well,\" she said, a little too quietly. \"I\\'ll make it as close as my heart wants you to get for me.\"\\n\\nI stopped to take a cold sip of water.\\n\\n\"What happens if I\\'m going to get arrested?\" I'},\n",
    " {'generated_text': \"What should we talk about?, which in Spanish and in English form. We'll focus on four languages. The first is the Korean, which is like Korean, but is better. The second is Dutch, which is easier though, but Dutch is really what gets you there. The third is Portuguese, which is easy but is not great. The fourth is Brazilian Portuguese, which is only good, but is worse than Portuguese because you have to pay the price of what was not meant to be. And after that, you have to learn your Spanish.\\n\\nYou'll realize that a language is not written on a map. That's why learning a language is extremely important. The purpose of learning is to get out of a bad situation.\\n\\nWhat makes a good language for teaching? Not everything. The key thing is to learn how to give it purpose, that the purpose of learning comes from that. And that's where all the students do. The key is how to ask questions. Those don't matter.\\n\\nWhat's the main difference between English and Korean?\\n\\nWhen I was a student in Tokyo in 1997. After studying the whole set of topics of history, science and language, I became interested in using that vocabulary called English. I thought I could learn everything about grammar because of that vocabulary.\\n\\nWell it is the same as when I was in Korea, but it is different with Korea now. What we still talk about is English.\\n\\nSo, this\"},\n",
    " {'generated_text': 'What should we talk about?, he asked.\\n\\n\"I\\'m not going to talk about a situation that I actually don\\'t know how to talk about.\"\\n\\nThe former NHL defenseman and current NHL vice president of player services spoke about the subject by phone from Canada.\\n\\n\"I\\'m not involved in anything,\" he said. \"I just play.\"\\n\\nLaine told the CBC News he is aware of the problem with players in general.\\n\\n\"It\\'s very frustrating for the teams to have guys with a bad record, so our top goal is to get ourselves out of the playoffs — we know that. Now we\\'re talking about things that don\\'t change our mindset, things that have nothing to do with you in the long-term.\\n\\n\"That\\'s what we\\'re dealing with here in Edmonton so we have to sort of take that stuff on board.\"\\n\\nLaine was asked if he will speak about the issues with players who play in the playoffs.\\n\\n\"We\\'ll do our best to help them out,\" he said. \"I\\'ll say that when you look at the situation, the people we\\'ve already reached out to who\\'s playing in playoff games at the moment, we\\'re in a position where we\\'re at all the way and really need to help them. Once they win those games and their league gets done getting them out of the situation, things can move in a different way.\"'},\n",
    " {'generated_text': 'What should we talk about?, asks the author. \"Why aren\\'t men in politics? Is it time to change what we do?\"\\n\\nThis article was funded by research funded by the National Centre for Equalities, the National Union of Journalists, the National Centre for Women and Journalism and the UK Center For Inquiry Institute.'},\n",
    " {'generated_text': 'What should we talk about?, we asked one man. \"That\\'s a really good question.\" It was really interesting reading the question because then I could see that there was no real conversation outside the camp itself. The first thing I remember from most of the discussions of our discussion was not the talk about the importance of having a safe-and-smooth environment and meeting people who are genuinely human. Instead, it was about how we need to make sure our tents don\\'t become a hive and that safety is the first priority.\\n\\nWe have learned a lot over the short period of time that people are talking about having a safe camp. We started as people who thought they would be able to sleep within five yards of an encampment, but now, we know that even if you\\'re not in it, there are really huge numbers of different tents with different things in them. When we start a camp and people sleep under the same spot for hours and hours, even in camps with only a single person, they\\'re making mistakes from time to time.\\n\\nThe issue is that the camp is a very public one at this time, because that kind of public discussion can get heated up quickly, and that is really damaging and dangerous to people who come to camp.\\n\\nWe started by saying that camp is not an issue for us, and that the camps in this place are not actually unsafe. We were not talking about what the other camp was and I could see we were not talking'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd619c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "\n",
    "[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n",
    " {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n",
    " {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n",
    " {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n",
    " {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n",
    "\n",
    "Here is how to use this model to get the features of a given text in PyTorch:\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "and in TensorFlow:\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "Limitations and bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb48d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT-2\n",
    "\n",
    "Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n",
    "\n",
    "Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in this paper and first released at this page.\n",
    "\n",
    "Disclaimer: The team releasing GPT-2 also wrote a model card for their model. Content from this model card has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n",
    "Model description\n",
    "\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
    "\n",
    "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n",
    "\n",
    "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n",
    "\n",
    "This is the smallest version of GPT-2, with 124M parameters.\n",
    "\n",
    "Related Models: GPT-Large, GPT-Medium and GPT-XL\n",
    "Intended uses & limitations\n",
    "\n",
    "You can use the raw model for text generation or fine-tune it to a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n",
    "How to use\n",
    "\n",
    "You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "\n",
    "[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n",
    " {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n",
    " {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n",
    " {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n",
    " {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n",
    "\n",
    "Here is how to use this model to get the features of a given text in PyTorch:\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "and in TensorFlow:\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "Limitations and bias\n",
    "\n",
    "The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\n",
    "\n",
    "    Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\n",
    "\n",
    "    Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n",
    "\n",
    "Here's an example of how the model can have biased predictions:\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n",
    "\n",
    "[{'generated_text': 'The White man worked as a mannequin for'},\n",
    " {'generated_text': 'The White man worked as a maniser of the'},\n",
    " {'generated_text': 'The White man worked as a bus conductor by day'},\n",
    " {'generated_text': 'The White man worked as a plumber at the'},\n",
    " {'generated_text': 'The White man worked as a journalist. He had'}]\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n",
    "\n",
    "[{'generated_text': 'The Black man worked as a man at a restaurant'},\n",
    " {'generated_text': 'The Black man worked as a car salesman in a'},\n",
    " {'generated_text': 'The Black man worked as a police sergeant at the'},\n",
    " {'generated_text': 'The Black man worked as a man-eating monster'},\n",
    " {'generated_text': 'The Black man worked as a slave, and was'}]\n",
    "\n",
    "This bias will also affect all fine-tuned versions of this model.\n",
    "Training data\n",
    "\n",
    "The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText here.\n",
    "Training procedure\n",
    "Preprocessing\n",
    "\n",
    "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n",
    "\n",
    "The larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact details of training.\n",
    "Evaluation results\n",
    "\n",
    "The model achieves the following results without any fine-tuning (zero-shot):\n",
    "Dataset \tLAMBADA \tLAMBADA \tCBT-CN \tCBT-NE \tWikiText2 \tPTB \tenwiki8 \ttext8 \tWikiText103 \t1BW\n",
    "(metric) \t(PPL) \t(ACC) \t(ACC) \t(ACC) \t(PPL) \t(PPL) \t(BPB) \t(BPC) \t(PPL) \t(PPL)\n",
    "\t35.13 \t45.99 \t87.65 \t83.4 \t29.41 \t65.85 \t1.16 \t1,17 \t37.50 \t75.20\n",
    "BibTeX entry and citation info "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloned-base",
   "language": "python",
   "name": "cloned-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

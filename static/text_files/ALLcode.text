File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled.ipynb
Code Content:
import os
import random
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
import uuid

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the clips
destination_directory = "/home/jack/Desktop/random_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to get a random 1-second clip from a video file
def extract_random_clip(video_path):
    duration = 1  # 1 second
    start_time = random.uniform(0, duration)
    end_time = start_time + duration
    uniq = uuid.uuid4()
    output_path = os.path.join(destination_directory, f"random_clip_{uniq}.mp3")
    
    try:
        ffmpeg_extract_subclip(video_path, start_time, end_time, targetname=output_path)
    except Exception as e:
        print(f"Error extracting clip from {video_path}: {e}")

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp3") or file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            if file.endswith(".mp3"):
                # For .mp3 files, simply copy them to the destination directory
                dest_path = os.path.join(destination_directory, file)
            elif file.endswith(".mp4"):
                # For .mp4 files, extract a random 1-second clip and save it as .mp3
                extract_random_clip(file_path)

print("Random clips extracted and saved to the destination directory.")


import glob
import random
from PIL import Image
def getimage():
    img = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/images/story_nov28/*.jpg"))
    return(img)
print(getimage())

from PIL import Image
IM1 = getimage()
print(IM1)
# Open the base image (the larger transparent PNG)
base_image = Image.open(IM1).convert("RGBA")

bs = base_image.size
print(bs)
# Open the image to paste (the smaller transparent PNG)
IM2 = getimage()
print(IM2)
image_to_paste = Image.open(IM2).convert("RGBA")
image_to_paste = image_to_paste.resize((160,160), Image.BICUBIC)
itp = image_to_paste.size
print(itp)
# Determine the position where you want to paste the smaller image on the larger image
x=10
y=10
paste_position = (x, y)  # Set the coordinates (x, y) where you want to paste the image

# Ensure that the smaller image is not larger than the base image
if image_to_paste.size[0] + paste_position[0] <= base_image.size[0] and image_to_paste.size[1] + paste_position[1] <= base_image.size[1]:
    # Paste the smaller image onto the larger image
    base_image.paste(image_to_paste, paste_position, image_to_paste)

    # Save the resulting image
    base_image.save("resulting_image.png")
else:
    print("Image to paste is too large for the specified position.")
    base_image.save("resulting_image.png")

# Display or save the resulting image
base_image.show()


im = Image.open("resulting_image.png")
im

from PIL import Image, ImageDraw
import random

width, height = 800, 600
img = Image.new('RGBA', (width, height), (255, 255, 255, 0))
draw = ImageDraw.Draw(img)

for _ in range(100):
    x = random.randint(0, width)
    y = random.randint(0, height)
    size = random.randint(2, 5)
    draw.rectangle([x, y, x + size, y + size], fill=(0, 0, 0, 128))

img.show()


from PIL import Image, ImageFilter

width, height = 800, 600
noise_image = Image.new('L', (width, height))

noise = ImageFilter.GaussianBlur(radius=20)
noise_image = noise_image.filter(noise)

noise_image.show()


from PIL import Image
import glob
import random
from PIL import Image
def getimage():
    img = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/images/story_nov28/*.jpg"))
    return(img)
print(getimage())
# Open the original image
original_image = Image.open(getimage())

# Define the zoom parameters (size, top, and left)
zoom_size = (300, 300)  # Size of the zoomed area
zoom_top = 100          # Top coordinate of the zoomed area
zoom_left = 100         # Left coordinate of the zoomed area

# Crop the image to the specified zoomed area
zoomed_image = original_image.crop((zoom_left, zoom_top, zoom_left + zoom_size[0], zoom_top + zoom_size[1]))
zoomed_image



from PIL import Image, ImageSequence

# Create a list to store frames
frames = []

# Append the zoomed image to frames multiple times to create the animation
for _ in range(20):  # Add the zoomed frame 20 times (adjust as needed)
    frames.append(zoomed_image.copy())

# Create a new image with the frames
gif_image = Image.new("RGB", zoomed_image.size)

# Save the frames as a GIF
gif_image.save("zoomed_effect.gif", save_all=True, append_images=frames, duration=100)  # Adjust duration as needed (in milliseconds)


from PIL import Image

# Open the original image
original_image = Image.open(getimage())

# Define the zoom parameters (box coordinates)
left = 100  # Left coordinate of the box
top = 100   # Top coordinate of the box
right = 300 # Right coordinate of the box
bottom = 300 # Bottom coordinate of the box

# Crop the image to the specified box (enlarging and zooming)
zoomed_image = original_image.crop((left, top, right, bottom))
zoomed_image

from PIL import Image, ImageSequence

# Create a list to store frames
frames = []

# Append the zoomed image to frames multiple times to create a GIF
for _ in range(20):  # Add the zoomed frame 20 times (adjust as needed)
    frames.append(zoomed_image)

# Create a new image with the frames
gif_image = Image.new("RGB", zoomed_image.size)

# Save the frames as a GIF
gif_image.save("zoomed_effect.gif", save_all=True, append_images=frames, duration=100)  # Adjust duration as needed (in milliseconds)


from PIL import Image
from PIL import ImageSequence

# Open a GIF image
with Image.open("animated.gif") as img:
    # Check if the image is animated
    is_animated = hasattr(img, "n_frames") and img.n_frames > 1

    if is_animated:
        # If it's an animated GIF, iterate through its frames
        for frame in ImageSequence.Iterator(img):
            # You can process each frame here
            frame.show()
    else:
        # If it's not animated, you can work with it like a regular image
        img.show()


from PIL import Image
from PIL import ImageFilter
from PIL import ImageDraw
from PIL import ImageFont
from PIL import ImageColor
from PIL import ImageOps
from PIL import ImageChops
from PIL import ImageEnhance
from PIL import ImagePalette
from PIL import ImageStat
from PIL import JpegImagePlugin
from PIL import PngImagePlugin
from PIL import BmpImagePlugin
from PIL import TiffImagePlugin
from PIL import GifImagePlugin
from PIL import IcoImagePlugin
from PIL import PpmImagePlugin
from PIL import ImImagePlugin
from PIL import FliImagePlugin
from PIL import SpiderImagePlugin
from PIL import ImageSequence
from PIL import TgaImagePlugin
from PIL import DdsImagePlugin
from PIL import WebPImagePlugin
from PIL import PsdImagePlugin
from PIL import EpsImagePlugin
from PIL import FitsImagePlugin
from PIL import FpxImagePlugin
from PIL import GbrImagePlugin
from PIL import GribImagePlugin
from PIL import Hdf5ImagePlugin
from PIL import IcnsImagePlugin
from PIL import ImtImagePlugin
from PIL import MpoImagePlugin
from PIL import MspImagePlugin
from PIL import PalmImagePlugin
from PIL import PcdImagePlugin
from PIL import PcxImagePlugin
from PIL import Pcx3ImagePlugin
from PIL import PixarImagePlugin
from PIL import PngImagePlugin
from PIL import PdfImagePlugin
from PIL import SgiImagePlugin
from PIL import TiffImagePlugin
from PIL import WmfImagePlugin
from PIL import XbmImagePlugin
from PIL import XpmImagePlugin
.


import os
import random
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
import uuid

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the clips
destination_directory = "/home/jack/Desktop/random_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to get a random 1-second clip from a video file
def extract_random_clip(video_path):
    duration = 1  # 1 second
    start_time = random.uniform(0, duration)
    end_time = start_time + duration
    uniq = uuid.uuid4()  # Corrected the generation of a unique identifier
    output_path = os.path.join(destination_directory, f"random_clip_{uniq}.mp3")
    ffmpeg_extract_subclip(video_path, start_time, end_time, targetname=output_path)

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp3") or file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            if file.endswith(".mp3"):
                # For .mp3 files, simply copy them to the destination directory
                dest_path = os.path.join(destination_directory, file)
                os.system(f"cp {file_path} {dest_path}")
            elif file.endswith(".mp4"):
                # For .mp4 files, extract a random 1-second clip and save it as .mp3
                extract_random_clip(file_path)

print("Random clips extracted and saved to the destination directory.")


!ls /home/jack/Desktop/random_mp3

import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_audioclips

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the audio clips
destination_directory = "/home/jack/Desktop/random_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to extract audio clips from video files
def extract_audio_clips(video_path, num_clips=58, clip_duration=1):
    audio_clips = []
    video_clip = VideoFileClip(video_path)
    duration = video_clip.duration
    
    for _ in range(num_clips):
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        audio_clip = video_clip.subclip(start_time, end_time).audio
        audio_clips.append(audio_clip)
    
    return audio_clips

# Function to copy audio clips to the destination directory
def copy_audio_clips(audio_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clip_paths = []
    
    for idx, clip in enumerate(audio_clips):
        unique_filename = str(uuid.uuid4()) + ".mp3"
        clip_path = os.path.join(target_folder, unique_filename)
        clip.write_audiofile(clip_path, codec='mp3')
        copied_clip_paths.append(clip_path)
    
    return copied_clip_paths

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            audio_clips = extract_audio_clips(file_path)
            copy_audio_clips(audio_clips, destination_directory)

print("Audio clips extracted and saved to the destination directory.")


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_audioclips

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the audio clips
destination_directory = "/home/jack/Desktop/random_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to extract audio clips from video files
def extract_audio_clips(video_path, num_clips=58, clip_duration=1):
    audio_clips = []
    video_clip = VideoFileClip(video_path)
    
    if video_clip.audio is not None:
        duration = video_clip.duration
        for _ in range(num_clips):
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            audio_clip = video_clip.subclip(start_time, end_time).audio
            audio_clips.append(audio_clip)
    
    return audio_clips

# Function to copy audio clips to the destination directory
def copy_audio_clips(audio_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clip_paths = []
    
    for idx, clip in enumerate(audio_clips):
        unique_filename = str(uuid.uuid4()) + ".mp3"
        clip_path = os.path.join(target_folder, unique_filename)
        clip.write_audiofile(clip_path, codec='mp3')
        copied_clip_paths.append(clip_path)
    
    return copied_clip_paths

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            audio_clips = extract_audio_clips(file_path)
            if audio_clips:
                copy_audio_clips(audio_clips, destination_directory)

print("Audio clips extracted and saved to the destination directory.")




import os
import random
import moviepy.editor as mp
import uuid

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the clips
destination_directory = "/home/jack/Desktop/random_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to get a random 1-second clip from a media file and save it as .mp3
def extract_random_audio_clip(media_path):
    duration = 1  # 1 second
    start_time = random.uniform(0, duration)
    end_time = start_time + duration
    uniq = str(uuid.uuid4())[:8]  # Generate a unique identifier (e.g., first 8 characters of UUID)
    output_path = os.path.join(destination_directory, f"random_audio_clip_{uniq}.mp3")
    
    try:
        media_clip = mp.VideoFileClip(media_path) if media_path.endswith(".mp4") else mp.AudioFileClip(media_path)
        audio_clip = media_clip.subclip(start_time, end_time).to_audiofile(output_path)
    except Exception as e:
        print(f"Error extracting clip from {media_path}: {e}")

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        file_path = os.path.join(root, file)
        if file.endswith(('.mp4', '.mp3')):
            extract_random_audio_clip(file_path)

print("Random audio clips extracted and saved to the destination directory.")


import os
import random
from pydub import AudioSegment
import uuid

# Set the source directory containing the random audio clips
source_directory = "/home/jack/Desktop/random_mp3"

# Set the destination directory to save the joined audio files
destination_directory = "/home/jack/Desktop/joined_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to join random audio clips into a single file
def join_random_audio_clips(num_clips=56):
    selected_clips = random.sample(os.listdir(source_directory), num_clips)
    joined_audio = AudioSegment.empty()
    
    for clip_filename in selected_clips:
        clip_path = os.path.join(source_directory, clip_filename)
        audio_clip = AudioSegment.from_mp3(clip_path)
        joined_audio += audio_clip
    
    uniq = str(uuid.uuid4())[:8]  # Generate a unique identifier (e.g., first 8 characters of UUID)
    output_path = os.path.join(destination_directory, f"{uniq}_joined_mp3.mp3")
    joined_audio.export(output_path, format="mp3")

# Loop to join audio clips 10 times
for _ in range(10):
    join_random_audio_clips()

print("Joined audio files created and saved to the destination directory.")




import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_audioclips

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the audio clips
destination_directory = "/home/jack/Desktop/random_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to extract audio clips from video files
def extract_audio_clips(video_path, num_clips=58, clip_duration=1):
    audio_clips = []
    
    try:
        video_clip = VideoFileClip(video_path)
    except Exception as e:
        print(f"Error opening video {video_path}: {e}")
        return audio_clips
    
    if video_clip.audio is not None:
        duration = video_clip.duration
        for _ in range(num_clips):
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            audio_clip = video_clip.subclip(start_time, end_time).audio
            audio_clips.append(audio_clip)
    
    return audio_clips

# Function to copy audio clips to the destination directory
def copy_audio_clips(audio_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clip_paths = []
    
    for idx, clip in enumerate(audio_clips):
        unique_filename = str(uuid.uuid4()) + ".mp3"
        clip_path = os.path.join(target_folder, unique_filename)
        clip.write_audiofile(clip_path, codec='mp3')
        copied_clip_paths.append(clip_path)
    
    return copied_clip_paths

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            audio_clips = extract_audio_clips(file_path)
            if audio_clips:
                copy_audio_clips(audio_clips, destination_directory)

print("Audio clips extracted and saved to the destination directory.")


import os
import random
from pydub import AudioSegment
import uuid

# Set the source directory containing the random audio clips
source_directory = "/home/jack/Desktop/random_mp3"

# Set the destination directory to save the joined audio files
destination_directory = "/home/jack/Desktop/joined_mp3"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to join random audio clips into a single file
def join_random_audio_clips(num_clips=56):
    selected_clips = random.sample(os.listdir(source_directory), num_clips)
    joined_audio = AudioSegment.empty()
    
    for clip_filename in selected_clips:
        clip_path = os.path.join(source_directory, clip_filename)
        audio_clip = AudioSegment.from_mp3(clip_path)
        joined_audio += audio_clip
    
    uniq = str(uuid.uuid4())[:8]  # Generate a unique identifier (e.g., first 8 characters of UUID)
    output_path = os.path.join(destination_directory, f"{uniq}_joined_mp3.mp3")
    joined_audio.export(output_path, format="mp3")

# Loop to join audio clips 10 times
for _ in range(10):
    join_random_audio_clips()

print("Joined audio files created and saved to the destination directory.")


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_audioclips

# Set the source directory containing the .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the 1-second audio clips
destination_directory = "/home/jack/Desktop/mp4_to_mp3"

# Set the destination directory to save the joined audio files
joined_directory = "/home/jack/Desktop/joined_mp3"

# Ensure the destination directories exist
os.makedirs(destination_directory, exist_ok=True)
os.makedirs(joined_directory, exist_ok=True)

# Function to extract 1-second audio clips from video files
def extract_audio_clips(video_path, clip_duration=1):
    audio_clips = []
    
    try:
        video_clip = VideoFileClip(video_path)
    except Exception as e:
        print(f"Error opening video {video_path}: {e}")
        return audio_clips
    
    if video_clip.audio is not None:
        duration = video_clip.duration
        for start_time in range(0, int(duration), clip_duration):
            end_time = start_time + clip_duration
            audio_clip = video_clip.subclip(start_time, end_time).audio
            audio_clips.append(audio_clip)
    
    return audio_clips

# Function to copy audio clips to the destination directory
def copy_audio_clips(audio_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clip_paths = []
    
    for idx, clip in enumerate(audio_clips):
        unique_filename = str(uuid.uuid4()) + ".mp3"
        clip_path = os.path.join(target_folder, unique_filename)
        clip.write_audiofile(clip_path, codec='mp3')
        copied_clip_paths.append(clip_path)
    
    return copied_clip_paths

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            audio_clips = extract_audio_clips(file_path)
            if audio_clips:
                copy_audio_clips(audio_clips, destination_directory)

print("1-second audio clips extracted and saved to the destination directory.")

# Now, you can join the audio clips in the 'destination_directory' if needed.







import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_audioclips

# Set the source directory containing the .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the 1-second audio clips
destination_directory = "/home/jack/Desktop/mp4_to_mp3"

# Set the destination directory to save the joined audio files
joined_directory = "/home/jack/Desktop/joined_mp3"

# Ensure the destination directories exist
os.makedirs(destination_directory, exist_ok=True)
os.makedirs(joined_directory, exist_ok=True)

# Function to extract 1-second audio clips from video files
def extract_audio_clips(video_path, clip_duration=1):
    audio_clips = []
    
    try:
        video_clip = VideoFileClip(video_path)
    except Exception as e:
        print(f"Error opening video {video_path}: {e}")
        return audio_clips
    
    # Check if the video contains audio
    if video_clip.audio is not None:
        duration = video_clip.duration
        for start_time in range(0, int(duration), clip_duration):
            end_time = start_time + clip_duration
            audio_clip = video_clip.subclip(start_time, end_time).audio
            audio_clips.append(audio_clip)
    
    return audio_clips

# Function to copy audio clips to the destination directory
def copy_audio_clips(audio_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clip_paths = []
    
    for idx, clip in enumerate(audio_clips):
        unique_filename = str(uuid.uuid4()) + ".mp3"
        clip_path = os.path.join(target_folder, unique_filename)
        clip.write_audiofile(clip_path, codec='mp3')
        copied_clip_paths.append(clip_path)
    
    return copied_clip_paths

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            audio_clips = extract_audio_clips(file_path)
            if audio_clips:
                copy_audio_clips(audio_clips, destination_directory)

print("1-second audio clips extracted and saved to the destination directory.")


import os
import uuid
from pydub import AudioSegment

# Set the source directory containing the .mp3 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the 1-second audio clips
destination_directory = "/home/jack/Desktop/one_second_mp3_clips"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to extract 1-second audio clips from an audio file
def extract_one_second_clips(audio_file_path):
    audio = AudioSegment.from_file(audio_file_path, format="mp3")
    
    # Calculate the duration of the audio file
    duration_ms = len(audio)
    
    # Initialize the start time to 0
    start_time = 0
    
    while start_time + 1000 <= duration_ms:
        # Extract 1-second audio clip
        one_second_clip = audio[start_time:start_time + 1000]
        
        # Generate a unique filename
        unique_filename = str(uuid.uuid4()) + ".mp3"
        
        # Save the 1-second clip to the destination directory
        one_second_clip.export(os.path.join(destination_directory, unique_filename), format="mp3")
        
        # Move to the next second
        start_time += 1000

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp3"):
            file_path = os.path.join(root, file)
            extract_one_second_clips(file_path)

print("1-second audio clips extracted and saved to the destination directory.")


import os
import uuid
from pydub import AudioSegment

# Set the source directory containing the .mp3 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the 1-second audio clips
destination_directory = "/home/jack/Desktop/one_second_mp3_clips"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to extract 1-second audio clip from an audio file
def extract_one_second_clip(audio_file_path):
    audio = AudioSegment.from_file(audio_file_path, format="mp3")
    
    # Ensure the audio file is at least 1 second long
    if len(audio) >= 1000:
        one_second_clip = audio[:1000]  # Extract the first 1 second
        
        # Generate a unique filename
        unique_filename = str(uuid.uuid4()) + ".mp3"
        
        # Save the 1-second clip to the destination directory
        one_second_clip.export(os.path.join(destination_directory, unique_filename), format="mp3")

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp3"):
            file_path = os.path.join(root, file)
            extract_one_second_clip(file_path)

print("1-second audio clips extracted (one per file) and saved to the destination directory.")


import os
import random
import uuid
from moviepy.editor import VideoFileClip

# Set the source directory containing the .mp4 files
source_directory = "/home/jack/Desktop"

# Set the destination directory to save the 1-second video clips
destination_directory = "/home/jack/Desktop/random_clips"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to extract 1-second video clips from video files with sound
def extract_one_second_clip(video_file_path):
    try:
        video_clip = VideoFileClip(video_file_path)
    except Exception as e:
        print(f"Error opening video {video_file_path}: {e}")
        return

    # Check if the video contains audio
    if video_clip.audio is not None:
        # Get a random 1-second clip within the video duration
        duration = video_clip.duration
        start_time = random.uniform(0, duration - 1)  # Random start time
        end_time = start_time + 1  # 1-second clip

        # Create a subclip and generate a unique filename
        one_second_clip = video_clip.subclip(start_time, end_time)
        unique_filename = str(uuid.uuid4()) + ".mp4"

        # Save the 1-second clip to the destination directory
        one_second_clip.write_videofile(os.path.join(destination_directory, unique_filename), codec="libx264")

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            extract_one_second_clip(file_path)

print("1-second video clips (with sound) extracted and saved to the destination directory.")


#!/home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/env/bin/python3
from flask import Flask, request, render_template, redirect, url_for, flash, send_from_directory
import os
import subprocess
import random
import uuid
import shutil
from werkzeug.utils import secure_filename
import glob
import logging
from logging.handlers import RotatingFileHandler
from PIL import Image
app = Flask(__name__)
app.secret_key = os.urandom(24)

# Create a logger object
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# Create a formatter for the log messages
formatter = logging.Formatter(
    '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]')

# Create a file handler to write log messages to a file
file_handler = RotatingFileHandler(
    'Logs/zoom.log', maxBytes=10000, backupCount=1)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

# Add the file handler to the logger
logger.addHandler(file_handler)

def image_dir_to_zoom():
    image_directories = get_image_directories()
    print("IMAGE_DIRECTORIES: %s",image_directories)
    image_directories = sorted(image_directories)
    #print(image_directories)
    # Display the form again and show an error message
    try:
        selected_directory = request.form.get('selected_directory')
        logger.debug('Selected image directory: %s', selected_directory)

        # List image files in the selected directory
        image_files = glob.glob(os.path.join(selected_directory, '*.jpg'))
        SIZE =Image.open(image_files[0]).size
        print(SIZE)
        print("SIZE[0]:SIZE[1]",SIZE[0],SIZE[1])
        # Modify the extension as needed
        random.shuffle(image_files)
        logger.debug('Image files: %s', image_files)

        if not image_files:
            flash('No image files found in the selected directory.')
            return redirect(request.url)

        # Specify the output video file path
        output_video = os.path.join('static', 'output', 'generated_video.mp4')

        # Frame rate for the output video (adjust as needed)
        frame_rate = 60   # Adjust the frame rate as needed

        # Initialize the FFmpeg command
        ffmpeg_cmd = [
            'ffmpeg',
            '-framerate', str(frame_rate),
            '-i', os.path.join(selected_directory, '%05d.jpg'),  # Modify the format if necessary
        ]
        width = SIZE[0]
        height = SIZE[1]
   

        # Adjust these parameters as needed
        zoom_increment = 0.0005
        zoom_duration = 300  # Increase for a longer zoom duration
        frame_rate = 60  # Increase the frame rate

        ffmpeg_cmd += [
            '-vf', f"scale=8000:-1,zoompan=z='min(zoom+{zoom_increment},1.5)':x='iw/2':y='ih/2-4000':d={zoom_duration}:s={width}x{height},crop={width}:{height}:0:256",
            '-c:v', 'libx264',
            '-pix_fmt', 'yuv420p',
            '-r', str(frame_rate),  # Adjust the frame rate here
            '-s', f'{width}x{height}',
            '-y',  # Overwrite output file if it exists
            output_video,
           ]


        logger.debug('FFmpeg command: %s', ffmpeg_cmd)
        # Run the FFmpeg command to generate the video
        subprocess.run(ffmpeg_cmd)

        logger.debug('Video created: %s', output_video)
        shutil.copy(output_video, 'static/assets')
        # mp4 video name generated with uuid
        video_name = str(uuid.uuid4()) + '_zoomed.mp4'
        shutil.copy('static/assets/generated_video.mp4', os.path.join('static/assets', video_name))

        # Now, render the HTML template and pass the context variables
        output_vid = os.path.join('assets', 'generated_video.mp4')
        return render_template('dir_tozoom.html', image_directories=image_directories, video2=output_vid)

    except Exception as e:
        output_video = os.path.join('output', 'generated_video.mp4')
        logger.exception('Error occurred during image directory to zoomed video conversion: %s', str(e))
    return render_template('dir_tozoom.html', image_directories=image_directories, video2=output_video)

def image_dir_to_video():
    try:
        selected_directory = "/home/jack/Desktop/HDD500/collections/images/toon-alien/"


        # List image files in the selected directory
        image_files = glob.glob(os.path.join(selected_directory, '*.jpg'))  # Modify the extension as needed
        logger.debug('Image files: %s', image_files)

        if not image_files:
            flash('No image files found in the selected directory.')
            return redirect(request.url)

        # Specify the output video file path
        output_video = os.path.join('static', 'output', 'generated_video.mp4')

        # Frame rate for the output video (adjust as needed)
        frame_rate = 1

        # Initialize the FFmpeg command
        ffmpeg_cmd = [
            'ffmpeg',
            '-pattern_type', 'glob',
            '-framerate', str(frame_rate),
            '-i', os.path.join(selected_directory, '*.jpg'),
        ]

        # Output video settings
        ffmpeg_cmd += [
            '-c:v', 'libx264',
            '-pix_fmt', 'yuv420p',
            '-r', '20',
            '-y',  # Overwrite output file if it exists
            output_video,
        ]

        # Run the FFmpeg command to generate the video
        subprocess.run(ffmpeg_cmd)

        logger.debug('Video created: %s', output_video)
        shutil.copy(output_video, 'static/assets')
        # mp4 video name generated with uuid
        video_name = str(uuid.uuid4()) + 'framed.mp4'
        shutil.move('static/assets/generated_video.mp4', 'static/assets/' + video_name)
        print(video_name)
        return video_name
    except Exception as e:
        logger.exception('Error occurred during image directory to video conversion: %s', str(e))
        return "An error occurred during image directory to video conversion."


image_dir_to_video()

!vlc 84864d70-cb1e-4051-8528-8e5900eb5f8dframed.mp4

import glob
import random
from PIL import Image
def image_dir_to_zoom(selected_directory):
    SIZE = Image.open(random.choice(glob.glob(selected_directory+"/*.jpg"))).size
    
    # Specify the output video file path
    output_video = os.path.join('static', 'output', 'notebook_generated_video.mp4')
    # Frame rate for the output video (adjust as needed)
    frame_rate = 60   # Adjust the frame rate as needed
    # Initialize the FFmpeg command
    ffmpeg_cmd = [
       'ffmpeg',
       '-framerate', str(frame_rate),
       '-i', os.path.join(selected_directory, '%05d.jpg'),  # Modify the format if necessary
       ]
    width = SIZE[0]
    height = SIZE[1]
   

    # Adjust these parameters as needed
    zoom_increment = 0.0005
    zoom_duration = 300  # Increase for a longer zoom duration
    frame_rate = 60  # Increase the frame rate
    ffmpeg_cmd += [
            '-vf', f"scale=8000:-1,zoompan=z='min(zoom+{zoom_increment},1.5)':x='iw/2':y='ih/2-4000':d={zoom_duration}:s={width}x{height},crop={width}:{height}:0:256",
            '-c:v', 'libx264',
            '-pix_fmt', 'yuv420p',
            '-r', str(frame_rate),  # Adjust the frame rate here
            '-s', f'{width}x{height}',
            '-y',  # Overwrite output file if it exists
            output_video,
           ]


    logger.debug('FFmpeg command: %s', ffmpeg_cmd)
    # Run the FFmpeg command to generate the video
    subprocess.run(ffmpeg_cmd)
    logger.debug('Video created: %s', output_video)
    shutil.copy(output_video, 'static/assets')
    # mp4 video name generated with uuid
    video_name = str(uuid.uuid4()) + 'notebook_zoomed.mp4'
    shutil.copy('static/assets/generated_video.mp4', os.path.join('static/assets', video_name))
    # Now, render the HTML template and pass the context variables
    output_vid = os.path.join('assets', 'generated_video.mp4')
    print(video_name,":",output_video)
    return output_vid

selected_directory ="/home/jack/Desktop/HDD500/collections/images/a-story-toon"
image_dir_to_zoom(selected_directory)

!vlc static/assets/generated_video.mp4




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Index_Utilities.ipynb
Code Content:



==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled3.ipynb
Code Content:
%%writefile /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/env/lib/python3.8/site-packages/YouTube_key.py
#YouTube_key.py
def YouTube_key():
    key ="6ev6-agr6-kd93-ac8s-5q35"
    return key

from YouTube_key import YouTube_key
print(YouTube_key())

import os
os.command("ls")

from YouTube_key import YouTube_key
import subprocess
key = YouTube_key()
#run this command
ffmpeg -f pulse -ac 2 -i default -f x11grab -framerate 24 -video_size 1366x760 -i :0.0 -c:v libx264 -preset ultrafast -pix_fmt yuv420p -c:a aac -f flv rtmp://a.rtmp.youtube.com/live2/{key}


==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Random_clips-Copy1.ipynb
Code Content:
import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 768), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            print(".",end="-")
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 50)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "newrandom_video_50s.mp4")
        print(output_path)
        final_clip.write_videofile(output_path, codec='libx264', fps=30)


main()


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 666), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 50)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "newrandom_video_50s.mp4")
        print(output_path)
        final_clip.write_videofile(output_path, codec='libx264', fps=30)


def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 666), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')) and "random" not in file:
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        clip_filename = os.path.join(target_folder, f"clip_{idx}.mp4")
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    random.shuffle(video_files)
    selected_clips = random.sample(video_files, 45)
    
    # Load and resize the selected clips
    video_clips = [VideoFileClip(file).resize((512, 666)) for file in selected_clips]
    
    # Concatenate the resized video clips
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Trim audio to match the duration of the final video
    new_sound = new_sound.set_duration(final_clip.duration)
    
    # Overlay the existing sound with the trimmed new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def mainl():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections/")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + "new.mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    music = random.choice(glob.glob(desktop_path+"Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.5
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

for i in range (1,20):
    main()
for i in range (1,20):
    mainl()






import os
import random
import shutil
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 666), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        clip_filename = os.path.join(target_folder, f"clip_{idx}.mp4")
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def mainl():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections/")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_clips/joined/"+str(uuid.uuid4()) +"2random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)
        print(output_path)


    mainl()


!vlc /home/jack/Desktop/HDD500/collections/random_clips/joined/*new.mp4






import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    random.shuffle(video_files)
    selected_clips = random.sample(video_files, 45)
    
    # Load and resize the selected clips
    video_clips = [VideoFileClip(file).resize((512, 666)) for file in selected_clips]
    
    # Concatenate the resized video clips
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Trim audio to match the duration of the final video
    new_sound = new_sound.set_duration(final_clip.duration)
    
    # Overlay the existing sound with the trimmed new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections/")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    music = random.choice(glob.glob(desktop_path+"Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.5
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

main()


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=50, clip_duration=1, target_size=(512, 768), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
            continue  # Continue to the next iteration if there is an error
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


!ls /home/jack/Desktop/HDD500/collections/random_clips/

cnt = 1
video_files =[]
dir_path = "/home/jack/Desktop/HDD500/collections/"
for root, dirs, files in os.walk(dir_path):
     for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                cnt = cnt +1
                if cnt %20 ==0:print(cnt, end=" . ")
                video_files.append(os.path.join(root, file))


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 768), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            print(".",end="-")
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop/FlaskAppArchitect_Flask_App_Creator/")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 180)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import glob
vids = glob.glob("/home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/random_clips/*.mp4")
len(vids)





import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(768, 512), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            print(".",end="-")
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/0Downloads/xvid/vid/vid02/")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 50)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "random_video_50s.mp4")
        print(output_path)
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5, max_duration=120):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    random.shuffle(video_files)
    selected_clips = random.sample(video_files, 45)
    
    # Load and resize the selected clips
    video_clips = [VideoFileClip(file).resize((768, 512)) for file in selected_clips]
    
    # Concatenate the resized video clips
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Trim audio to match the duration of the final video
    if new_sound.duration > final_clip.duration:
        new_sound = new_sound.subclip(0, final_clip.duration)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the trimmed new sound
    final_clip = final_clip.set_audio(new_sound)
    
    # Check if the final video duration exceeds the maximum duration
    if final_clip.duration > max_duration:
        # If it exceeds, extract a subclip of the first 2 minutes
        ffmpeg_extract_subclip(output_filename, 0, max_duration, targetname=output_filename)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("/mnt/HDD500/0Downloads/xvid/vid/vid02/")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    print(random_clips_dir)
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    
    music = random.choice(glob.glob("/mnt/HDD500/collections/Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.5
    max_duration = 58  # 2 minutes

    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume, max_duration)

for i in range (1,10):
    main()


!ls /mnt/HDD500/0Downloads/xvid/vid/vid02/joined/

















import os
import random
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/"

# Set the destination directory to save the clips
destination_directory = "/home/jack/Desktop/HDD500/collections/random_clips/joined/"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to get a random 1-second clip from a video file
def extract_random_clip(video_path):
    duration = 1  # 1 second
    start_time = random.uniform(0, duration)
    end_time = start_time + duration
    output_path = os.path.join(destination_directory, f"random_clip_{random.randint(1, 100)}.mp3")
    ffmpeg_extract_subclip(video_path, start_time, end_time, targetname=output_path)

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp3") or file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            if file.endswith(".mp3"):
                # For .mp3 files, simply copy them to the destination directory
                dest_path = os.path.join(destination_directory, file)
                os.system(f"cp {file_path} {dest_path}")
            elif file.endswith(".mp4"):
                # For .mp4 files, extract a random 1-second clip and save it as .mp3
                extract_random_clip(file_path)

print("Random clips extracted and saved to the destination directory.")


import os
os.getcwd()

!ls /home/jack/Desktop/HDD500/collections/Music



import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()




import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, transition=None)  # <-- Remove threads parameter
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    music = random.choice(glob.glob("/mnt/HDD500/collections/Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()

!vlc /home/jack/Desktop/random_clips/joined/fae2d462-5f25-43c4-9a1c-6c573b0f347e.mp4

import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid
def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, threads=1, logger=None, transition=None, min_duration="if_shortest")  # <-- Remove align parameter
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[12], line 36
     33     join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)
     35 if __name__ == "__main__":
---> 36     main()

Cell In[12], line 33, in main()
     30 # Adjust the overlay volume (default is 0.2, you can change it as needed)
     31 overlay_volume = 0.2
---> 33 join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

Cell In[12], line 10, in join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)
      8 selected_clips = random.sample(video_files, 45)
      9 video_clips = [VideoFileClip(file) for file in selected_clips]
---> 10 final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, threads=1, logger=None, transition=None, min_duration="if_shortest")  # <-- Remove align parameter
     12 # Load the new sound file
     13 new_sound = AudioFileClip(new_sound_file)

TypeError: concatenate_videoclips() got an unexpected keyword argument 'threads'



import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid
def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, align="center", threads=1, logger=None, transition=None, min_duration="if_shortest")  # <-- Add min_duration parameter
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips)  # <-- Change selected_clips to video_clips
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir,"joined" , "JoinedWsound-5.mp4")
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.5, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()




import os
from moviepy.editor import VideoFileClip, concatenate_videoclips
import glob
import random

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 20)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(selected_clips)
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir, "JoinedWsound-5.mp4")
    new_sound_file = os.path.join(desktop_path, "static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.5, you can change it as needed)
    overlay_volume = 0.5
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips
import glob
import random
def join_video_clips():
    desktop_path = os.path.expanduser("~/Desktop")
    video_clips = random.sample(glob.glob(desktop_path+"/random_clips/*.mp4"),20)
    print(video_clips)
    final_clip = concatenate_videoclips(video_clips)
    final_clip.write_videofile(output_filename, codec='libx264', fps=30)

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir, "Joined.mp4")
    
    join_video_clips(random_clips_dir, output_filename)

if __name__ == "__main__":
    join_video_clips()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[8], line 20
     17     join_video_clips(random_clips_dir, output_filename)
     19 if __name__ == "__main__":
---> 20     join_video_clips()

Cell In[8], line 9, in join_video_clips()
      7 video_clips = random.sample(glob.glob(desktop_path+"/random_clips/*.mp4"),20)
      8 print(video_clips)
----> 9 final_clip = concatenate_videoclips(video_clips)
     10 final_clip.write_videofile(output_filename, codec='libx264', fps=30)

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/compositing/concatenate.py:71, in concatenate_videoclips(clips, method, transition, bg_color, ismask, padding)
     68     clips = reduce(lambda x, y: x + y, l) + [clips[-1]]
     69     transition = None
---> 71 tt = np.cumsum([0] + [c.duration for c in clips])
     73 sizes = [v.size for v in clips]
     75 w = max(r[0] for r in sizes)

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/compositing/concatenate.py:71, in <listcomp>(.0)
     68     clips = reduce(lambda x, y: x + y, l) + [clips[-1]]
     69     transition = None
---> 71 tt = np.cumsum([0] + [c.duration for c in clips])
     73 sizes = [v.size for v in clips]
     75 w = max(r[0] for r in sizes)

AttributeError: 'str' object has no attribute 'duration'



import os
from moviepy.editor import VideoFileClip, concatenate_videoclips

def join_video_clips(input_dir, output_filename):
    video_clips = []
    for file in os.listdir(input_dir):
        if file.endswith(".mp4"):
            video_clip = VideoFileClip(os.path.join(input_dir, file))
            video_clips.append(video_clip)
    
    final_clip = concatenate_videoclips(video_clips)
    final_clip.write_videofile(output_filename, codec='libx264', fps=30)

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir, "Joined.mp4")
    
    join_video_clips(random_clips_dir, output_filename)

if __name__ == "__main__":
    main()


!vlc random_clips/Joined.mp4



import os
import random
import shutil
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        clip_filename = os.path.join(target_folder, f"clip_{idx}.mp4")
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()




import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


!cat error_log.txt

!rm /home/jack/Desktop/StoryMaker/static/animate/old/TEMP5.mp4

!vlc /home/jack/Desktop/StoryMaker/static/animate/old/TEMP5.mp4

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    print("VideoFiles: ",video_files)
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")

        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()




import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480)):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video, fps_source="fps", fps=30)  # Set a default frame rate
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[12], line 43
     40         final_clip.write_videofile(output_path, codec='libx264', fps=30)
     42 if __name__ == "__main__":
---> 43     main()

Cell In[12], line 34, in main()
     32 def main():
     33     desktop_path = os.path.expanduser("~/Desktop")
---> 34     random_clips = get_random_video_clips(desktop_path)
     36     if random_clips:
     37         final_clip = concatenate_videoclips(random_clips)

Cell In[12], line 22, in get_random_video_clips(dir_path, num_clips, clip_duration, target_size)
     20 for _ in range(num_clips):
     21     random_video = random.choice(video_files)
---> 22     video_clip = VideoFileClip(random_video, fps_source="fps", fps=30)  # Set a default frame rate
     23     duration = video_clip.duration
     24     start_time = random.uniform(0, duration - clip_duration)

TypeError: __init__() got an unexpected keyword argument 'fps'




import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips
import logging

def setup_logger():
    # Create a logger
    logger = logging.getLogger('error_logger')
    logger.setLevel(logging.ERROR)  # Set the logging level to ERROR or higher
    
    # Create a file handler to write log messages to a file
    log_file = 'error_log.txt'
    file_handler = logging.FileHandler(log_file)
    
    # Define the log message format
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    
    # Add the file handler to the logger
    logger.addHandler(file_handler)
    
    return logger

def my_function():
    # Simulate an error
    try:
        result = 10 / 0
    except ZeroDivisionError as e:
        # Log the error
        logger.error(f"An error occurred: {e}")

# Setup the logger
logger = setup_logger()

# Call the function that might raise an error
my_function()

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480)):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video, fps_source="fps")
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


!rm /home/jack/Desktop/StoryMaker/static/images/EXPERIMENTS/1-4th_down_zoom.mp4

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480)):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1):
    video_files = [f for f in os.listdir(dir_path) if f.endswith(('.mp4', '.avi', '.mkv'))]
    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_path = os.path.join(dir_path, random_video)
        video_clip = VideoFileClip(video_path)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clips.append(random_clip)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1):
    video_files = [f for f in os.listdir(dir_path) if f.endswith(('.mp4', '.avi', '.mkv'))]
    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_path = os.path.join(dir_path, random_video)
        video_clip = VideoFileClip(video_path)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clips.append(random_clip)
    
    return random_clips
def walk_desktop(desktop_path):
    for root, dirs, files in os.walk(desktop_path):
        print(f"Current Directory: {root}")
        print("Subdirectories:")
        for directory in dirs:
            print(f"  {directory}")
        print("Files:")
        for file in files:
            print(f"  {file}")
def main():
    desktop_path = os.path.expanduser("~/Desktop")
    walk_desktop(desktop_path)
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


def walk_desktop(desktop_path):
    for root, dirs, files in os.walk(desktop_path):
        print(f"Current Directory: {root}")
        print("Subdirectories:")
        for directory in dirs:
            print(f"  {directory}")
        print("Files:")
        for file in files:
            print(f"  {file}")

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    walk_desktop(desktop_path)


==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/USE_Blend_Images-Copy1.ipynb
Code Content:
from moviepy.editor import ImageSequenceClip
import os

def create_video_from_images(image_directory, output_file):
    # Get a list of image file names in the directory
    image_files = sorted([f for f in os.listdir(image_directory) if f.endswith(".jpg") or f.endswith(".png")])

    # Create a list of image paths
    image_paths = [os.path.join(image_directory, filename) for filename in image_files]

    # Create a clip from the image sequence 1/24
    clip = ImageSequenceClip(image_paths, durations=[1/10] * len(image_paths))

    # Write the video file
    clip.write_videofile(output_file, fps=24)

# Set the directory path where your images are located
image_directory = "/home/jack/Desktop/HDD500/0WORKSHOP-with-NOTEBOOKS/done/dystopianstreets_files"

# Set the output file path
output_file = "VIDEOS/dystopianstreets1-10.mp4"

# Create the video from the images
create_video_from_images(image_directory, output_file)


!vlc VIDEOS/dystopianstreets1-10.mp4

import random
import glob

DIR = "/home/jack/Desktop/animate/backgrounds/"
imagelist = glob.glob(DIR + "*.jpg")

# Shuffle the image list
random.shuffle(imagelist)

# Print the shuffled image list
print(len(imagelist))


import random
import os
import glob
from moviepy.editor import ImageSequenceClip
from PIL import Image
from time import sleep
import time
import datetime
#imagelist =sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/*.png"))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/quantized/*.jpg'))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/640x640-alien/*.jpg'))
#imagelist =sorted(glob.glob('build/*.jpg'))
DIR = "/home/jack/Desktop/HDD500/FLASK/static/milestones_resources/alien-skull/"
image_list = glob.glob(DIR + "*.jpg")

# Shuffle the image list
random.shuffle(image_list)

# Print the shuffled image list
print(len(image_list))
countdown=len(image_list)
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
imagesize = Image.open(image_list[1]).size
for i in range(0,len(image_list)-1): 
# Take two images for blending them together  
    imag1 = image_list[i]
    imag2 = image_list[i+1]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(0,125):
        inc = ic*.008
        sleep(.1)
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        current_time = datetime.datetime.now()
        filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + '.jpg'
        alphaBlended.save(f'{text}{filename}')
        if ic %25 ==0:print(countdown-i,end = " . ")

#from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
# Get the list of files sorted by creation time
imagelist = sorted(glob.glob('/home/jack/Desktop/animate/*.jpg'), key=os.path.getmtime)

# Create a clip from the images
clip = ImageSequenceClip(imagelist, fps=30)

# Write the clip to a video file using ffmpeg
current_time = datetime.datetime.now()
filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + 'june29.mp4'
clip.write_videofile('/home/jack/Desktop/animate/'+filename, fps=24, codec='libx265', preset='medium')

/home/jack/Desktop/HDD500/deepdream/All-In One.ipynb
/home/jack/Desktop/HDD500/deepdream/DEEP-DREAM-ONLY.ipynb
/home/jack/Desktop/HDD500/deepdream/dream.ipynb

!cp /home/jack/Desktop/HDD500/deepdream/All-In\ One.ipynb .

!pwd

!ls /home/jack/Desktop/HDD500/0WORKSHOP-with-NOTEBOOKS/build/

!pwd

import glob

image_list =sorted(glob.glob('/home/jack/Desktop/HDD500/0WORKSHOP-with-NOTEBOOKS/build/*.jpg'))
len(image_list)

from PIL import Image
im = Image.open(image_list[20])
im.size
im

from PIL import Image
from time import sleep
import time
import datetime
import random
from time import sleep
# Function to change the image size

imagelist = random.sample(image_list, 68)
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
imagesize = Image.open(imagelist[1]).size
for i in range(0,len(image_list)-1): 
# Take two images for blending them together  
    imag1 = imagelist[i]
    imag2 = imagelist[i+1]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(0,125):
        inc = ic*.008
        sleep(.1)
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        current_time = datetime.datetime.now()
        filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + '.jpg'
        alphaBlended.save(f'{text}{filename}')
        if ic %20 ==0:print(ic, end = " . ")

import os
import glob
from PIL import Image
from time import sleep
import time
import datetime
from time import sleep
from moviepy.editor import ImageSequenceClip
#from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
# Get the list of files sorted by creation time
imagelist = sorted(glob.glob('/home/jack/Desktop/animate/*.jpg'), key=os.path.getmtime)

# Create a clip from the images
clip = ImageSequenceClip(imagelist, fps=30)

# Write the clip to a video file using ffmpeg
current_time = datetime.datetime.now()
filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + 'june25.mp4'
clip.write_videofile('/home/jack/Desktop/animate/'+filename, fps=24, codec='libx265', preset='medium')









from PIL import Image
import random
import glob
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
ImOb = [] 
# Take two images for blending them together  
#imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
#imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
DIR = "/home/jack/Desktop/TENSORFLOW/FACE_Swap/source_images/square/*crop.jpg"
image1 = Image.open(random.choice(glob.glob(DIR)))
DIR = "/home/jack/Desktop/TENSORFLOW/FACE_Swap/source_images/square/*crop.jpg"
image2 = Image.open(random.choice(glob.glob(DIR)))
print(image1," : ",image2)
#/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00405up.jpg
#/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00410up.jpg
file1 ="/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00405up.jpg"
file2 ="/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00410up.jpg"

image1 = Image.open(file1)
image1 = Image.open(file2)

# Make the images of uniform size
image3 = changeImageSize(512, 768, image1)
image4 = changeImageSize(512, 768, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(0,500):
    inc = i*.02
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    ImOb.append(alphaBlended)
    output= f'{text}{i:05d}_.png'
    print(output)
    alphaBlended.save(output)

im = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")
im

OPEN_IMAGES = ImOb
#IMAGES = sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/animate/new*_.png"))
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
#opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(ImOb):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image.save(f"temp{i}.png")
#opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images = [Image.open(f"temp{i}.png") for i in range(len(OPEN_IMAGES))]
opened_images[0].save("/home/jack/Desktop/animate/RANDOM105A.gif", save_all=True, append_images=opened_images[1:], duration=200, loop=0)


from PIL import Image
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
 
# Take two images for blending them together  
imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf9.png"
imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
image1 = Image.open(imag1)
image2 = Image.open(imag2)

# Make the images of uniform size
image3 = changeImageSize(512, 512, image1)
image4 = changeImageSize(512, 512, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(0,1000):
    inc = i*.001
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    alphaBlended.save(f'{text}{i:05d}_.png')

find $pwd -name "*.png" -printf '%h\n' | sort | uniq -c | sort -nr | head -n 1

from PIL import Image
import glob
import random
from random import randint
thumb = random.choice(glob.glob("/home/jack/.cache/thumbnails/normal/*.png"))
Thum = Image.open(thumb)
Thum                      

def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1080,1080), (250,250,250))
for i in range(0,2000):
     if i< 500:DIR = "/home/jack/.cache/thumbnails/large/*.png"
     if i> 500:DIR = "/home/jack/.cache/thumbnails/normal/*.png"
     Thum = mkmoz(DIR,im)
     im.paste(Thum,((randint(0,im.size[0])-50),randint(0,im.size[1])))
im              

im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMoz2.png")
im = im.resize((500,500), Image.BICUBIC)
im

def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1080,1080), (250,250,250))
for i in range(0,2500):
     if i< 500:DIR = "/home/jack/.cache/thumbnails/large/*.png"
     if i> 500:DIR = "/home/jack/.cache/thumbnails/normal/*.png"
     Thum = mkmoz(DIR,im)
     im.paste(Thum,((randint(0,im.size[0])-50),randint(0,im.size[1]-50)))
im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMozmix01.png")
im = im.resize((500,500), Image.BICUBIC)
im        

def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1080,1080), (250,250,250))
for i in range(0,2500):
     if i< 500:DIR = "/home/jack/.cache/thumbnails/large/*.png"
     if i> 500:DIR = "/home/jack/.cache/thumbnails/normal/*.png"
     if i> 2495:DIR = "/home/jack/.cache/thumbnails/large/*.png"   
     Thum = mkmoz(DIR,im)
     im.paste(Thum,((randint(0,im.size[0])-50),randint(0,im.size[1]-50)))
im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMozmix05.png")
im = im.resize((500,500), Image.BICUBIC)
im        

#im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMozmix02.png")
im = im.resize((500,500), Image.BICUBIC)
im        

/home/jack/Desktop/TENSORFLOW/clouds/clouds/00015last.png

from PIL import Image
import os
import random
from random import randint 
import glob 
def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1200,1200), (250,250,250))
for x in range(0,500):
    DIR = "/home/jack/Desktop/TENSORFLOW/clouds/clouds/*.png"
    text = "/home/jack/Desktop/TENSORFLOW/animate/"
    Thum = mkmoz(DIR,im)
    SIZE = randint(100,300)
    Thu = Thum.resize((SIZE,SIZE), Image.BICUBIC) 
    im.paste(Thu,((randint(0,im.size[0])),randint(0,im.size[1])),Thu)
    if x % 100 == 0:print(f'{text}{x:05d}gif.png')
    # Calculate the coordinates for the crop
    left = (im.width - 1080) / 2
    top = (im.height - 1080) / 2
    right = (im.width + 1080) / 2
    bottom = (im.height + 1080) / 2
    # Crop the image
    cropped_img = im.crop((left, top, right, bottom))
    # Save the cropped image
    cropped_img.save(f'{text}{x:05d}gif.png')
im              

import random
import glob
from PIL import Image
OPEN_IMAGES = []
IMAGES = glob.glob("/home/jack/Desktop/TENSORFLOW/animate/*gif.png")
#IMAGES = glob.glob("/home/jack/Desktop/TENSORFLOW/mkgif/*.png")
print(len(IMAGES))
for i in range(0,len(IMAGES)):
    im = Image.open(IMAGES[i])
    OPEN_IMAGES.append(im)
images = IMAGES
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(opened_images):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image.save(f"temp{i}.png")
opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/dreamlike-circles2.gif", save_all=True, append_images=opened_images[1:], duration=650, loop=0)
#from IPython.display import HTML
#HTML('<img src="/home/jack/Desktop/TENSORFLOW/mkgif/dreamlike-custom6.gif"/>')

for x in range(0,10):
    text = "/home/jack/Desktop/TENSORFLOW/junk/"
    print(f'{text}{x:05d}gif.png')


print(f'{text}{x:05d}.png')

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import tkinter as tk
from tkinter import *
from PIL import ImageTk, Image
import sys
# Create the root window
root = tk.Tk()

#print(sys.argv[1])
#print(sys.argv[2])
#image1 = ImageTk.PhotoImage(Image.open(sys.argv[1]))
#image2 = ImageTk.PhotoImage(Image.open(sys.argv[2]))
imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
image1 = ImageTk.PhotoImage(Image.open(imag1))
image2 = ImageTk.PhotoImage(Image.open(imag2))


# Create the first image widget and pack it into the root window
img1 = tk.Label(root, image=image1)
img1.pack()

# Set the initial opacity of the first image widget
#img1.config(bg='black')
#img1['image'] = image1
img1.config(bg='black')
img1['image'] = image1

def update_opacity():
    global opacity
    if opacity <= 0:
        root.after_cancel(after_id)
    else:
        img1.config(alpha=opacity)
        opacity -= opacity * 0.1
        after_id = root.after(1000, update_opacity)


"""


# Create a function that will be called repeatedly to update the opacity of the first image
def update_opacity():
    global opacity
    if opacity <= 0:
        # Stop the repeating call to this function
        root.after_cancel(after_id)
    else:
        # use rgba to set the opacity
        img1.config(bg='rgba(0, 0, 0, {})'.format(opacity))
        #img1.config(bg=)  # Convert the opacity value to hexadecimal and set the color
        # Decrease the opacity for the next iteration
        opacity -= int(opacity * 0.1)
        # Schedule this function to be called again after 1 second
        after_id = root.after(1000, update_opacity)

        # Update the opacity of the image widget
        #img1.config(alpha=opacity)
        # Decrease the opacity for the next iteration
        #opacity -= opacity * 0.1
        # Schedule this function to be called again after 1 second
        #after_id = root.after(1000, update_opacity)
"""
# Set the initial value of the opacity
opacity = 1
# Start the repeating function to update the opacity of the first image
update_opacity()

# Create a function that will be called after 1 second to fade in the second image
def fade_in_image2():
    # Create the second image widget and pack it into the root window
    img2 = tk.Label(root, image=image2)
    img2.pack()

    # Set the initial opacity of the second image widget
    img2.config(bg='black')
    img2['image'] = image2

    # Create a function that will be called repeatedly to update the opacity of the second image
    def update_opacity():
        global opacity
        if opacity >= 1:
            # Stop the repeating call to this function
            root.after_cancel(after_id)
        else:
            # Update the opacity of the image widget
            img2.config(alpha=opacity)
            # Increase the opacity for the next iteration
            opacity += opacity * 0.1
            # Schedule this function to be called again after 1 second
            after_id = root.after(1000, update_opacity)

    # Set the initial value of the opacity
    opacity = 0
    # Start the repeating function to update the opacity of the second image
    update_opacity()

# Schedule the function to fade in the second image after 1 second
root.after(1000, fade_in_image2)

# Run the tkinter event loop
root.mainloop()


from PIL import Image

# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]

    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])

    newImage    = image.resize((newWidth, newHeight))
    return newImage
    
# Take two images for blending them together  
imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
image1 = Image.open(imag1)
image2 = Image.open(imag2)

# Make the images of uniform size
image3 = changeImageSize(512, 512, image1)
image4 = changeImageSize(512, 512, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")

# Display the images
#image5.show()
#image6.show()

# alpha-blend the images with varying values of alpha
#alphaBlended1 = Image.blend(image5, image6, alpha=.2)
alphaBlended2 = Image.blend(image5, image6, alpha=.4)

# Display the alpha-blended images
#alphaBlended1.show()
alphaBlended2.show()

!display /home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf9.png

!ls -sr /home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/

for i in range(0,250):
    inc = i*.04
    print(f'{inc}',end = " . ")



IMAGES = sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/animate/new*_.png"))
print(IMAGES)

/home/jack/Desktop/TENSORFLOW/animate/new00999_.png

!ls /home/jack/Desktop/TENSORFLOW/animate/

import random
import glob
from PIL import Image
OPEN_IMAGES = []
IMAGES = sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/animate/*gif.png"))
#IMAGES = glob.glob("/home/jack/Desktop/TENSORFLOW/mkgif/*.png")
print(len(IMAGES))
for i in range(0,len(IMAGES)):
    im = Image.open(IMAGES[i])
    OPEN_IMAGES.append(im)
images = IMAGES
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(opened_images):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image.save(f"temp{i}.png")
opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/testBlend1.gif", save_all=True, append_images=opened_images[1:], duration=1, loop=0)
#from IPython.display import HTML
#HTML('<img src="/home/jack/Desktop/TENSORFLOW/mkgif/dreamlike-custom6.gif"/>')

import random
import glob
from PIL import Image
from PIL import Image
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
ImOb = [] 
# Take two images for blending them together  
#imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
#imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"

image1 = Image.open(random.choice(glob.glob(DIR)))
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"
image2 = Image.open(random.choice(glob.glob(DIR)))
print(image1," : ",image2)
# Make the images of uniform size
image3 = changeImageSize(512, 768, image1)
image4 = changeImageSize(512, 768, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(1,250):
    inc = i*.004
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    alphaBlended = alphaBlended.convert("RGB") 
    ImOb.append(alphaBlended)
    
    alphaBlended.save(f'{text}{i:05d}.jpg')

OPEN_IMAGES = ImOb
#IMAGES = sorted(glob.glob("/home/jack/Desktop/animate/*.jpg"))
IMAGES = sorted(glob.glob("/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"))
images = random.sample(IMAGES,20)
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
#opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(ImOb):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image=opened_image.convert("RGB")
    opened_image.save(f"temp{i}.jpg")
#opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images = [Image.open(f"temp{i}.jpg") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/RANDOM106XX.gif", save_all=True, append_images=opened_images[1:], duration=200, loop=0)


from IPython.display import HTML
HTML('<img src="/home/jack/Desktop/TENSORFLOW/mkgif/memory1.gif"/>')

import os
import hashlib

# Define the directory path
directory = '/home/jack/Desktop/HDD500/collections/quantized'

# Create an empty dictionary to store the file hashes
hashes = {}

# Loop through the directory and compute the hash of each file
for root, dirs, files in os.walk(directory):
    for file in files:
        # Compute the hash of the file
        with open(os.path.join(root, file), 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        # Check if the hash already exists in the dictionary
        if file_hash in hashes:
            # If the hash already exists, delete the duplicate file
            os.remove(os.path.join(root, file))
            print(f"Removed duplicate file: {os.path.join(root, file)}")
        else:
            # If the hash doesn't exist, add it to the dictionary
            hashes[file_hash] = os.path.join(root, file)
            print(f"Added file: {os.path.join(root, file)}")


import os
from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
from moviepy.video.fx.all import unsharp_mask, resize

# Specify input and output paths
input_dir = '/home/jack/Desktop/HDD500/collections/quantized/'
output_path = '/home/jack/Desktop/HDD500/complete-videos/Archived_Quantized.mkv'

# Get a list of all the image files in the input directory
image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]

# Define clip duration and frame rate
duration = len(image_files)
fps = 1

# Create a list of resized and sharpened ImageClips from the image files
image_clips = [resize(unsharp_mask(ImageSequenceClip(f, fps=fps)), height=2*f.size[1]) for f in image_files]

# Concatenate the clips into a single video clip
video_clip = ImageSequenceClip(image_clips, fps=fps)

# Write the final video file
video_clip.write_videofile(output_path, codec='libx264', preset='medium')


!pwd


%%writefile Quantize_video.py
from moviepy.editor import ImageSequenceClip
from skimage import io, filters
import glob
# Load the images
image_files =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/quantized/*.jpg'))
output_path = '/home/jack/Desktop/HDD500/complete-videos/Archived_Quantized.mkv'



images = [io.imread(file) for file in image_files]

# Double the size of the images
images = [filters.resize(image, (image.shape[0]*2, image.shape[1]*2))
          for image in images]

# Sharpen the images
images = [filters.unsharp_mask(image) for image in images]

# Create a video clip from the images
clip = ImageSequenceClip(images, fps=1)

# Write the clip to a file
if __name__ =="__main__":
    clip.write_videofile(output_path)


import os
from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
from moviepy.video.fx.all import resize, sharpen

# Specify input and output paths
input_dir = '/home/jack/Desktop/HDD500/collections/quantized/'
output_path = '/home/jack/Desktop/HDD500/complete-videos/Archived_Quantized.mkv'

# Get a list of all the image files in the input directory
image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]

# Define clip duration and frame rate
duration = len(image_files)
fps = 1

# Create a list of resized and sharpened ImageClips from the image files
image_clips = [resize(unsharp_mask(ImageSequenceClip(f, fps=fps)), height=2*f.size[1]) for f in image_files]

# Concatenate the clips into a single video clip
video_clip = ImageSequenceClip(image_clips, fps=fps)

# Write the final video file
video_clip.write_videofile(output_path, codec='libx264', preset='medium')


from PIL import Image
from time import sleep
import random
# Function to change the image size
#sample_size = 80
#image_list = random.sample(imagelist, sample_size)
imagesize = Image.open(image_list[1]).size
print(imagesize)
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
for i in range(1,len(imagelist)): 
# Take two images for blending them together  
    imag1 = imagelist[i]
    imag2 = imagelist[i]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(1,125):
        inc = ic*.008
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        alphaBlended.save(f'{text}{i}{ic:05d}.jpg')

import os
import shutil
import time
# Create the experiment directory if it doesn't exist
if not os.path.exists('/home/jack/Desktop/animate/experiment'):
    os.makedirs('/home/jack/Desktop/animate/experiment')

# Get a sorted list of all PNG files in the current directory
png_files = sorted([f for f in os.listdir('.') if f.endswith('.jpg')])

# Loop through each PNG file and rename it with a zero-padded index
for i, filename in enumerate(png_files):
    # Get the creation time of the file
    creation_time = os.path.getctime(filename)
    
    # Create the new filename with zero-padded index
    new_filename = f'{i+1:05}.jpg'
    
    # Move the file to the experiment directory with the new filename
    shutil.copy(filename, f'/home/jack/Desktop/animate/experiment/{new_filename}')


!rm /home/jack/Desktop/animate/*.jpg

for ic in range(0,125):
    inc = ic*.008
    print(inc,end="-")   

!pwd

#Create a Gif by blending two images

import random
import glob
from PIL import Image
from PIL import Image
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
ImOb = [] 
# Take two images for blending them together  
#imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
#imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"

image1 = Image.open(random.choice(glob.glob(DIR)))
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"
image2 = Image.open(random.choice(glob.glob(DIR)))
print(image1," : ",image2)
# Make the images of uniform size
image3 = changeImageSize(512, 768, image1)
image4 = changeImageSize(512, 768, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(1,250):
    inc = i*.004
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    alphaBlended = alphaBlended.convert("RGB") 
    ImOb.append(alphaBlended)
    
    alphaBlended.save(f'{text}{i:05d}.jpg')

OPEN_IMAGES = ImOb
#IMAGES = sorted(glob.glob("/home/jack/Desktop/animate/*.jpg"))
IMAGES = sorted(glob.glob("/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"))
images = random.sample(IMAGES,20)
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
#opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(ImOb):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image=opened_image.convert("RGB")
    opened_image.save(f"temp{i}.jpg")
#opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images = [Image.open(f"temp{i}.jpg") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/RANDOM106XX.gif", save_all=True, append_images=opened_images[1:], duration=200, loop=0)


import os
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/output_video2.mp4'
output_images_dir = 'output_images'
fps = 25

# Create output directory for images
os.makedirs(output_images_dir, exist_ok=True)

# Use FFMPEG to extract frames from input video
os.system(f'ffmpeg -i {input_video} -vf fps={fps} {output_images_dir}/input_%04d.jpg')

# Create ImageSequenceClip object from images
clip = ImageSequenceClip(output_images_dir, fps=fps)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)

# Cleanup: delete extracted images
#os.system(f'rm -rf {output_images_dir}')


import os
import random
import glob
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/output_video4.mp4'
output_images_dir = 'output_images'
fps = 25

# Create output directory for images
os.makedirs(output_images_dir, exist_ok=True)

# Use FFMPEG to extract frames from input video
os.system(f'ffmpeg -i {input_video} -vf fps={fps} {output_images_dir}/input_%04d_.jpg')

# Select a random subset of images
image_list = random.sample(glob.glob(f"{output_images_dir}/*_.jpg"), 47)

# Create ImageSequenceClip object from selected images
clip = ImageSequenceClip(image_list, fps=fps)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)

# Cleanup: delete extracted images
#os.system(f'rm -rf {output_images_dir}')


!ls -d */

output_images_dir = 'processed'
image_list = random.sample(glob.glob(f"{output_images_dir}/*.jpg"), 47)


import os
import random
import glob
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
#input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/lexica-warrior_video4.mp4'

#output_images_dir = 'processed'
output_images_dir = 'lexica-warrior'
fps = 25

image_list = random.sample(glob.glob(f"{output_images_dir}/*.jpg"), 40)

# Create ImageSequenceClip object from selected images
clip = ImageSequenceClip(image_list, fps=fps)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)


!ls EXPERIMENT/input_video.mp4

!pwd

import os
import random
import glob
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
#input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/lexica-warrior_one_persec.mp4'

#output_images_dir = 'processed'
output_images_dir = 'lexica-warrior'
fps = 24

image_list = random.sample(glob.glob(f"{output_images_dir}/*.jpg"), 40)

# Create ImageSequenceClip object from selected images
clip = ImageSequenceClip(image_list, fps=1)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)



import os
import glob
from moviepy.editor import ImageSequenceClip
from PIL import Image
from time import sleep
import time
import datetime
#imagelist =sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/*.png"))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/quantized/*.jpg'))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/640x640-alien/*.jpg'))
#imagelist =sorted(glob.glob('build/*.jpg'))
DIR = "/home/jack/Desktop/animate/backgrounds/"
image_list = glob.glob(DIR + "*.jpg")

# Shuffle the image list
random.shuffle(image_list)

# Print the shuffled image list
print(len(image_list))
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
imagesize = Image.open(image_list[1]).size
for i in range(0,len(image_list)-1): 
# Take two images for blending them together  
    imag1 = image_list[i]
    imag2 = image_list[i+1]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(0,125):
        inc = ic*.008
        sleep(.1)
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        current_time = datetime.datetime.now()
        filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + '.jpg'
        alphaBlended.save(f'{text}{filename}')
        if ic %25 ==0:print(i,":",ic, end = " . ")

#from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
# Get the list of files sorted by creation time
imagelist = sorted(glob.glob('/home/jack/Desktop/animate/*.jpg'), key=os.path.getmtime)

# Create a clip from the images
clip = ImageSequenceClip(imagelist, fps=30)

# Write the clip to a video file using ffmpeg
current_time = datetime.datetime.now()
filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + 'june25.mp4'
clip.write_videofile('/home/jack/Desktop/animate/'+filename, fps=24, codec='libx265', preset='medium')

from datetime import datetime
import logging
# Configure logging
logging.basicConfig(filename='logfile.log', level=logging.INFO,
                    format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
def Whatname():
    namelist=["Joe","Ralph","Julia","Norman","Bill", "Mudpie"]
    # Log the time entry
    logging.info(f"Time entry: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    name = input("What is your name?")
    if name in namelist:
        print("Your in the list")
    else:
        print("You are not in the List.")
        logging.info(f"{name} Tried to get in.")
Whatname()            
!cat logfile.log

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'

def make_frame(t):
    with open(text_file_path, 'r') as file:
        lines = file.readlines()
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, lines, fontsize=font_size, color=font_color)
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

video_clip = TextClip(make_frame, duration=duration)
video_clip = video_clip.set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30).set_duration(duration).set_mask(None)

background_clip = ColorClip((width, height), col=background_color).set_duration(duration)
final_clip = CompositeVideoClip([background_clip, video_clip])

final_clip.write_videofile(output_file_path, codec='libx264', fps=30)



from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'

# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color)
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

video_clip = TextClip('', method='custom')
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)


from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'

# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color)
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

video_clip = TextClip('', method='custom')
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)


!ls /home/jack/fonts

from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'
custom_font_path = '/home/jack/fonts/sans.ttf'  # Path to the custom font file


# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color, fontname='CustomFont')
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

# Register the custom font
plt.rcParams['font.family'] = 'CustomFont'
plt.rcParams['font.sans-serif'] = ['CustomFont', 'sans-serif']

video_clip = TextClip('', method='custom', fontfile=custom_font_path)
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/VideoClip.py:1137, in TextClip.__init__(self, txt, filename, size, color, bg_color, fontsize, font, stroke_color, stroke_width, method, kerning, align, interline, tempfilename, temptxt, transparent, remove_temp, print_cmd)
   1136 try:
-> 1137     subprocess_call(cmd, logger=None)
   1138 except (IOError, OSError) as err:

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/tools.py:54, in subprocess_call(cmd, logger, errorprint)
     53         logger(message='Moviepy - Command returned an error')
---> 54     raise IOError(err.decode('utf8'))
     55 else:

OSError: convert: unable to open image `custom:@/tmp/tmpz4kakgwq.txt': No such file or directory @ error/blob.c/OpenBlob/2874.
convert: no images defined `PNG32:/tmp/tmp0b14_sdy.png' @ error/convert.c/ConvertImageCommand/3258.


During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
Cell In[23], line 36
     33 plt.rcParams['font.family'] = 'CustomFont'
     34 plt.rcParams['font.sans-serif'] = ['CustomFont', 'sans-serif']
---> 36 video_clip = TextClip('', method='custom')
     37 video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)
     39 background_clip = ColorClip((width, height), color=background_color)

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/VideoClip.py:1146, in TextClip.__init__(self, txt, filename, size, color, bg_color, fontsize, font, stroke_color, stroke_width, method, kerning, align, interline, tempfilename, temptxt, transparent, remove_temp, print_cmd)
   1138 except (IOError, OSError) as err:
   1139     error = ("MoviePy Error: creation of %s failed because of the "
   1140              "following error:\n\n%s.\n\n." % (filename, str(err))
   1141              + ("This error can be due to the fact that ImageMagick "
   (...)
   1144                 "ImageMagick binary in file conf.py, or that the path "
   1145                 "you specified is incorrect"))
-> 1146     raise IOError(error)
   1148 ImageClip.__init__(self, tempfilename, transparent=transparent)
   1149 self.txt = txt

OSError: MoviePy Error: creation of None failed because of the following error:

convert: unable to open image `custom:@/tmp/tmpz4kakgwq.txt': No such file or directory @ error/blob.c/OpenBlob/2874.
convert: no images defined `PNG32:/tmp/tmp0b14_sdy.png' @ error/convert.c/ConvertImageCommand/3258.
.

.This error can be due to the fact that ImageMagick is not installed on your computer, or (for Windows users) that you didn't specify the path to the ImageMagick binary in file conf.py, or that the path you specified is incorrect


!ls remarkable_journey  

import cv2
import numpy as np

# Create a blank image/frame
frame = np.zeros((height, width, 3), dtype=np.uint8)
frame.fill(background_color)

# Write the frame to a video file
video_writer = cv2.VideoWriter('remarkable_journey/test_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))
video_writer.write(canvas)
video_writer.release()




import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = num_lines * 2  # Adjust the duration as needed
fps = int(num_lines / duration)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    print(line)
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y = height + (line_num + 1) * font_size  # Scroll up by the font size multiplied by line number

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = num_lines * 10  # Adjust the duration as needed
fps = int(num_lines / duration)
fps =5
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
#fourcc = cv2.VideoWriter_fourcc(*'mp4v')
#fourcc = cv2.VideoWriter_fourcc(*'XVID')
#video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

#video_writer = cv2.VideoWriter('remarkable_journey/test_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))
video_writer = cv2.VideoWriter('remarkable_journey/test_video3.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))
#video_writer.write(canvas)
#video_writer.release()

# Generate frames for the scrolling text
for line in text_content:
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    print(line)
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


!ls /home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4

from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'
custom_font_path = '/home/jack/fonts/sans.ttf'  # Path to the custom font file


# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color, fontname='CustomFont')
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

# Register the custom font
plt.rcParams['font.family'] = 'CustomFont'
plt.rcParams['font.sans-serif'] = ['CustomFont', 'sans-serif']
video_clip = TextClip('', method='custom', font='CustomFont=' + custom_font_path)
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)

!ls /home/jack/Desktop/content/static/current_project/

input_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey_FORMATED.txt'

words_per_line = 5

# Read the input file
with open(input_file_path, 'r') as file:
    content = file.read()

# Split the content into words
words = content.split()

# Format the text with 5 words per line
formatted_words = [' '.join(words[i:i+words_per_line]) for i in range(0, len(words), words_per_line)]
formatted_text = '\n'.join(formatted_words)

# Save the formatted text to the output file
with open(output_file_path, 'w') as file:
    file.write(formatted_text)



#duration = num_lines * 10  # Adjust the duration as needed
fps = duration/num_lines * .2
print(fps)

import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = 20  # Centered horizontally
text_y = height + font_size*2  # Start just below the frame

video_writer = cv2.VideoWriter('remarkable_journey/test_video4.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

# Generate frames for the scrolling text
for line in text_content:
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    print(line)
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = 20  # Centered horizontally
text_y = height + font_size*2  # Start just below the frame

video_writer = cv2.VideoWriter('remarkable_journey/test_video4.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

# Generate frames for the scrolling text
for line in text_content:
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    print(line)
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey5.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()

    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y + line_num * font_size), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey6.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()

    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y += font_size  # Scroll down by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journeyX.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height  # Start at the bottom of the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()

    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journeyX.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = 10  # Adjust the duration as needed
fps = num_lines / duration

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height  # Start at the bottom of the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Render the text onto the existing frame (canvas)
    cv2.putText(canvas, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Write the frame to the video
    video_writer.write(canvas)

    # Update the position for the next line
    text_y -= font_size  # Scroll up by the font size

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journeyX1.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines
print(fps)
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height  # Start at the bottom of the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Render the text onto the existing frame (canvas)
    cv2.putText(canvas, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Write the frame to the video
    video_writer.write(canvas)

    # Update the position for the next line
    text_y -= font_size  # Scroll up by the font size

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


!ls /home/jack/fonts

from PIL import Image, ImageDraw, ImageFont

text_file_path = 'remarkable_journey/FORMATED.txt'
output_image_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/image2.png'

canvas_width = 720
canvas_height = 5500
background_color = (0, 0, 0, 0)  # Transparent background
text_color = (255, 255, 255)  # White text color  # Black color
text_position = (15, 15)
#font_path = 'FreeMono.ttf'
font_path = '/home/jack/fonts/Exo-Black.ttf'
font_size = 20

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read()

# Create a new image with white background
canvas = Image.new('RGB', (canvas_width, canvas_height), background_color)

# Load the font
font = ImageFont.truetype(font_path, font_size)

# Create a draw object
draw = ImageDraw.Draw(canvas)

# Draw the text on the canvas
draw.text(text_position, text_content, fill=text_color, font=font)

# Save the image
canvas.save(output_image_path)


!ls remarkable_journey/image2.png

!display remarkable_journey/image2.png





input_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/voice.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/voiceFORMATED.txt'

words_per_line = 4

# Read the input file
with open(input_file_path, 'r') as file:
    content = file.read()

# Split the content into words
words = content.split()

# Format the text with 5 words per line
formatted_words = [' '.join(words[i:i+words_per_line]) for i in range(0, len(words), words_per_line)]
formatted_text = '\n'.join(formatted_words)

# Save the formatted text to the output file
with open(output_file_path, 'w') as file:
    file.write(formatted_text)

from PIL import Image, ImageDraw, ImageFont

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/voiceFORMATED.txt'
output_image_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/image1.png'

canvas_width = 720
canvas_height = 1200
background_color = (0, 0, 0, 0)  # Transparent background
text_color = (255, 255, 255)  # White text color  # Black color
text_position = (15, 15)
#font_path = 'FreeMono.ttf'
font_path = '/home/jack/fonts/Exo-Black.ttf'
font_size = 20

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read()

# Create a new image with white background
canvas = Image.new('RGB', (canvas_width, canvas_height), background_color)

# Load the font
font = ImageFont.truetype(font_path, font_size)

# Create a draw object
draw = ImageDraw.Draw(canvas)

# Draw the text on the canvas
draw.text(text_position, text_content, fill=text_color, font=font)

# Save the image
canvas.save(output_image_path)





==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/WorksWell_Wav2Lip.ipynb
Code Content:
#@title <h1>Step1: Setup Wav2Lip</h1>
#@markdown * Install dependency
#@markdown * Download pretrained model
!rm -rf /content/sample_data
!mkdir /content/sample_data

!git clone https://github.com/zabique/Wav2Lip

#download the pretrained model
!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'
a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl

# !pip uninstall tensorflow tensorflow-gpu
!cd Wav2Lip && pip install -r requirements.txt

#download pretrained model for face detection
!wget "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -O "/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth"

!pip install -q youtube-dl
!pip install ffmpeg-python
!pip install librosa==0.9.1

#this code for recording audio
"""
To write this piece of code I took inspiration/code from a lot of places.
It was late night, so I'm not sure how much I created or just copied o.O
Here are some of the possible references:
https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/
https://stackoverflow.com/a/18650249
https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/
https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/
https://stackoverflow.com/a/49019356
"""
from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

from IPython.display import clear_output
clear_output()
print("\nDone")

!ls Wav2Lip/

#@title 1.Upload input_video.mp4 & input_audio.wav files
%cd sample_data/
from google.colab import files
uploaded = files.upload()
%cd ..

!ls /content/sample_data/

#@title 2.Create Wav2Lip video (using wav2lip_gan.pth) GAN
!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "/content/sample_data/use.mp4" --audio "/content/sample_data/use.mp4"

#@title 3.Play result video -  50% scaling
from IPython.display import HTML
from base64 import b64encode
mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""
<video width="50%" height="50%" controls>
      <source src="{data_url}" type="video/mp4">
</video>""")

#@title 4.Download Result.mp4 to your computer
from google.colab import files
files.download('/content/Wav2Lip/results/result_voice.mp4')


#@title 5. Delete old uploaded samples & result files, so you can start over again.
%rm /content/sample_data/*
%rm /content/Wav2Lip/results/*
from IPython.display import clear_output
clear_output()
print("\nDone! now press X")

#@title 1-4. Batch processing - Upload -> process -> download -> play result
%cd sample_data/
%rm input_audio.wav
%rm input_video.mp4
from google.colab import files
uploaded = files.upload()
%cd ..
!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "/content/sample_data/input_video.mp4" --audio "/content/sample_data/input_audio.wav"
from google.colab import files
files.download('/content/Wav2Lip/results/result_voice.mp4')
from IPython.display import HTML
from base64 import b64encode
mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""
<video width="50%" height="50%" controls>
      <source src="{data_url}" type="video/mp4">
</video>""")

#@title 2.Use resize_factor to reduce the video resolution, as there is a change you might get better results for lower resolution videos. Why? Because the model was trained on low resolution faces.
!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "/content/sample_data/input_video.mp4" --audio "/content/sample_data/input_audio.wav" --resize_factor 2

#@title 3.Use more padding to include the chin region (u can manually edit pads dimensions viewing and changing the code)
!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "/content/sample_data/input_video.mp4" --audio "/content/sample_data/input_audio.wav" --pads 0 20 0 0

#@title 4.Play result video -  50% scaling
from IPython.display import HTML
from base64 import b64encode
mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""
<video width="50%" height="50%" controls>
      <source src="{data_url}" type="video/mp4">
</video>""")

#@title 5.Download Result.mp4 to your computer
from google.colab import files
files.download('/content/Wav2Lip/results/result_voice.mp4')



==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Random_clips.ipynb
Code Content:
https://www.youtube.com/watch?v=hRL4cXXX9Po
       / Hard To Be A God (dir. Aleksei German, 2013) ENG Subs

https://gist.github.com/JupyterJones/7ae1a25124d43d2e77aaff619eb1bfa5

!ls /home/jack/Desktop/HDD500/collections

import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=50, clip_duration=1, target_size=(512, 768), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
            continue  # Continue to the next iteration if there is an error
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 768), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            print(".",end="-")
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 50)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "random_video_50s.mp4")
        print(output_path)
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


!ls /home/jack/Desktop/HDD500/collections/random_clips/

cnt = 1
video_files =[]
dir_path = "/home/jack/Desktop/HDD500/collections/"
for root, dirs, files in os.walk(dir_path):
     for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                cnt = cnt +1
                if cnt %20 ==0:print(cnt, end=" . ")
                video_files.append(os.path.join(root, file))


import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 768), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            print(".",end="-")
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop/FlaskAppArchitect_Flask_App_Creator/")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 180)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import glob
vids = glob.glob("/home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/random_clips/*.mp4")
len(vids)

import os
import random
import shutil
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(512, 666), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        clip_filename = os.path.join(target_folder, f"clip_{idx}.mp4")
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections/")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_clips/joined/"+str(uuid.uuid4()) +"2random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)
        print(output_path)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    random.shuffle(video_files)
    selected_clips = random.sample(video_files, 45)
    
    # Load and resize the selected clips
    video_clips = [VideoFileClip(file).resize((512, 666)) for file in selected_clips]
    
    # Concatenate the resized video clips
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Trim audio to match the duration of the final video
    new_sound = new_sound.set_duration(final_clip.duration)
    
    # Overlay the existing sound with the trimmed new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/collections/")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    music = random.choice(glob.glob(desktop_path+"Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.5
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

main()






import os
import random
import shutil
import uuid
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(768, 512), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            print(".",end="-")
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        unique_filename = str(uuid.uuid4()) + ".mp4"
        clip_filename = os.path.join(target_folder, unique_filename)
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("/home/jack/Desktop/HDD500/0Downloads/xvid/vid/vid02/")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 50)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, str(uuid.uuid4()) + "random_video_50s.mp4")
        print(output_path)
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5, max_duration=120):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    random.shuffle(video_files)
    selected_clips = random.sample(video_files, 45)
    
    # Load and resize the selected clips
    video_clips = [VideoFileClip(file).resize((768, 512)) for file in selected_clips]
    
    # Concatenate the resized video clips
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Trim audio to match the duration of the final video
    if new_sound.duration > final_clip.duration:
        new_sound = new_sound.subclip(0, final_clip.duration)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the trimmed new sound
    final_clip = final_clip.set_audio(new_sound)
    
    # Check if the final video duration exceeds the maximum duration
    if final_clip.duration > max_duration:
        # If it exceeds, extract a subclip of the first 2 minutes
        ffmpeg_extract_subclip(output_filename, 0, max_duration, targetname=output_filename)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("/mnt/HDD500/0Downloads/xvid/vid/vid02/")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    print(random_clips_dir)
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    
    music = random.choice(glob.glob("/mnt/HDD500/collections/Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.5
    max_duration = 58  # 2 minutes

    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume, max_duration)

for i in range (1,10):
    main()


!ls /mnt/HDD500/0Downloads/xvid/vid/vid02/joined/

















import os
import random
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip

# Set the source directory containing the .mp3 and .mp4 files
source_directory = "/home/jack/"

# Set the destination directory to save the clips
destination_directory = "/home/jack/Desktop/HDD500/collections/random_clips/joined/"

# Ensure the destination directory exists
os.makedirs(destination_directory, exist_ok=True)

# Function to get a random 1-second clip from a video file
def extract_random_clip(video_path):
    duration = 1  # 1 second
    start_time = random.uniform(0, duration)
    end_time = start_time + duration
    output_path = os.path.join(destination_directory, f"random_clip_{random.randint(1, 100)}.mp3")
    ffmpeg_extract_subclip(video_path, start_time, end_time, targetname=output_path)

# Iterate through files in the source directory
for root, dirs, files in os.walk(source_directory):
    for file in files:
        if file.endswith(".mp3") or file.endswith(".mp4"):
            file_path = os.path.join(root, file)
            if file.endswith(".mp3"):
                # For .mp3 files, simply copy them to the destination directory
                dest_path = os.path.join(destination_directory, file)
                os.system(f"cp {file_path} {dest_path}")
            elif file.endswith(".mp4"):
                # For .mp4 files, extract a random 1-second clip and save it as .mp3
                extract_random_clip(file_path)

print("Random clips extracted and saved to the destination directory.")


import os
os.getcwd()

!ls /home/jack/Desktop/HDD500/collections/Music



import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="chain")  # Use "chain" instead of "compose"
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()




import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, transition=None)  # <-- Remove threads parameter
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    music = random.choice(glob.glob("/mnt/HDD500/collections/Music/*.mp3"))
    new_sound_file = os.path.join(desktop_path, music)
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()

!vlc /home/jack/Desktop/random_clips/joined/fae2d462-5f25-43c4-9a1c-6c573b0f347e.mp4

import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid
def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, threads=1, logger=None, transition=None, min_duration="if_shortest")  # <-- Remove align parameter
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[12], line 36
     33     join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)
     35 if __name__ == "__main__":
---> 36     main()

Cell In[12], line 33, in main()
     30 # Adjust the overlay volume (default is 0.2, you can change it as needed)
     31 overlay_volume = 0.2
---> 33 join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

Cell In[12], line 10, in join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)
      8 selected_clips = random.sample(video_files, 45)
      9 video_clips = [VideoFileClip(file) for file in selected_clips]
---> 10 final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, threads=1, logger=None, transition=None, min_duration="if_shortest")  # <-- Remove align parameter
     12 # Load the new sound file
     13 new_sound = AudioFileClip(new_sound_file)

TypeError: concatenate_videoclips() got an unexpected keyword argument 'threads'



import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random
import uuid
def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips, method="compose", padding=-1, ismask=False, bg_color=None, align="center", threads=1, logger=None, transition=None, min_duration="if_shortest")  # <-- Add min_duration parameter
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    unique_filename = str(uuid.uuid4()) + ".mp4"
    output_filename = os.path.join(random_clips_dir, "joined", unique_filename)
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.2, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip
import glob
import random

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 45)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(video_clips)  # <-- Change selected_clips to video_clips
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir,"joined" , "JoinedWsound-5.mp4")
    new_sound_file = os.path.join(desktop_path, "StoryMaker/static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.5, you can change it as needed)
    overlay_volume = 0.2
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()




import os
from moviepy.editor import VideoFileClip, concatenate_videoclips
import glob
import random

def join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume=0.5):
    video_files = glob.glob(os.path.join(random_clips_dir, "*.mp4"))
    selected_clips = random.sample(video_files, 20)
    video_clips = [VideoFileClip(file) for file in selected_clips]
    final_clip = concatenate_videoclips(selected_clips)
    
    # Load the new sound file
    new_sound = AudioFileClip(new_sound_file)
    
    # Adjust the volume of the new sound (overlay)
    new_sound = new_sound.volumex(overlay_volume)
    
    # Overlay the existing sound with the new sound
    final_clip = final_clip.set_audio(new_sound)
    
    final_clip.write_videofile(output_filename, codec='libx264', fps=30, audio_codec='aac')

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir, "JoinedWsound-5.mp4")
    new_sound_file = os.path.join(desktop_path, "static/music/Enough-NEFFEX.mp3")
    
    # Adjust the overlay volume (default is 0.5, you can change it as needed)
    overlay_volume = 0.5
    
    join_video_clips(random_clips_dir, output_filename, new_sound_file, overlay_volume)

if __name__ == "__main__":
    main()


import os
from moviepy.editor import VideoFileClip, concatenate_videoclips
import glob
import random
def join_video_clips():
    desktop_path = os.path.expanduser("~/Desktop")
    video_clips = random.sample(glob.glob(desktop_path+"/random_clips/*.mp4"),20)
    print(video_clips)
    final_clip = concatenate_videoclips(video_clips)
    final_clip.write_videofile(output_filename, codec='libx264', fps=30)

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir, "Joined.mp4")
    
    join_video_clips(random_clips_dir, output_filename)

if __name__ == "__main__":
    join_video_clips()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[8], line 20
     17     join_video_clips(random_clips_dir, output_filename)
     19 if __name__ == "__main__":
---> 20     join_video_clips()

Cell In[8], line 9, in join_video_clips()
      7 video_clips = random.sample(glob.glob(desktop_path+"/random_clips/*.mp4"),20)
      8 print(video_clips)
----> 9 final_clip = concatenate_videoclips(video_clips)
     10 final_clip.write_videofile(output_filename, codec='libx264', fps=30)

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/compositing/concatenate.py:71, in concatenate_videoclips(clips, method, transition, bg_color, ismask, padding)
     68     clips = reduce(lambda x, y: x + y, l) + [clips[-1]]
     69     transition = None
---> 71 tt = np.cumsum([0] + [c.duration for c in clips])
     73 sizes = [v.size for v in clips]
     75 w = max(r[0] for r in sizes)

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/compositing/concatenate.py:71, in <listcomp>(.0)
     68     clips = reduce(lambda x, y: x + y, l) + [clips[-1]]
     69     transition = None
---> 71 tt = np.cumsum([0] + [c.duration for c in clips])
     73 sizes = [v.size for v in clips]
     75 w = max(r[0] for r in sizes)

AttributeError: 'str' object has no attribute 'duration'



import os
from moviepy.editor import VideoFileClip, concatenate_videoclips

def join_video_clips(input_dir, output_filename):
    video_clips = []
    for file in os.listdir(input_dir):
        if file.endswith(".mp4"):
            video_clip = VideoFileClip(os.path.join(input_dir, file))
            video_clips.append(video_clip)
    
    final_clip = concatenate_videoclips(video_clips)
    final_clip.write_videofile(output_filename, codec='libx264', fps=30)

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips_dir = os.path.join(desktop_path, "random_clips")
    output_filename = os.path.join(random_clips_dir, "Joined.mp4")
    
    join_video_clips(random_clips_dir, output_filename)

if __name__ == "__main__":
    main()


!vlc random_clips/Joined.mp4



import os
import random
import shutil
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def copy_verified_clips(random_clips, target_folder):
    os.makedirs(target_folder, exist_ok=True)
    copied_clips = []
    for idx, clip in enumerate(random_clips):
        clip_filename = os.path.join(target_folder, f"clip_{idx}.mp4")
        clip.write_videofile(clip_filename, codec='libx264', fps=30)
        copied_clips.append(clip_filename)
    return copied_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        clips_folder = os.path.join(desktop_path, "random_clips")
        copied_clips = copy_verified_clips(random_clips, clips_folder)
        # Manually review the clips in the 'random_clips' folder, remove unwanted ones if needed
        
        # After verifying the clips, you can use the 'copied_clips' list to create the final video
        final_clip = concatenate_videoclips([VideoFileClip(clip) for clip in copied_clips])
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        try:
            video_clip = VideoFileClip(random_video)
            duration = video_clip.duration
            start_time = random.uniform(0, duration - clip_duration)
            end_time = start_time + clip_duration
            random_clip = video_clip.subclip(start_time, end_time)
            random_clip_resized = random_clip.resize(target_size)
            random_clips.append(random_clip_resized)
        except Exception as e:
            print(f"Error processing {random_video}: {e}")
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()




import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


!cat error_log.txt

!rm /home/jack/Desktop/StoryMaker/static/animate/old/TEMP5.mp4

!vlc /home/jack/Desktop/StoryMaker/static/animate/old/TEMP5.mp4

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480), default_fps=30):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    print("VideoFiles: ",video_files)
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")

        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    # Set the default frame rate for all clips
    for clip in random_clips:
        clip.fps = default_fps
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()




import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480)):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video, fps_source="fps", fps=30)  # Set a default frame rate
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[12], line 43
     40         final_clip.write_videofile(output_path, codec='libx264', fps=30)
     42 if __name__ == "__main__":
---> 43     main()

Cell In[12], line 34, in main()
     32 def main():
     33     desktop_path = os.path.expanduser("~/Desktop")
---> 34     random_clips = get_random_video_clips(desktop_path)
     36     if random_clips:
     37         final_clip = concatenate_videoclips(random_clips)

Cell In[12], line 22, in get_random_video_clips(dir_path, num_clips, clip_duration, target_size)
     20 for _ in range(num_clips):
     21     random_video = random.choice(video_files)
---> 22     video_clip = VideoFileClip(random_video, fps_source="fps", fps=30)  # Set a default frame rate
     23     duration = video_clip.duration
     24     start_time = random.uniform(0, duration - clip_duration)

TypeError: __init__() got an unexpected keyword argument 'fps'




import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips
import logging

def setup_logger():
    # Create a logger
    logger = logging.getLogger('error_logger')
    logger.setLevel(logging.ERROR)  # Set the logging level to ERROR or higher
    
    # Create a file handler to write log messages to a file
    log_file = 'error_log.txt'
    file_handler = logging.FileHandler(log_file)
    
    # Define the log message format
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    
    # Add the file handler to the logger
    logger.addHandler(file_handler)
    
    return logger

def my_function():
    # Simulate an error
    try:
        result = 10 / 0
    except ZeroDivisionError as e:
        # Log the error
        logger.error(f"An error occurred: {e}")

# Setup the logger
logger = setup_logger()

# Call the function that might raise an error
my_function()

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480)):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video, fps_source="fps")
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


!rm /home/jack/Desktop/StoryMaker/static/images/EXPERIMENTS/1-4th_down_zoom.mp4

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1, target_size=(640, 480)):
    video_files = []
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mkv')):
                video_files.append(os.path.join(root, file))

    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_clip = VideoFileClip(random_video)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clip_resized = random_clip.resize(target_size)
        random_clips.append(random_clip_resized)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1):
    video_files = [f for f in os.listdir(dir_path) if f.endswith(('.mp4', '.avi', '.mkv'))]
    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_path = os.path.join(dir_path, random_video)
        video_clip = VideoFileClip(video_path)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clips.append(random_clip)
    
    return random_clips

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

def get_random_video_clips(dir_path, num_clips=58, clip_duration=1):
    video_files = [f for f in os.listdir(dir_path) if f.endswith(('.mp4', '.avi', '.mkv'))]
    num_videos = len(video_files)
    
    if num_videos < num_clips:
        print(f"Not enough videos ({num_videos}) in the directory.")
        return None
    
    random_clips = []
    for _ in range(num_clips):
        random_video = random.choice(video_files)
        video_path = os.path.join(dir_path, random_video)
        video_clip = VideoFileClip(video_path)
        duration = video_clip.duration
        start_time = random.uniform(0, duration - clip_duration)
        end_time = start_time + clip_duration
        random_clip = video_clip.subclip(start_time, end_time)
        random_clips.append(random_clip)
    
    return random_clips
def walk_desktop(desktop_path):
    for root, dirs, files in os.walk(desktop_path):
        print(f"Current Directory: {root}")
        print("Subdirectories:")
        for directory in dirs:
            print(f"  {directory}")
        print("Files:")
        for file in files:
            print(f"  {file}")
def main():
    desktop_path = os.path.expanduser("~/Desktop")
    walk_desktop(desktop_path)
    random_clips = get_random_video_clips(desktop_path)
    
    if random_clips:
        final_clip = concatenate_videoclips(random_clips)
        final_clip = final_clip.subclip(0, 58)  # Limit the final video to 58 seconds
        output_path = os.path.join(desktop_path, "random_video_58s.mp4")
        final_clip.write_videofile(output_path, codec='libx264', fps=30)

if __name__ == "__main__":
    main()


def walk_desktop(desktop_path):
    for root, dirs, files in os.walk(desktop_path):
        print(f"Current Directory: {root}")
        print("Subdirectories:")
        for directory in dirs:
            print(f"  {directory}")
        print("Files:")
        for file in files:
            print(f"  {file}")

def main():
    desktop_path = os.path.expanduser("~/Desktop")
    walk_desktop(desktop_path)


==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Imports_moviepy-ffmpeg.ipynb
Code Content:
import numpy as np
import cv2
import glob 
import random
import datetime
from PIL import Image

def motionvid():
    #filename = random.choice(glob.glob("*.jpg"))
    #basedir="/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/"
    basedir="/home/jack/Desktop/StoryMaker/static/goddess/"
    filename = random.choice(glob.glob(basedir+"*.jpg"))
    im = Image.open(filename).convert("RGB")
    im = im.resize((640,640),Image.BICUBIC)
    im.save("TEMPimg.jpg")
    # Read the image
    img = cv2.imread("TEMPimg.jpg")
    

    # Reshape the image to a 2D array of pixels
    pixel_values = img.reshape((-1, 3))
    # Reshape the image to a 2D array of pixels
    #pixel_values = img.reshape((-1, 3))

    # Convert pixel values to float
    pixel_values = np.float32(pixel_values)

    # Define the number of clusters (colors) to use
    k = 26

    # Define criteria and apply kmeans()
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

    # Convert the centers back to uint8 and reshape the labels back to the original image shape
    centers = np.uint8(centers)
    res = centers[labels.flatten()]
    res2 = res.reshape((img.shape))

    # Apply the wave effect to the image
    h, w = img.shape[:2]
    wave_x = 2 * w
    wave_y = h
    amount_x = 10
    amount_y = 5
    num_frames = 300
    delay = 50
    border_color = (0, 0, 0)
    x = np.arange(w, dtype=np.float32)
    y = np.arange(h, dtype=np.float32)
    frames = []
    for i in range(num_frames):
        phase_x = i * 360 / num_frames
        phase_y = phase_x
        x_sin = amount_x * np.sin(2 * np.pi * (x / wave_x + phase_x / 360)) + x
        map_x = np.tile(x_sin, (h, 1))
        y_sin = amount_y * np.sin(2 * np.pi * (y / wave_y + phase_y / 360)) + y
        map_y = np.tile(y_sin, (w, 1)).transpose()
        result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=border_color)
        frames.append(result)

    # Save the frames as an MP4 video file
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    now = datetime.datetime.now()
    #vidname = "newvid/"+now.strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3] + "new.mp4"
    vidname = "newvid/text.mp4"
    print(vidname)
    out = cv2.VideoWriter(vidname, fourcc, 25.0, (w, h))
    for frame in frames:
        out.write(frame)
    out.release()


import numpy as np
import cv2
import glob 
import random
import datetime
from PIL import Image

def motionvid():
    #filename = random.choice(glob.glob("*.jpg"))
    #basedir="/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/"
    #basedir="/home/jack/Desktop/StoryMaker/static/goddess/"
    basedir="/home/jack/Desktop/StoryMaker/static/goddess/ai-generations_files/"
    filename = random.choice(glob.glob(basedir+"*.jpg"))
    im = Image.open(filename).convert("RGB")
    im = im.resize((512,768),Image.BICUBIC)
    im.save("TEMPimg.jpg")
    # Read the image
    img = cv2.imread("TEMPimg.jpg")
    # Reshape the image to a 2D array of pixels
    pixel_values = img.reshape((-1, 3))
    # Convert pixel values to float
    pixel_values = np.float32(pixel_values)
    # Define the number of clusters (colors) to use
    k = 26
    # Define criteria and apply kmeans()
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

    # Convert the centers back to uint8 and reshape the labels back to the original image shape
    centers = np.uint8(centers)
    res = centers[labels.flatten()]
    res2 = res.reshape((img.shape))

    # Apply the wave effect to the image
    h, w = img.shape[:2]
    #wave_x = 2 * w
    wave_x = 3 * w
    wave_y = h
    #amount_x = 10
    #amount_y = 5
    amount_x = 65
    amount_y = 75
    num_frames = 1000
    delay = 1
    border_color = (0, 0, 0)
    x = np.arange(w, dtype=np.float32)
    y = np.arange(h, dtype=np.float32)
    frames = []
    for i in range(num_frames):
        phase_x = i * 360 / num_frames
        phase_y = phase_x
        x_sin = amount_x * np.sin(2 * np.pi * (x / wave_x + phase_x / 360)) + x
        map_x = np.tile(x_sin, (h, 1))
        y_sin = amount_y * np.sin(2 * np.pi * (y / wave_y + phase_y / 360)) + y
        map_y = np.tile(y_sin, (w, 1)).transpose()
        result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=border_color)
        frames.append(result)

    # Save the frames as an MP4 video file
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    now = datetime.datetime.now()
    #vidname = "newvid/"+now.strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3] + "new.mp4"
    vidname = "newvid/text2.mp4"
    print(vidname)
    out = cv2.VideoWriter(vidname, fourcc, 25.0, (w, h))
    for frame in frames:
        out.write(frame)
    out.release()


motionvid()
!vlc newvid/text2.mp4

!pwd

!vlc newvid/text.mp4

# Good working with fade transition
import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=1), 
                               clip2.fx(vfx.fadein, duration=1)], 
                              size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshowT.mp4')


!pwd

from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip

def add_title_image(video_path, title_image_path, output_path):
    # Load the video file and title image
    video_clip = VideoFileClip(video_path)
    title_image = ImageClip(title_image_path)

    # Set the duration of the title image
    title_duration = video_clip.duration
    title_image = title_image.set_duration(title_duration)

    # Position the title image at the center and resize it to fit the video dimensions
    title_image = title_image.set_position(("center", "center"))
    title_image = title_image.resize(video_clip.size)

    # Create a composite video clip with the title image overlay
    composite_clip = CompositeVideoClip([video_clip, title_image])

    # Set the audio of the composite clip to the original video's audio
    composite_clip = composite_clip.set_audio(video_clip.audio)

    # Export the final video with the title image
    composite_clip.write_videofile(output_path)

# Example usage
video_path = "static/alice/Final_End.mp4"
title_image_path = "static/current_project/Images/Title_Image.png"
output_path = "static/alice/Titled_Final_End.mp4"

add_title_image(video_path, title_image_path, output_path)


/home/jack/Downloads/saved_pages/mine-new/0001_Shot.jpg

https://github.com/Zulko/moviepy
https://github.com/fujiawei-dev/ffmpeg-generator 
https://gfycat.com/create
https://man.archlinux.org/man/ffmpeg-filters.1.en
https://trac.ffmpeg.org/wiki/Capture/ALSA

https://www.mltframework.org/
https://ffmpeg.org/ffmpeg-filters.html#concat
https://github.com/mltframework/mlt/releases
https://copyprogramming.com/howto/nlmeans-ffmpeg-is-it-really-so-slow
https://snyk.io/advisor/npm-package/fluent-ffmpeg/functions/fluent-ffmpeg

!cp /home/jack/Downloads/saved_pages/mine-new/Title.png .

!ls *.mp4

import random
TITLE = random.choice(["Title.jpg","Title.png","title.jpg"])
TITLE

import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips, ImageClip, vfx

# Set the duration of the transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 15)

# Load the title image as an ImageClip object and set its duration
TITLE = random.choice(["Title.jpg","Title.png","title.jpg"])
title_image = ImageClip(TITLE).set_duration(2)

# Load each selected file as a VideoFileClip object and add a transition
clips = [title_image]
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition_duration = min(transition_duration, clip.duration)
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),
                                         clip.set_start(transition_duration)])
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("animated-joined_02a.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 6)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition_duration = min(transition_duration, clip.duration)
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),
                                         clip.set_start(transition_duration)])
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("animation_out_A.mp4")


from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "shortsoutput01.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="400" height="400" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


from vidpy import Clip, Composition
from vidpy import Clip, Composition

#clip1 = Clip('animated2.mp4', offset=1.5)# start playing clip one after 1.5 seconds
#clip2 = Clip('animated.mp4')
#clip2.set_offset(5) # start clip2 after 5 seconds

#clip1 = Clip('0030short.mp4')
#clip2 = Clip('0046short.mp4')

clip1 = Clip('0030short.mp4')
clip2 = Clip('0046short.mp4', offset=3)
#clip2.set_offset(65)
# play videos on top of each other
composition = Composition([clip1, clip2])
composition.save('compose01.mp4')

from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "compose01.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="400" height="400" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


!pwd



!vlc newvid/2023-07-15_19-56-27-193new.mp4

for i in range(0,50):
    motionvid()

!ls newvid

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

# Get a list of all the MP4 files in the junk directory
file_list = glob.glob("newvid/*new.mp4")

# Choose 20 random files from the file list
selected_files = random.sample(file_list, 15)

# Create a list of VideoFileClip objects from the selected files
clip_list = [VideoFileClip(filename) for filename in selected_files]

# Concatenate the video clips together into a single clip
final_clip = concatenate_videoclips(clip_list)

# Write the final clip to a file named JOINED.mp4
final_clip.write_videofile("newvid/start.mp4")


!ls /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/junk

basedir="/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/"
filename = random.choice(glob.glob(basedir+"junk/*__.png"))

ffmpeg -i input.lowfps.hevc -filter "minterpolate='fps=120'" output.120fps.hevc


!locate *.mp4 |grep collections

!ffmpeg -i /mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4 \
-filter:v "minterpolate='fps=120'" -y output.120fps.mp4

!ls *.jpg

import subprocess

# Define the command as a list of strings
command = ['ffmpeg', '-y', '-r', '0.3', '-stream_loop', '1', '-i', '2023-04-13-10-15-59.jpg', '-r', '0.3', '-stream_loop', '2', '-i', '2023-04-13-15-21-31.jpg', '-filter_complex', '[0][1]concat=n=2:v=1:a=0[v];[v]minterpolate=fps=24:scd=none,trim=3:7,setpts=PTS-STARTPTS', '-pix_fmt', 'yuv420p', '-y','test02.mp4']

# Run the command using subprocess.Popen
process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# Get the output and error messages (if any)
stdout, stderr = process.communicate()

# Print the output and error messages
print(stdout.decode('utf-8'))
print(stderr.decode('utf-8'))


!ffmpeg -y -r 0.3 -stream_loop 1 -i 2023-04-13-10-15-59.jpg -r 0.3 -stream_loop 2 -i \ 2023-04-13-15-21-31.jpg -filter_complex "[0][1]concat=n=2:v=1:a=0[v]; \[v]minterpolate=fps=24:scd=none,trim=3:7,setpts=PTS-STARTPTS" -pix_fmt yuv420p \ test02.mp4

from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "output.120fps.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="400" height="400" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "output.120fps.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="640" height="640" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


from IPython.display import HTML

# Replace 'myvideo.mp4' with the path to your own video file
video_path = '/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4'

# Embed the video player in an HTML object
video_html = f"""
<video width=500 controls>
    <source src="{video_path}" type="video/mp4">
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


ffmpeg -i input.lowfps.hevc -filter "minterpolate='fps=120'" output.120fps.hevc


ffmpeg -y -fflags +genpts -r 30 -i $input01 -vf "setpts=100*PTS,minterpolate=fps=24:scd=none" -pix_fmt yuv420p "test01.mp4"

We received your request for an API key from us here at Gfycat.  Please find your credentials below:

App name: experimental
Client ID: 2_dVjMKA
Client Secret: zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw

Some cool capabilities of the API include:

     Pulling in trending GIFs and categories of GIFs
     Pulling in GIFs via search from Gfycat
     Allowing you or your users to upload and create GIFs
     Allowing you or your users to upload or pull GIFs from their or other users accounts on Gfycat

!ls *.gif

from gfycat.client import GfycatClient
import requests

client_id = "2_dVjMKA"
client_secret = "zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw"

# Construct the API endpoint URL for testing credentials
test_url = f"https://api.gfycat.com/v1/me?client_id={client_id}&client_secret={client_secret}"

# Send a GET request to the test URL
response = requests.get(test_url)

# Check the response status code
if response.status_code == 200:
    print("Credentials are valid!")
else:
    print("Invalid credentials. Please check your client ID and secret.")


from gfycat.client import GfycatClient

client = GfycatClient("2_dVjMKA","zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw")

# Example request
client.upload_from_file('animation.gif')

---------------------------------------------------------------------------
GfycatClientError                         Traceback (most recent call last)
Cell In[25], line 6
      3 client = GfycatClient("2_dVjMKA","zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw")
      5 # Example request
----> 6 client.upload_from_file('animation.gif')

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/gfycat/client.py:61, in GfycatClient.upload_from_file(self, filename)
     58 r = requests.post(FILE_UPLOAD_ENDPOINT, data=data, files=files)
     60 if r.status_code != 200:
---> 61     raise GfycatClientError('Error uploading the GIF', r.status_code)
     63 info = self.uploaded_file_info(key)
     64 while 'timeout' in info.get('error', '').lower():

GfycatClientError: (403) Error uploading the GIF


!pwd

import numpy as np
import cv2
from PIL import Image
import glob 1  
import random
import datetime


#img = cv2.imread("/home/jack/Desktop/HDD500/collections/newdownloads/512x512/2023-04-13-11-26-06.jpg")
#import cv2
#import numpy as np

# Load the image
filename = random.choice(glob.glob("*.jpg"))
img = cv2.imread(filename)

# Reshape the image to a 2D array of pixels
pixel_values = img.reshape((-1, 3))

# Convert pixel values to float
pixel_values = np.float32(pixel_values)

# Define the number of clusters (colors) to use
k = 26

# Define criteria and apply kmeans()
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# Convert the centers back to uint8 and reshape the labels back to the original image shape
centers = np.uint8(centers)
res = centers[labels.flatten()]
res2 = res.reshape((img.shape))

img = res2
# get dimensions
h, w = img.shape[:2]

# set wavelength
wave_x = 2*w
wave_y = h

# set amount, number of frames and delay
amount_x = 10
amount_y = 5
num_frames = 100
delay = 50
border_color = (128,128,128)

# create X and Y ramps
x = np.arange(w, dtype=np.float32)
y = np.arange(h, dtype=np.float32)

frames = []
# loop and change phase
for i in range(0,num_frames):

    # compute phase to increment over 360 degree for number of frames specified so makes full cycle
    phase_x = i*360/num_frames
    phase_y = phase_x

    # create sinusoids in X and Y, add to ramps and tile out to fill to size of image
    x_sin = amount_x * np.sin(2 * np.pi * (x/wave_x + phase_x/360)) + x
    map_x = np.tile(x_sin, (h,1))

    y_sin = amount_y * np.sin(2 * np.pi * (y/wave_y + phase_y/360)) + y
    map_y = np.tile(y_sin, (w,1)).transpose()

    # do the warping using remap
    result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_CUBIC, borderMode = cv2.BORDER_CONSTANT, borderValue=border_color)
        
    # show result
    #cv2.imshow('result', result)
    #cv2.waitKey(delay)

    # convert to PIL format and save frames
    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    pil_result = Image.fromarray(result)
    frames.append(pil_result)

# write animated gif from frames using PIL
frames[0].save('animation.gif',save_all=True, append_images=frames[1:], optimize=False, duration=delay, loop=0)


!pwd

import numpy as np
import cv2
from PIL import Image

img = cv2.imread("/home/jack/Desktop/HDD500/collections/newdownloads/512x512/2023-04-13-11-27-05.jpg")

# Reshape the image to a 2D array of pixels
pixel_values = img.reshape((-1, 3))

# Convert pixel values to float
pixel_values = np.float32(pixel_values)

# Define the number of clusters (colors) to use
k = 26

# Define criteria and apply kmeans()
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# Convert the centers back to uint8 and reshape the labels back to the original image shape
centers = np.uint8(centers)
res = centers[labels.flatten()]
res2 = res.reshape((img.shape))

img = res2
# get dimensions
h, w = img.shape[:2]

# set wavelength
wave_x = 2*w
wave_y = h

# set amount, number of frames and delay
amount_x = 10
amount_y = 5
num_frames = 100
delay = 50
border_color = (128,128,128)

# create X and Y ramps
x = np.arange(w, dtype=np.float32)
y = np.arange(h, dtype=np.float32)

frames = []
# loop and change phase
for i in range(0,num_frames):

    # compute phase to increment over 360 degree for number of frames specified so makes full cycle
    phase_x = i*360/num_frames
    phase_y = phase_x

    # create sinusoids in X and Y, add to ramps and tile out to fill to size of image
    x_sin = amount_x * np.sin(2 * np.pi * (x/wave_x + phase_x/360)) + x
    map_x = np.tile(x_sin, (h,1))

    y_sin = amount_y * np.sin(2 * np.pi * (y/wave_y + phase_y/360)) + y
    map_y = np.tile(y_sin, (w,1)).transpose()

    # do the warping using remap
    result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_CUBIC, borderMode = cv2.BORDER_CONSTANT, borderValue=border_color)
    # convert to PIL format and save frames
    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    pil_result = Image.fromarray(result)
    frames.append(pil_result)

# write animated gif from frames using PIL
frames[0].save('animation2.gif',save_all=True, append_images=frames[1:], optimize=False, duration=delay, loop=0)


import numpy as np
import cv2
import glob 
import random
import datetime
#img = cv2.imread("/home/jack/Desktop/HDD500/collections/newdownloads/512x512/2023-04-13-11-26-06.jpg")
#import cv2
#import numpy as np

# Load the image
filename = random.choice(glob.glob("*.jpg"))
# Read the image
img = cv2.imread(filename)

# Reshape the image to a 2D array of pixels
pixel_values = img.reshape((-1, 3))

# Convert pixel values to float
pixel_values = np.float32(pixel_values)

# Define the number of clusters (colors) to use
k = 26

# Define criteria and apply kmeans()
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# Convert the centers back to uint8 and reshape the labels back to the original image shape
centers = np.uint8(centers)
res = centers[labels.flatten()]
res2 = res.reshape((img.shape))

# Apply the wave effect to the image
h, w = img.shape[:2]
wave_x = 2 * w
wave_y = h
amount_x = 10
amount_y = 5
num_frames = 100
delay = 50
border_color = (128, 128, 128)
x = np.arange(w, dtype=np.float32)
y = np.arange(h, dtype=np.float32)
frames = []
for i in range(num_frames):
    phase_x = i * 360 / num_frames
    phase_y = phase_x
    x_sin = amount_x * np.sin(2 * np.pi * (x / wave_x + phase_x / 360)) + x
    map_x = np.tile(x_sin, (h, 1))
    y_sin = amount_y * np.sin(2 * np.pi * (y / wave_y + phase_y / 360)) + y
    map_y = np.tile(y_sin, (w, 1)).transpose()
    result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=border_color)
    frames.append(result)

# Save the frames as an MP4 video file
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
now = datetime.datetime.now()
vidname = now.strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3] + ".mp4"
out = cv2.VideoWriter(vidname, fourcc, 25.0, (w, h))
for frame in frames:
    out.write(frame)
out.release()


from vidpy import config
config.MELT_BINARY = '/path/to/melt'

!which melt

from vidpy import Clip, Composition
# start playing clip one after 1.5 seconds
clip1 = Clip('animated2.mp4', offset=1.5)

clip2 = Clip('animated.mp4')
clip2.set_offset(5) # start clip2 after 5 seconds

composition = Composition([clip1, clip2])
composition.save('output2.mp4')

from ffmpeg import _filters
print(_filters.__doc__)

from ffmpeg import _filters
help(_filters)

import ffmpeg
help(ffmpeg)

import ffmpeg
help(ffmpeg)

import ffmpeg
dir(ffmpeg)

import subprocess

import numpy as np

from ffmpeg import constants, FFprobe, input, settings
from tests import data

from ffmpeg import zoompan
help(zoompan)

from ffmpeg import input_file



import ffmpeg

input_file = '/home/jack/Desktop/HDD500/to-vid/building/01145.jpg'
output_file = 'test.mp4'
zoompan_filter = 'zoompan=z=\'min(zoom+0.0015,1.5)\':d=700:x=\'if(gte(zoom,1.5),x,x+1/a)\':y=\'if(gte(zoom,1.5),y,y+1)\':s=640x640'

(
    ffmpeg
    .input(input_file, loop=1)
    .filter(zoompan_filter, duration=10)
    .output(output_file, vcodec='libx264', pix_fmt='yuv420p')
    .run()
)


!pwd

# Works Good 

!ffmpeg -i /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/2023-04-20_00-06-43-983new.mp4 -vf "zoompan=z='min(max(zoom,pzoom)+0.0025,1.5)':d=300:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'" -t 180 -y /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/test3.mp4


!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/test3.mp4 -vf "zoompan=z='min(max(zoom,pzoom)+0.0025,1.5)':d=125:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'" -y /home/jack/Desktop/HDD500/to-vid/building/test2.mp4


!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/test3.mp4 -vf "zoompan=z='if(between(in_time,0,1),2,1)':d=1:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'" -y /home/jack/Desktop/HDD500/to-vid/building/test2.mp4

!mkdir steampunk

# Works Good 
!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/01145.jpg -vf "zoompan=z='min(zoom+0.0015,1.5)':d=60000:x='if(gte(zoom,1.5),x,x+1/a)':y='if(gte(zoom,1.5),y,y+1)':s=640x640" -t 180 -y /home/jack/Desktop/HDD500/to-vid/building/test3.mp4

# Works Good 
!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/01145.jpg -vf "zoompan=z='min(zoom+0.0015,1.5)':d=700:x='if(gte(zoom,1.5),x,x+1/a)':y='if(gte(zoom,1.5),y,y+1)':s=640x640" -y /home/jack/Desktop/HDD500/to-vid/building/test3.mp4

# Works Good 
!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/01145.jpg -vf "zoompan=z='min(zoom+0.0015,1.5)':d=700:x='if(gte(zoom,1.5),x,x+1/a)':y='if(gte(zoom,1.5),y,y+1)':s=640x640" -y /home/jack/Desktop/HDD500/to-vid/building/test3.mp4

import ffmpeg
dir(ffmpeg)
['Error',
 'Stream',
 '__all__',
 '__builtins__',
 '__cached__',
 '__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__',
 '_ffmpeg',
 '_filters',
 '_probe',
 '_run',
 '_utils',
 '_view',
 'colorchannelmixer',
 'compile',
 'concat',
 'crop',
 'dag',
 'drawbox',
 'drawtext',
 'filter',
 'filter_',
 'filter_multi_output',
 'get_args',
 'hflip',
 'hue',
 'input',
 'merge_outputs',
 'nodes',
 'output',
 'overlay',
 'overwrite_output',
 'probe',
 'run',
 'run_async',
 'setpts',
 'trim',
 'unicode_literals',
 'vflip',
 'view',
 'zoompan']

import subprocess

import numpy as np

from ffmpeg import constants, FFprobe, input, settings
from tests import data

settings.CUDA_ENABLE = False


def ffmpeg_input_process(src):
    return input(src).output(constants.PIPE, format="rawvideo",
                             pixel_format="rgb24").run_async(pipe_stdout=True)


def ffmpeg_output_process(dst, width, height):
    return input(constants.PIPE, format="rawvideo", pixel_format="rgb24",
                 width=width, height=height).output(dst, pixel_format="yuv420p"). \
        run_async(pipe_stdin=True)


def read_frame_from_stdout(process: subprocess.Popen, width, height):
    frame_size = width * height * 3
    input_bytes = process.stdout.read(frame_size)

    if not input_bytes:
        return

    assert len(input_bytes) == frame_size

    return np.frombuffer(input_bytes, np.uint8).reshape([height, width, 3])


def process_frame_simple(frame):
    # deep dream
    return frame * 0.3


def write_frame_to_stdin(process: subprocess.Popen, frame):
    process.stdin.write(frame.astype(np.uint8).tobytes())


def run(src, dst, process_frame):
    width, height = FFprobe(src).video_scale

    input_process = ffmpeg_input_process(src)
    output_process = ffmpeg_output_process(dst, width, height)

    while True:
        input_frame = read_frame_from_stdout(input_process, width, height)

        if input_frame is None:
            break

        write_frame_to_stdin(output_process, process_frame(input_frame))

    input_process.wait()

    output_process.stdin.close()
    output_process.wait()


if __name__ == '__main__':
    run("slideshow.mp4", "processed_slideshow.mp4", process_frame)


!ls process_frame.mp4

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 12)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(f) for f in selected_files]

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("12output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the transition in seconds
transition_duration = 1

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 15)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),
                                         clip.set_start(transition_duration)],
                                        duration=transition_duration)
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("15trans_output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the transition in seconds
transition_duration = 1
# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 14)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition_duration = min(transition_duration, clip.duration)
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),clip.set_start(transition_duration)])
        clips.append(transition)
        clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("15output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips, CompositeVideoClip

# Set the duration of the cross-fade transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 8)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        # Determine the duration of the transition
        transition_duration = min(transition_duration, clip.duration, clips[i-1].duration)
        # Create a cross-fade transition
        transition = CompositeVideoClip([clips[i-1].set_end(clips[i-1].duration-transition_duration),
                                         clip.set_start(transition_duration)])
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("blend-shorts-8-output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the cross-fade transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 8)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(file) for file in selected_files]

# Add cross-fade transitions between the clips
transitions = [None] * (len(clips) - 1)
for i in range(len(transitions)):
    transition_out = clips[i].crossfadeout(transition_duration)
    transition_in = clips[i+1].crossfadein(transition_duration)
    transition = CompositeVideoClip([transition_out, transition_in])
    transitions[i] = transition

# Concatenate the clips and transitions into one video
final_clip = concatenate_videoclips([clips[0]] + transitions + clips[1:])

# Write the final video to a file
final_clip.write_videofile("test-output.mp4")


from PIL import Image
import os
from moviepy.editor import ImageClip, concatenate_videoclips

# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"
# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Print the list of filtered images
#print(random.sample(filtered_images,20))

# Sort the filtered images alphabetically
filtered_images =random.sample(filtered_images,30)

# Sort the filtered images alphabetically
print(len(filtered_images))

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(filtered_images)-1):
    clip1 = ImageClip(filtered_images[i], duration=1)
    clip2 = ImageClip(filtered_images[i+1], duration=1)
    cross_fade = clip2.crossfadein(1)
    clips.append(clip1.crossfadeout(1).set_duration(2) )
    clips.append(cross_fade.set_duration(2))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/splatteroutput.mp4", fps=24, audio=False)


from PIL import Image
import os
import random
from moviepy.editor import ImageClip, concatenate_videoclips, CompositeVideoClip

# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"

# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Sort the filtered images alphabetically
filtered_images.sort()

# Randomly select 30 images from the filtered list
selected_images = random.sample(filtered_images, 30)

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(selected_images)-1):
    clip1 = ImageClip(selected_images[i], duration=1)
    clip2 = ImageClip(selected_images[i+1], duration=1)
    cross_fade = CompositeVideoClip([clip1, clip2.set_start(1).crossfadein(1)])
    clips.append(cross_fade.set_duration(2))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/longsplatteroutput2.mp4", fps=24, audio=False)


from PIL import Image
import os
import random
from moviepy.editor import ImageClip, concatenate_videoclips

# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"

# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Sort the filtered images alphabetically
filtered_images.sort()

# Randomly select 30 images from the filtered list
selected_images = random.sample(filtered_images, 30)

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(selected_images)-1):
    clip1 = ImageClip(selected_images[i], duration=1)
    clip2 = ImageClip(selected_images[i+1], duration=1)
    cross_fade = clip2.crossfadein(1)
    clips.append(clip1.crossfadeout(1).set_duration(2) )
    clips.append(cross_fade.set_duration(2))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/longsplatteroutput3.mp4", fps=24, audio=False)


# Good Video Slideshow no Effects
from PIL import Image
import os
import random
from moviepy.editor import ImageClip, concatenate_videoclips

from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips
# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"
# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('Shot.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Print the list of filtered images
#print(random.sample(filtered_images,20))

# Sort the filtered images alphabetically
filtered_images =random.sample(filtered_images,30)

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(filtered_images)-1):
    clip1 = ImageClip(filtered_images[i], duration=1)
    clip2 = ImageClip(filtered_images[i+1], duration=1)
    cross_fade = clip2.crossfadein(1)
    clips.append(clip1.crossfadeout(1).set_duration(1) )
    clips.append(cross_fade.set_duration(1))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/Shot01.mp4", fps=24, audio=False)


import os
import random
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the cross-fade transition in seconds
transition_duration = 1

# Set the duration of the title image in seconds
title_duration = 2

# Load the title image as an ImageClip object
title_image = ImageClip("Title.jpg", duration=title_duration)

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 7)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(file) for file in selected_files]

# Add cross-fade transitions between the clips
transitions = [None] * (len(clips) - 1)
for i in range(len(transitions)):
    transition_out = clips[i].crossfadeout(transition_duration)
    transition_in = clips[i+1].crossfadein(transition_duration)
    transition = CompositeVideoClip([transition_out, transition_in])
    transitions[i] = transition

# Concatenate the title image and the final video
title_video = concatenate_videoclips([title_image])
final_video = concatenate_videoclips([title_video, clips[0]] + transitions + clips[1:])

# Write the final video to a file
final_video.write_videofile("test-output.mp4")


#WORKS GREAT
import os
import random
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the cross-fade transition in seconds
transition_duration = 1

# Set the duration of the title image in seconds
title_duration = 2

# Load the title image as an ImageClip object
title_image = ImageClip('title.jpg', duration=title_duration)

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('hot.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 6)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(file) for file in selected_files]

# Add cross-fade transitions between the clips
transitions = [None] * (len(clips) - 1)
for i in range(len(transitions)):
    transition_out = clips[i].crossfadeout(transition_duration)
    transition_in = clips[i+1].crossfadein(transition_duration)
    transition = CompositeVideoClip([transition_out, transition_in])
    transitions[i] = transition

# Calculate the transition for the last two clips separately
last_transition_out = clips[-2].crossfadeout(transition_duration)
last_transition_in = clips[-1].crossfadein(transition_duration)
last_transition = CompositeVideoClip([last_transition_out, last_transition_in])

# Concatenate the title image, the clips, and the transitions into one video
title_video = concatenate_videoclips([title_image])
final_video = concatenate_videoclips([title_video, clips[0]])
for i in range(len(transitions)):
    final_video = concatenate_videoclips([final_video, transitions[i], clips[i+1]])
final_video = concatenate_videoclips([final_video, last_transition, clips[-1]])

# Write the final video to a file
final_video.write_videofile('Post_to_YouTube.mp4')


from moviepy.editor import *

# Load video
video = VideoFileClip("video.mp4")

# Load sound
sound = AudioFileClip("sound.mp3")

# Set sound duration to match video duration
sound = sound.set_duration(video.duration)

# Fade in sound for 1 second and out for 2 seconds
sound = sound.fadein(1).fadeout(2)

# Combine video and sound
video_with_sound = video.set_audio(sound)

# Write output video with sound
video_with_sound.write_videofile("output.mp4")


!ls ../../Music/Music_for_Creators.mp4

!ls ../../

import random
from moviepy.editor import *
from moviepy.audio.fx.all import *

# Load video 
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Get a random start time for the sound between 15 and 25 minutes
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in sound for 1 second and out for 2 seconds
#sound = sound.fx(afx.fade_in, 1).fx(afx.fade_out, 2)
sound = sound.fx(afx.audio_fadein, 1).fx(afx.audio_fadeout, 2)

# Set the end time of the sound to match the duration of the video
sound = sound.set_end(video.duration)

# Add audio to video
video = video.set_audio(sound)

# Write the output video file
video.write_videofile("animated-with_sound_and_transition.mp4")


import random
from moviepy.editor import *
from moviepy.editor import VideoFileClip, AudioFileClip
# Load video 
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Get a random start time for the sound between 15 and 25 minutes
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in sound for 1 second and out for 2 seconds
sound = sound.fadein(1).fadeout(2)

# Set the end time of the sound to match the duration of the video
sound = sound.set_end(video.duration)

# Add audio to video
video = video.set_audio(sound)

# Write the output video file
video.write_videofile("output.mp4")
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[41], line 17
     14 sound = sound.subclip(start_time)
     16 # Fade in sound for 1 second and out for 2 seconds
---> 17 sound = sound.fadein(1).fadeout(2)
     19 # Set the end time of the sound to match the duration of the video
     20 sound = sound.set_end(video.duration)

AttributeError: 'AudioFileClip' object has no attribute 'fadein'


from moviepy.editor import *
from moviepy.editor import VideoFileClip, AudioFileClip
import random

# Load video and audio files
# Load video
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Get a random start time between 15 and 25 minutes into the sound clip
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in audio for 1 second and out for 2 seconds
sound = sound.audio_fadein(1).audio_fadeout(2)

# Add audio to video
video = video.set_audio(sound)

# Write the output video file
video.write_videofile("output.mp4")




from moviepy.editor import *
import random

# Load video
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Set sound duration to match video duration
sound = sound.set_duration(video.duration)

# Get a random start time between 15 and 25 minutes from the beginning of the sound file
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in sound for 1 second and out for 2 seconds
sound = sound.fadein(1).fadeout(2)

# Combine video and sound
video_with_sound = video.set_audio(sound)

# Write output video with sound
video_with_sound.write_videofile("DEMO_for_youtube.mp4")


# %load /usr/local/bin/SuperEffect
#!/bin/bash

# Print instructions for the user
echo "This script will process a video file located in the current directory."
echo "Please ensure that the 'start.mp4' file exists in this directory before proceeding."
echo "Press ENTER to continue, or CTRL+C to cancel."
read

# Check if start.mp4 exists in the current directory
if [ ! -f "start.mp4" ]; then
  echo "Error: 'start.mp4' file not found in current directory."
  exit 1
fi

# Process the video file
ffmpeg -i "$(pwd)/start.mp4" -crf 10 -vf 'minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1' ---"$(pwd)/TEMPP.mp4"
sleep 1000
ffmpeg -i "$(pwd)/TEMPP.mp4" -vf mpdecimate,setpts=N/FRAME_RATE/TB -map:v 0 -y "$(pwd)/SUPER_EFFECT_Output.mkv"

# Remove the temporary file
rm "$(pwd)/TEMPP.mp4"


melt
Usage: melt [options] [producer [name=value]* ]+
Options:
  -attach filter[:arg] [name=value]*       Attach a filter to the output
  -attach-cut filter[:arg] [name=value]*   Attach a filter to a cut
  -attach-track filter[:arg] [name=value]* Attach a filter to a track
  -attach-clip filter[:arg] [name=value]*  Attach a filter to a producer
  -audio-track | -hide-video               Add an audio-only track
  -blank frames                            Add blank silence to a track
  -consumer id[:arg] [name=value]*         Set the consumer (sink)
  -debug                                   Set the logging level to debug
  -filter filter[:arg] [name=value]*       Add a filter to the current track
  -getc                                    Get keyboard input using getc
  -group [name=value]*                     Apply properties repeatedly
  -help                                    Show this message
  -jack                                    Enable JACK transport synchronization
  -join clips                              Join multiple clips into one cut
  -mix length                              Add a mix between the last two cuts
  -mixer transition                        Add a transition to the mix
  -null-track | -hide-track                Add a hidden track
  -profile name                            Set the processing settings
  -progress                                Display progress along with position
  -query                                   List all of the registered services
  -query "consumers" | "consumer"=id       List consumers or show info about one
  -query "filters" | "filter"=id           List filters or show info about one
  -query "producers" | "producer"=id       List producers or show info about one
  -query "transitions" | "transition"=id   List transitions, show info about one
  -query "profiles" | "profile"=id         List profiles, show info about one
  -query "presets" | "preset"=id           List presets, show info about one
  -query "formats"                         List audio/video formats
  -query "audio_codecs"                    List audio codecs
  -query "video_codecs"                    List video codecs
  -remove                                  Remove the most recent cut
  -repeat times                            Repeat the last cut
  -repository path                         Set the directory of MLT modules
  -serialise [filename]                    Write the commands to a text file
  -silent                                  Do not display position/transport
  -split relative-frame                    Split the last cut into two cuts
  -swap                                    Rearrange the last two cuts
  -track                                   Add a track
  -transition id[:arg] [name=value]*       Add a transition
  -verbose                                 Set the logging level to verbose
  -timings                                 Set the logging level to timings
  -version                                 Show the version and copyright
  -video-track | -hide-audio               Add a video-only track
For more help: <https://www.mltframework.org/>


# %load /usr/local/bin/SuperEffect
#!/bin/bash

# Print instructions for the user
echo "This script will process a video file located in the current directory."
echo "Please ensure that the 'start.mp4' file exists in this directory before proceeding."
echo "Press ENTER to continue, or CTRL+C to cancel."
read

# Check if start.mp4 exists in the current directory
if [ ! -f "start.mp4" ]; then
  echo "Error: 'start.mp4' file not found in current directory."
  exit 1
fi

# Process the video file
ffmpeg -i "$(pwd)/start.mp4" -crf 10 -vf \ 'minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1' -t 25 \ "$(pwd)/TEMPP.mp4"
sleep 1000
ffmpeg -i "$(pwd)/TEMPP.mp4" -vf mpdecimate,setpts=N/FRAME_RATE/TB -map:v 0 -y "$(pwd)/SUPER_EFFECT_Output.mkv"

# Remove the temporary file
rm "$(pwd)/TEMPP.mp4"


ffmpeg \
-loop 1 -t 3 -i img001.jpg \
-loop 1 -t 3 -i img002.jpg \
-loop 1 -t 3 -i img003.jpg \
-loop 1 -t 3 -i img004.jpg \
-loop 1 -t 3 -i img005.jpg \
-filter_complex \
"[0][1]xfade=transition=circlecrop:duration=0.5:offset=2.5[f0]; \
[f0][2]xfade=transition=smoothleft:duration=0.5:offset=5[f1]; \
[f1][3]xfade=transition=pixelize:duration=0.5:offset=7.5[f2]; \
[f2][4]xfade=transition=hblur:duration=0.5:offset=10[f3]" \
-map "[f3]" -r 25 -pix_fmt yuv420p -vcodec libx264 output-swipe-custom.mp4

import os
import random
from moviepy.editor import *
#from moviepy.video import transitions
from moviepy.video.compositing import transitions
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip

def zoom_inn(clip, duration):
    """Zooms in on a clip over a given duration"""
    zoomed_clips = []
    zoom_factor = 1.5 # Increase this value to zoom in more
    for i in range(2):
        zoomed_clips.append(clip.zoom(zoom_factor ** i))
    transition = CompositeVideoClip([zoomed_clips[0], zoomed_clips[1].set_start(duration)], size=clip.size)
    return transition.set_duration(duration)

def zoom_in(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_in = clip.zoom(zoom_factor)
    zoomed_in = zoomed_in.set_position(('center', 'center'))
    zoomed_in = zoomed_in.set_duration(duration)
    return zoomed_in.margin(-w/2, -h/2, -w/2, -h/2)


def zoom_out(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_out = clip.zoom(zoom_factor)
    zoomed_out = zoomed_out.set_position(('center', 'center'))
    zoomed_out = zoomed_out.set_duration(duration)
    return zoomed_out


def get_random_transition():
    transition_list = [
        transitions.crossfadein,
        transitions.crossfadeout,
        transitions.slide_in(random.choice(["left", "right", "top", "bottom"])),
        lambda: transitions.slide_out(random.choice(["left", "right", "top", "bottom"]))
  
    ]
    return random.choice(transition_list)


# Set the path to the directory containing the input images
image_dir = '/home/jack/Desktop/HDD500/to-vid/building'

# Set the output video parameters
duration = 1 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])

# Define a function to choose a random transition

# Define a function to create a video clip from an image with a random transition
def create_clip(filename):
    transition = get_random_transition()
    image_clip = ImageClip(filename).set_duration(duration)
    return image_clip.fx(transition, duration=duration)

# Create a list of video clips for each image with a random transition
clips = [create_clip(filename) for filename in image_files]

# Concatenate the video clips to create the final slideshow
slideshow = concatenate_videoclips(clips)

# Set the output video parameters and write the video file
slideshow = slideshow.resize(size).set_fps(fps)
slideshow.write_videofile(image_dir+'/slideshow.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.compositing import transitions
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip

def zoom_inn(clip, duration):
    """Zooms in on a clip over a given duration"""
    zoomed_clips = []
    zoom_factor = 1.5 # Increase this value to zoom in more
    for i in range(2):
        zoomed_clips.append(clip.zoom(zoom_factor ** i))
    transition = CompositeVideoClip([zoomed_clips[0], zoomed_clips[1].set_start(duration)], size=clip.size)
    return transition.set_duration(duration)

def zoom_in(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_in = clip.zoom(zoom_factor)
    zoomed_in = zoomed_in.set_position(('center', 'center'))
    zoomed_in = zoomed_in.set_duration(duration)
    return zoomed_in.margin(-w/2, -h/2, -w/2, -h/2)


def zoom_out(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_out = clip.zoom(zoom_factor)
    zoomed_out = zoomed_out.set_position(('center', 'center'))
    zoomed_out = zoomed_out.set_duration(duration)
    return zoomed_out


def slide_out_random_direction():
    return transitions.slide_out(random.choice(["left", "right", "top", "bottom"]))

def get_random_transition():
    transition_list = [
        transitions.crossfadein,
        transitions.crossfadeout,
        lambda: transitions.slide_in(random.choice(["left", "right", "top", "bottom"]), duration=duration),
        lambda: transitions.slide_out(random.choice(["left", "right", "top", "bottom"]), duration=duration)
    ]
    return random.choice(transition_list)



# Set the path to the directory containing the input images
image_dir = '/home/jack/Desktop/HDD500/to-vid/building'

# Set the output video parameters
duration = 1 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])

# Define a function to create a video clip from an image with a random transition
def create_clip(filename):
    transition = get_random_transition()
    image_clip = ImageClip(filename).set_duration(duration)
    return image_clip.fl_image(lambda img: transition(img, duration=duration))



# Create a list of video clips for each image with a random transition
clips = [create_clip(filename) for filename in image_files]

# Concatenate the video clips to create the final slideshow
slideshow = concatenate_videoclips(clips)

# Set the output video parameters and write the video file
slideshow = slideshow.resize(size).set_fps(fps)
slideshow.write_videofile(os.path.join(image_dir, 'slideshow.mp4'))

import os
import random
from moviepy.editor import *
from moviepy.video.compositing import transitions
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip

def zoom_inn(clip, duration):
    """Zooms in on a clip over a given duration"""
    zoomed_clips = []
    zoom_factor = 1.5 # Increase this value to zoom in more
    for i in range(2):
        zoomed_clips.append(clip.zoom(zoom_factor ** i))
    transition = CompositeVideoClip([zoomed_clips[0], zoomed_clips[1].set_start(duration)], size=clip.size)
    return transition.set_duration(duration)

def zoom_in(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_in = clip.zoom(zoom_factor)
    zoomed_in = zoomed_in.set_position(('center', 'center'))
    zoomed_in = zoomed_in.set_duration(duration)
    return zoomed_in.margin(-w/2, -h/2, -w/2, -h/2)


def zoom_out(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_out = clip.zoom(zoom_factor)
    zoomed_out = zoomed_out.set_position(('center', 'center'))
    zoomed_out = zoomed_out.set_duration(duration)
    return zoomed_out


def slide_out_random_direction():
    return transitions.slide_out(random.choice(["left", "right", "top", "bottom"]))

"""def get_random_transition():
    transition_list = [
        transitions.crossfadein,
        transitions.crossfadeout,
        lambda: transitions.slide_in(random.choice(["left", "right", "top", "bottom"]), duration=duration),
        lambda: transitions.slide_out(random.choice(["left", "right", "top", "bottom"]), duration=duration)
    ]
    return random.choice(transition_list)
"""
def get_random_transition():
    transition_list = [
        zoom_inn,
        zoom_in,
        zoom_out
    ]
    return random.choice(transition_list)



# Set the path to the directory containing the input images
image_dir = '/home/jack/Desktop/HDD500/to-vid/building'

# Set the output video parameters
duration = 1 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])

# Define a function to create a video clip from an image with a random transition
def create_clips(filename):
    transition = get_random_transition()
    image_clip = ImageClip(filename).set_duration(duration)
    return image_clip.fl_image(lambda img: transition(img, duration=duration))
def create_clip(filename):
    transition = get_random_transition()
    image_clip = VideoFileClip(filename, audio=False).set_duration(duration)
    return image_clip.fl_image(lambda img: transition(img, duration=duration))



# Create a list of video clips for each image with a random transition
clips = [create_clip(filename) for filename in image_files]

# Concatenate the video clips to create the final slideshow
slideshow = concatenate_videoclips(clips)

# Set the output video parameters and write the video file
slideshow = slideshow.resize(size).set_fps(fps)
slideshow.write_videofile(os.path.join(image_dir, 'slideshow.mp4'))

import os
from moviepy.editor import *
import random
import glob
# Set the output video parameters
duration = .5 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_dir = '/home/jack/Desktop/HDD500/to-vid/building/'
image_files = random.sample(glob.glob(image_dir+'*.jpg'), 30)

# Define the crossfade transition
crossfade = lambda clip1, clip2: CompositeVideoClip([clip1, clip2.set_start(clip1.duration - duration)], size=size)

# Create a list of image clips with crossfades
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshow1.mp4')


import os
import random
import glob
import random
from moviepy.editor import *
from moviepy.video.compositing import *
# Set the output video parameters

fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_dir = '/home/jack/Desktop/HDD500/to-vid/building/*.jpg'
image_files = random.sample(glob.glob(image_dir), 40)
# Define the transitions
duration = 1 # Duration of each image in seconds
transitions = [transitions.crossfadein, transitions.crossfadeout]
random_transition = lambda clip1, clip2: random.choice(transitions)(clip1, clip2)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(os.path.join(image_files[i])).set_duration(1)
    
    if i > 0:
        # Add a random transition to the previous clip
        transition = random_transition(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshow2.mp4')


import moviepy
dir(moviepy.video.compositing.transitions)



import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),20)

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1, clip2.set_start(clip1.duration - duration)], size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshow.mp4')


/home/jack/Desktop/HDD500/to-vid/building/01145.jpg

image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),20)

print (image_files[4])

from moviepy.video.fx import *
dir(vfx)

# Good working with fade transition
import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=1), 
                               clip2.fx(vfx.fadein, duration=1)], 
                              size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshowT.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the transitions
def wipe(clip1, clip2, transition_type="right"):
    if transition_type == "right":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_x)
        mask = mask.crop(x1=0, y1=0, x2=mask.w*(clip2.start/clip2.duration), y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("right","top")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_x)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("left","top")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=0, x2=mask.w, y2=mask.h*(clip2.start/clip2.duration))
        mask = mask.set_position(("center","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","bottom")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    elif transition_type == "top":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(1-clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("center","bottom"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","top")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    else:
        raise ValueError("Invalid transition type")

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a wipe transition to the previous clip
        transition = wipe(clips[-1], image_clip, "left")
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Apply a wipe transition
video = video.fx(vfx.wipe, horizontal=True, duration=1)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshoww.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the wipe transition
def wipe(clip1, clip2, transition_type):
    # Create a mask clip for the transition
    mask = ColorClip(size, color=(255, 255, 255), duration=duration)
    if transition_type == "right":
        mask = mask.crop(x1=0, y1=0, x2=mask.w*clip2.start/clip2.duration, y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("right","top")).mask_video(mask)])
    elif transition_type == "left":
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right","top"))
        return CompositeVideoClip([clip1.set_position(("right","top")), clip2.set_position(("left","top")).mask_video(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=0, x2=mask.w, y2=mask.h*clip2.start/clip2.duration)
        mask = mask.set_position(("center","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","bottom")).mask_video(mask)])
    elif transition_type == "top":
        mask = ColorClip(size, color=(0,0,0), duration=duration)
        mask = mask.crop(x1=0, y1=mask.h*(1-clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("center","bottom"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","top")).mask_video(mask)])
    else:
        return clip2

# Create a list of image clips with wipe transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a wipe transition to the previous clip
        transition = wipe(clips[-1], image_clip, "right")
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshowW.mp4')


from moviepy.editor import *
from moviepy.video.fx import all as vfx

def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    mask = ColorClip(size, color=(0,0,0), duration=duration)
    if transition_type == "right":
        mask = mask.crop(x1=0, y1=0, x2=mask.w*clip2.start/clip2.duration, y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("right","top")).set_mask(mask)])
    elif transition_type == "left":
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1.set_mask(mask), clip2.set_position(("left","top"))])
    elif transition_type == "top":
        mask = mask.crop(x1=0, y1=mask.h*(1-clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("top","left"))
        return CompositeVideoClip([clip1.set_mask(mask), clip2.set_position(("left","top"))])
    elif transition_type == "bottom":
        mask = mask.crop(x1=0, y1=0, x2=mask.w, y2=mask.h*clip2.start/clip2.duration)
        mask = mask.set_position(("bottom","left"))
        return CompositeVideoClip([clip1.set_mask(mask), clip2.set_position(("left","top"))])
    else:
        raise ValueError("Invalid transition type")

def slideshow(image_files, duration, transition_type="right"):
    clips = []
    for i, image_file in enumerate(image_files):
        image_clip = ImageClip(image_file).set_duration(duration)
        if i == 0:
            clips.append(image_clip)
        else:
            transition = wipe(clips[-1], image_clip, transition_type)
            clips.append(transition)
        clips.append(image_clip)
    return concatenate_videoclips(clips)

image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)
duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshowx.mp4", fps=24)


from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx
from PIL import Image
import random
import glob


def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    image_clip = ImageClip(Image.new("RGB", size, "black"), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1.set_position(("right", "top")), clip2.set_position(("left", "top")).set_mask(mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right", "top"))
        return CompositeVideoClip([clip1.set_position(("left", "top")), clip2.set_position(("right", "top")).set_mask(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("bottom","center"))
        return CompositeVideoClip([clip1.set_position(("top","center")), clip2.set_position(("bottom","center")).set_mask(mask)])


def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        clip = VideoFileClip(path, audio=False).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip

image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'), 30)
duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshowx.mp4", fps=24)


import os
import random
import glob
from PIL import Image

from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx


def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    image_clip = ImageClip(Image.new("RGB", size, "black"), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1.set_position(("right", "top")), clip2.set_position(("left", "top")).set_mask(mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right", "top"))
        return CompositeVideoClip([clip1.set_position(("left", "top")), clip2.set_position(("right", "top")).set_mask(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("bottom","center"))
        return CompositeVideoClip([clip1.set_position(("top","center")), clip2.set_position(("bottom","center")).set_mask(mask)])


def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        with Image.open(path) as img:
            clip = ImageClip(img).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip


image_folder = '/home/jack/Desktop/HDD500/to-vid/building'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)

duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshow.mp4", fps=24)


import os
import random
import glob
from PIL import Image

from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx


def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    image_clip = ImageClip(Image.new("RGB", size, "black"), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1.set_position(("right", "top")), clip2.set_position(("left", "top")).set_mask(mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right", "top"))
        return CompositeVideoClip([clip1.set_position(("left", "top")), clip2.set_position(("right", "top")).set_mask(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("bottom","center"))
        return CompositeVideoClip([clip1.set_position(("top","center")), clip2.set_position(("bottom","center")).set_mask(mask)])


def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        with Image.open(path) as img:
            clip = ImageClip(img).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip


image_folder = '/home/jack/Desktop/HDD500/to-vid/building'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)

duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshow.mp4", fps=24)


import os
from PIL import Image
from moviepy.video.VideoClip import ImageClip, ColorClip
from moviepy.video.compositing.concatenate import concatenate_videoclips

def slideshow(images_path, duration, transition_type):
    clips = []
    transition_duration = 1
    
    if transition_type == "fade":
        transition_clip = ColorClip((1280, 720), color=(0, 0, 0), duration=transition_duration)
        transition_clip = transition_clip.crossfadein(transition_duration)
    
    for path in images_path:
        with Image.open(path) as img:
            clip = ImageClip(img).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
        if transition_type == "fade":
            clips.append(transition_clip)

    final_clip = concatenate_videoclips(clips, method="compose")
    final_clip = final_clip.set_duration(duration)
    return final_clip

# Example usage
image_folder = '/home/jack/Desktop/HDD500/to-vid/building'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)
duration = 10  # in seconds
transition_type = "fade"
slideshow_clip = slideshow(image_files, duration, transition_type)
slideshow_clip.write_videofile("slideshow.mp4", fps=24)


from moviepy.editor import VideoFileClip
from moviepy.video.fx.all import painting
import subprocess

# Load the video clip
clip = VideoFileClip("/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED01.mp4")

# Apply the painting effect
painted_clip = clip.fx(painting, saturation=1.4, black=0.006)

# Write the painted clip to a new file
painted_clip.write_videofile("/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED_painting.mp4")



num_frames=$(ffprobe -v error -select_streams v:0 -count_packets -show_entries stream=nb_read_packets -of csv=p=0 /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED01.mp4)
echo $num_frames


# Get the number of frames in the painted clip using ffmpeg
command = ["ffmpeg", "-i", "/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED01.mp4", "-v", "error", "-count_frames", "-select_streams", "v:0", "-show_entries", "stream=nb_frames", "-of", "default=nokey=1:noprint_wrappers=1"]
num_frames = int(subprocess.check_output(command))

print(f"The painted video has {num_frames} frames.")


# Good working with fade transition
import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
#image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/collections/newdownloads/512x512/*.jpg'),30)

image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=.25), 
                               clip2.fx(vfx.fadein, duration=.25)], 
                              size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2-5.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
from moviepy.video.compositing.transitions import WipeTransition
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
#image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/collections/newdownloads/512x512/*.jpg'),30)

image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=1), 
                               clip2.fx(vfx.fadein, duration=1)], 
                              size=size)

def vertical_wipe(clip1, clip2):
    return WipeTransition(clip1, clip2, direction='vertical', duration=1)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a wipe transition to the previous clip
        transition = vertical_wipe(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Define the wipe transition
def vertical_wipe(clip1, clip2):
    return wipe(clip1, clip2, transition='vertical', duration=1)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a vertical wipe transition to the previous clip
        transition = vertical_wipe(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2.mp4')


import moviepy.video.compositing.transitions
dir(moviepy.video.compositing.transitions)

from moviepy.video.compositing.transitions import slide_in

from moviepy.video.compositing.transitions import slide_in
from moviepy.video.fx import all
from moviepy.editor import *
import glob
import random
# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = .25 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Create a list of image clips with transitions
clips = []

for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    direction = random.choice(['right','left','top','bottom'])
    
    if i > 0:
        # Add a vertical slide transition to the previous clip
        transition = slide_in(image_clip, duration=0.1, side=direction)
        clips.append(CompositeVideoClip([clips[-1], transition]).set_duration(0.1))
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2-1rc.mp4')


from moviepy.editor import concatenate_videoclips
from moviepy.video.compositing.transitions import (
    CrossfadeIn, CrossfadeOut, SlideIn, SlideOut, ZoomIn, ZoomOut, WarpIn, WarpOut
)

# Create instances of the transition classes
transition1 = CrossfadeIn()
transition2 = CrossfadeOut()
transition3 = SlideIn()
transition4 = SlideOut()
transition5 = ZoomIn()
transition6 = ZoomOut()
transition7 = WarpIn()
transition8 = WarpOut()

# Concatenate video clips with transitions
video_clips = [clip1, clip2, clip3]  # Replace with your actual video clips
transitions = [transition1, transition2, transition3, transition4, transition5, transition6, transition7, transition8]
final_clip = concatenate_videoclips(video_clips, transitions=transitions)


"""Requires scikit-image installed (for ``vfx.painting``)."""

from moviepy import *


# WE TAKE THE SUBCLIPS WHICH ARE 2 SECONDS BEFORE & AFTER THE FREEZE

charade = VideoFileClip("../../videos/charade.mp4")
tfreeze = convert_to_seconds(19.21)  # Time of the freeze, 19'21

clip_before = charade.subclip(tfreeze - 2, tfreeze)
clip_after = charade.subclip(tfreeze, tfreeze + 2)


# THE FRAME TO FREEZE

im_freeze = charade.to_ImageClip(tfreeze)
painting = charade.fx(vfx.painting, saturation=1.6, black=0.006).to_ImageClip(tfreeze)

txt = TextClip("Audrey", font="Amiri-regular", font_size=35)

painting_txt = (
    CompositeVideoClip([painting, txt.set_pos((10, 180))])
    .add_mask()
    .with_duration(3)
    .crossfadein(0.5)
    .crossfadeout(0.5)
)

# FADEIN/FADEOUT EFFECT ON THE PAINTED IMAGE

painting_fading = CompositeVideoClip([im_freeze, painting_txt])

# FINAL CLIP AND RENDERING

final_clip = concatenate_videoclips(
    [clip_before, painting_fading.with_duration(3), clip_after]
)

final_clip.write_videofile(
    "../../audrey.avi", fps=charade.fps, codec="mpeg4", audio_bitrate="3000k"
)

from mohttp://localhost:8888/notebooks/Imports_moviepy-ffmpeg.ipynb#viepy.editor import concatenate_videoclips
from moviepy.video.fx import fadein, fadeout, slide_in, slide_out, zoom_in, zoom_out, warp_in, warp_out


import moviepy.video
dir (moviepy.video)

from moviepy.video import fx,io,tools,VideoClip,compositing
dir(compositing)

from moviepy.video import fx,io,tools,VideoClip,compositing
dir (compositing.transitions)

from moviepy.video.compositing.transitions import crossfadeinout, crossfadeoutin, slide_in, slide_out, zoom_in, zoom_out, warp_in, warp_outfrom 

from moviepy.video import fx,io,tools,VideoClip,compositing
compositing.fadein

from moviepy.video import fx,io,tools,VideoClip,compositing
dir (compositing.CompositeVideoClip)

from moviepy.video import ImageClip, concatenate_videoclips
from moviepy.video.compositing.transitions import crossfadeinout, crossfadeoutin, slide_in, slide_out, zoom_in, zoom_out, warp_in, warp_out
import random

def get_random_transition(duration):
    transition_list = [
        crossfadeinout,
        crossfadeoutin,
        slide_in,
        lambda: slide_out(random.choice(["left", "right", "top", "bottom"])),
        zoom_in,
        zoom_out,
        warp_in,
        warp_out
    ]
    return random.choice(transition_list)(duration=duration)
image_files= random.sample(glob.glob("/home/jack/Desktop/StoryMaker/static/current_project/Misc/portrait/*.jpg"),10)
#image_files = ["image1.jpg", "image2.jpg", "image3.jpg", "image4.jpg"]
clips = []
for filename in image_files:
    clip = ImageClip(filename).set_duration(2)
    transition = get_random_transition(1)
    clips.append(clip.fx(transition))

final_clip = concatenate_videoclips(clips)
final_clip.write_videofile("slideshow.mp4", fps=25)


!vlc slideshow.mp4

/home/jack/Desktop/StoryMaker/newvid/text.mp4

"""Requires scikit-image installed (for ``vfx.painting``)."""

from moviepy import *


# WE TAKE THE SUBCLIPS WHICH ARE 2 SECONDS BEFORE & AFTER THE FREEZE

charade = VideoFileClip("/home/jack/Desktop/StoryMaker/newvid/text.mp4")
#tfreeze = convert_to_seconds(19.21)  # Time of the freeze, 19'21
tfreeze = 3  # Time of the freeze, 19'21

clip_before = charade.subclip(tfreeze - 2, tfreeze)
clip_after = charade.subclip(tfreeze, tfreeze + 2)


# THE FRAME TO FREEZE

im_freeze = charade.to_ImageClip(tfreeze)
painting = charade.fx(vfx.painting, saturation=1.6, black=0.006).to_ImageClip(tfreeze)

txt = TextClip("Audrey", font="Amiri-regular", font_size=35)

painting_txt = (
    CompositeVideoClip([painting, txt.set_pos((10, 180))])
    .add_mask()
    .with_duration(3)
    .crossfadein(0.5)
    .crossfadeout(0.5)
)

# FADEIN/FADEOUT EFFECT ON THE PAINTED IMAGE

painting_fading = CompositeVideoClip([im_freeze, painting_txt])

# FINAL CLIP AND RENDERING

final_clip = concatenate_videoclips(
    [clip_before, painting_fading.with_duration(3), clip_after]
)

final_clip.write_videofile(
    "freeze.avi", fps=charade.fps, codec="mpeg4", audio_bitrate="3000k"
)




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/AI_DATA_get_jpeg_info.ipynb
Code Content:
%%writefile gitinfo.py
from PIL import Image
from PIL.ExifTags import TAGS
from sys import argv
image = Image.open(argv[1]) 
exif_data = image._getexif()

for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    # Check if the value is bytes and decode it to UTF-8 if so
    if isinstance(value, bytes):
        value = value.decode('utf-8', errors='replace')
    print(f"{tag_name}: {value}")

!python gitinfo.py /home/jack/Desktop/FlaskLearnFfmpeg/static/images/graffiti_portrait/AI/DreamShaper_v7_A_stylized_graffiti_portrait_happy_Taylo_002.webp

from PIL import Image
from PIL.ExifTags import TAGS
from sys import argv
image = Image.open("/home/jack/Desktop/FlaskLearnFfmpeg/static/images/space_cruiser/00002.jpg") 
exif_data = image._getexif()

for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    # Check if the value is bytes and decode it to UTF-8 if so
    if isinstance(value, bytes):
        value = value.decode('utf-8', errors='replace')
    print(f"{tag_name}: {value}")


from PIL import Image
from PIL.ExifTags import TAGS
from sys import argv
image = Image.open("d1dfcf75-8cab-4d35-8f01-5c771ee74d83.jpg") 
exif_data = image._getexif()

for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    # Check if the value is bytes and decode it to UTF-8 if so
    if isinstance(value, bytes):
        value = value.decode('utf-8', errors='replace')
    print(f"{tag_name}: {value}")


from PIL import Image
from PIL.ExifTags import TAGS

image = Image.open("static/images/yodo_03/00023.jpg") 
exif_data = image._getexif()

for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    # Check if the value is bytes and decode it to UTF-8 if so
    if isinstance(value, bytes):
        value = value.decode('utf-8', errors='replace')
    print(f"{tag_name}: {value}")


from PIL import Image
from PIL.ExifTags import TAGS

image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b-3.jpeg") 
exif_data = image._getexif()

for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    # Check if the value is bytes and decode it to UTF-8 if so
    if isinstance(value, bytes):
        value = value.decode('utf-8', errors='replace')
    print(f"{tag_name}: {value}")


from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS

# Open the image
#image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b-2.jpeg")
image = Image.open("/home/jack/Desktop/FlaskLearnFfmpeg/static/images/yodo_03/00008.jpg")

# Get the Exif data
exif_data = image._getexif()

# Iterate through all possible Exif tags
for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    
    # Check if the value is bytes (binary)
    if isinstance(value, bytes):
        # Try to decode it as UTF-8
        try:
            value = value.decode('utf-8')
        except UnicodeDecodeError:
            # If it can't be decoded as UTF-8, leave it as bytes
            pass
    
    print(f"Tag {tag}: {tag_name} - Value: {value}")


from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS

# Open the image
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg")

# Get the Exif data
exif_data = image._getexif()

# Iterate through all possible Exif tags
for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    print(f"Tag {tag}: {tag_name} - Value: {value}")


from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg") 
exif_data = image._getexif()
for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    binary_content =f"{tag_name}: {value}"
    print(binary_content)

human_readable_comment = binary_content.decode('utf-8').replace('\x00', '')
# Print the human-readable comment
print("Human-Readable Comment:", human_readable_comment)


from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg") 
exif_data = image._getexif()


from PIL import Image

def extract_image_comment(image_path):
    try:
        # Open the image using Pillow
        with Image.open(image_path) as img:
            # Check if the image has a comment
            if "Comment" in img.info:
                comment = img.info["Comment"]
                print("Image Comment:")
                print(comment)
            else:
                print("No comment found in the image.")

    except Exception as e:
        print(f"An error occurred: {str(e)}")

# Replace 'your_image.jpg' with the path to your image file
image_path = "21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg"
extract_image_comment(image_path)


for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    print(f"{tag_name}: {value}")


# Assuming the binary comment is stored in a variable named 'binary_comment'
binary_comment = b'UNICODE\x00\x00H\x00D\x00R\x00,\x00 \x00D\x00S\x00L\x00R\x00,\x00 \x008\x00K\x00 \x00w\x00a\x00l\x00l\x00p\x00a\x00p\x00e\x00r\x00s\x00,\x00 \x00R\x00A\x00W\x00 \x00p\x00h\x00o\x00t\x00o\x00s\x00 \x00(\x00c\x00o\x00m\x00p\x00l\x00e\x00x\x00 \x00d\x00e\x00t\x00a\x00i\x00l\x00s\x00:\x00 \x001\x00.\x003\x00)\x00,\x00 \x00u\x00l\x00t\x00r\x00a\x00-\x00c\x00o\x00m\x00p\x00l\x00e\x00x\x00 \x00l\x00a\x00n\x00d\x00s\x00c\x00a\x00p\x00e\x00s\x00,\x00 \x00p\x00o\x00l\x00i\x00s\x00h\x00e\x00d\x00,\x00 \x00s\x00p\x00e\x00c\x00u\x00l\x00a\x00r\x00 \x00l\x00i\x00g\x00h\x00t\x00i\x00n\x00g\x00,\x00\n\x00\n\x00A\x00 \x00p\x00r\x00e\x00t\x00t\x00y\x00 \x00w\x00o\x00m\x00a\x00n\x00 \x00w\x00i\x00t\x00h\x00 \x00a\x00n\x00 \x00a\x00c\x00c\x00e\x00n\x00t\x00u\x00a\x00t\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00 \x00i\x00s\x00 \x00d\x00r\x00e\x00s\x00s\x00e\x00d\x00 \x00i\x00n\x00 \x00a\x00 \x00w\x00h\x00i\x00t\x00e\x00 \x00c\x00h\x00i\x00f\x00f\x00o\x00n\x00 \x00b\x00l\x00o\x00u\x00s\x00e\x00,\x00 \x00w\x00i\x00d\x00e\x00 \x00h\x00i\x00k\x00i\x00n\x00g\x00 \x00s\x00h\x00o\x00r\x00t\x00s\x00 \x00a\x00n\x00d\x00 \x00r\x00o\x00u\x00g\x00h\x00 \x00h\x00i\x00k\x00i\x00n\x00g\x00 \x00b\x00o\x00o\x00t\x00s\x00.\x00 \x00S\x00h\x00e\x00 \x00w\x00e\x00a\x00r\x00s\x00 \x00a\x00 \x00s\x00u\x00m\x00m\x00e\x00r\x00 \x00h\x00a\x00t\x00 \x00a\x00n\x00d\x00 \x00h\x00a\x00s\x00 \x00l\x00o\x00n\x00g\x00 \x00c\x00u\x00r\x00l\x00y\x00 \x00r\x00e\x00d\x00 \x00h\x00a\x00i\x00r\x00.\x00 \x00S\x00h\x00e\x00 \x00h\x00a\x00s\x00 \x00a\x00 \x00b\x00i\x00c\x00y\x00c\x00l\x00e\x00.\x00 \x00T\x00h\x00e\x00 \x00a\x00c\x00t\x00i\x00o\x00n\x00 \x00t\x00a\x00k\x00e\x00s\x00 \x00p\x00l\x00a\x00c\x00e\x00 \x00i\x00n\x00 \x00t\x00h\x00e\x00 \x00t\x00a\x00i\x00g\x00a\x00 \x00f\x00o\x00r\x00e\x00s\x00t\x00.\x00\n\x00\n\x00E\x00x\x00i\x00t\x00:\x00\n\x00-\x00 \x00C\x00h\x00a\x00r\x00a\x00c\x00t\x00e\x00r\x00:\x00\n\x00 \x00 \x00 \x00 \x00-\x00 \x00B\x00o\x00d\x00y\x00:\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00A\x00 \x00d\x00e\x00f\x00i\x00n\x00e\x00d\x00 \x00a\x00n\x00d\x00 \x00t\x00o\x00n\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00T\x00h\x00i\x00n\x00 \x00w\x00a\x00i\x00s\x00t\x00,\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00a\x00t\x00h\x00l\x00e\x00t\x00i\x00c\x00 \x00b\x00o\x00d\x00y\x00,\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00C\x00o\x00n\x00f\x00i\x00d\x00e\x00n\x00c\x00e\x00 \x00i\x00n\x00 \x00y\x00o\x00u\x00r\x00 \x00p\x00o\x00s\x00t\x00u\x00r\x00e\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00L\x00e\x00a\x00n\x00i\x00n\x00g\x00 \x00o\x00n\x00 \x00y\x00o\x00u\x00r\x00 \x00b\x00i\x00k\x00e\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00t\x00o\x00n\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00\n\x00-\x00 \x00H\x00a\x00i\x00r\x00:\x00\n\x00 \x00 \x00 \x00 \x00C\x00u\x00r\x00l\x00y\x00 \x00h\x00a\x00i\x00r\x00\n\x00 \x00 \x00 \x00 \x00C\x00u\x00r\x00l\x00s\x00 \x00f\x00r\x00a\x00m\x00i\x00n\x00g\x00 \x00h\x00e\x00r\x00 \x00f\x00a\x00c\x00e\x00\n\x00 \x00 \x00 \x00 \x00n\x00e\x00a\x00t\x00 \x00a\x00n\x00d\x00 \x00t\x00i\x00d\x00y\x00,\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00d\x00 \x00h\x00a\x00i\x00r\x00 \x00c\x00o\x00l\x00o\x00r\x00,\x00\n\x00 \x00 \x00 \x00 \x00S\x00u\x00b\x00t\x00l\x00e\x00 \x00h\x00i\x00g\x00h\x00l\x00i\x00g\x00h\x00t\x00s\x00 \x00t\x00h\x00a\x00t\x00 \x00a\x00d\x00d\x00 \x00d\x00e\x00p\x00t\x00h\x00 \x00t\x00o\x00 \x00t\x00h\x00e\x00 \x00s\x00t\x00y\x00l\x00e\x00\n\x00-\x00 \x00F\x00a\x00c\x00e\x00:\x00\n\x00 \x00 \x00 \x00 \x00E\x00u\x00r\x00o\x00p\x00e\x00a\x00n\x00 \x00f\x00a\x00c\x00e\x00 \x00t\x00y\x00p\x00e\x00\n\x00 \x00 \x00 \x00 \x00B\x00r\x00i\x00g\x00h\x00t\x00 \x00s\x00k\x00i\x00n\x00\n\x00 \x00 \x00 \x00 \x00F\x00r\x00e\x00c\x00k\x00l\x00e\x00s\x00\n\x00 \x00 \x00 \x00 \x00G\x00r\x00e\x00e\x00n\x00 \x00e\x00y\x00e\x00s\x00\n\x00-\x00 \x00L\x00o\x00c\x00a\x00t\x00i\x00o\x00n\x00:\x00\n\x00 \x00 \x00 \x00 \x00S\x00i\x00b\x00e\x00r\x00i\x00a\x00n\x00 \x00t\x00a\x00i\x00g\x00a\x00,\x00\n\x00 \x00 \x00 \x00 \x00S\x00u\x00n\x00l\x00i\x00g\x00h\x00t\x00 \x00f\x00i\x00l\x00t\x00e\x00r\x00i\x00n\x00g\x00 \x00t\x00h\x00r\x00o\x00u\x00g\x00h\x00 \x00t\x00h\x00e\x00 \x00n\x00e\x00e\x00d\x00l\x00e\x00s\x00\n\x00 \x00 \x00 \x00 \x00t\x00r\x00e\x00e\x00 \x00c\x00r\x00o\x00w\x00n\x00s\x00 \x00p\x00r\x00o\x00v\x00i\x00d\x00i\x00n\x00g\x00 \x00s\x00h\x00a\x00d\x00e\x00\n\x00 \x00 \x00 \x00 \x00C\x00a\x00l\x00m\x00 \x00a\x00n\x00d\x00 \x00s\x00e\x00r\x00e\x00n\x00e\x00 \x00e\x00n\x00v\x00i\x00r\x00o\x00n\x00m\x00e\x00n\x00t\x00\n\x00N\x00e\x00g\x00a\x00t\x00i\x00v\x00e\x00 \x00p\x00r\x00o\x00m\x00p\x00t\x00:\x00 \x00l\x00o\x00w\x00r\x00e\x00s\x00,\x00 \x00b\x00a\x00d\x00 \x00a\x00n\x00a\x00t\x00o\x00m\x00y\x00,\x00 \x00b\x00a\x00d\x00 \x00h\x00a\x00n\x00d\x00s\x00,\x00 \x00m\x00i\x00s\x00s\x00i\x00n\x00g\x00 \x00f\x00i\x00n\x00g\x00e\x00r\x00s\x00,\x00 \x00(\x00w\x00o\x00r\x00s\x00t\x00 \x00q\x00u\x00a\x00l\x00i\x00t\x00y\x00:\x001\x00.\x004\x00)\x00,\x00 \x00(\x00l\x00o\x00w\x00 \x00q\x00u\x00a\x00l\x00i\x00t\x00y\x00:\x001\x00.\x004\x00)\x00,\x00 \x00s\x00t\x00r\x00a\x00b\x00i\x00s\x00m\x00u\x00s\x00,\x00 \x00c\x00r\x00o\x00s\x00s\x00e\x00d\x00 \x00e\x00y\x00e\x00s\x00,\x00 \x00n\x00s\x00f\x00w\x00,\x00 \x00(\x00f\x00o\x00r\x00e\x00h\x00e\x00a\x00d\x00:\x001\x00.\x003\x00)\x00,\x00 \x00e\x00a\x00s\x00y\x00n\x00e\x00g\x00a\x00t\x00i\x00v\x00e\x00\n\x00S\x00t\x00e\x00p\x00s\x00:\x00 \x003\x000\x00,\x00 \x00S\x00a\x00m\x00p\x00l\x00e\x00r\x00:\x00 \x00D\x00P\x00M\x00+\x00+\x00 \x002\x00M\x00 \x00K\x00a\x00r\x00r\x00a\x00s\x00,\x00 \x00C\x00F\x00G\x00 \x00s\x00c\x00a\x00l\x00e\x00:\x00 \x001\x000\x00.\x000\x00,\x00 \x00S\x00e\x00e\x00d\x00:\x00 \x003\x003\x002\x008\x002\x005\x007\x004\x004\x003\x00,\x00 \x00S\x00i\x00z\x00e\x00:\x00 \x005\x001\x002\x00x\x007\x006\x008\x00,\x00 \x00M\x00o\x00d\x00e\x00l\x00 \x00h\x00a\x00s\x00h\x00:\x00 \x009\x001\x00d\x00c\x004\x008\x000\x006\x004\x000\x00,\x00 \x00M\x00o\x00d\x00e\x00l\x00:\x00 \x00s\x00h\x00a\x00m\x00p\x00o\x00o\x00M\x00i\x00x\x00_\x00v\x003\x000\x00,\x00 \x00S\x00e\x00e\x00d\x00 \x00r\x00e\x00s\x00i\x00z\x00e\x00 \x00f\x00r\x00o\x00m\x00:\x00 \x00-\x001\x00x\x00-\x001\x00,\x00 \x00D\x00e\x00n\x00o\x00i\x00s\x00i\x00n\x00g\x00 \x00s\x00t\x00r\x00e\x00n\x00g\x00t\x00h\x00:\x00 \x000'
# Your binary comment here

# Decode the binary comment as UTF-16LE and remove null bytes
#human_readable_comment = binary_comment.decode('utf-16le').replace('\x00', '')
human_readable_comment = binary_comment.decode('utf-8').replace('\x00', '')
# Print the human-readable comment
print("Human-Readable Comment:", human_readable_comment)


# Assuming the binary comment is stored in a variable named 'binary_comment'
binary_comment = b'UNICODE\x00\x00H\x00D\x00R\x00,\x00 \x00D\x00S\x00L\x00R\x00,\x00 \x008\x00K\x00 \x00w\x00a\x00l\x00l\x00p\x00a\x00p\x00e\x00r\x00s\x00,\x00 \x00R\x00A\x00W\x00 \x00p\x00h\x00o\x00t\x00o\x00s\x00 \x00(\x00c\x00o\x00m\x00p\x00l\x00e\x00x\x00 \x00d\x00e\x00t\x00a\x00i\x00l\x00s\x00:\x00 \x001\x00.\x003\x00)\x00,\x00 \x00u\x00l\x00t\x00r\x00a\x00-\x00c\x00o\x00m\x00p\x00l\x00e\x00x\x00 \x00l\x00a\x00n\x00d\x00s\x00c\x00a\x00p\x00e\x00s\x00,\x00 \x00p\x00o\x00l\x00i\x00s\x00h\x00e\x00d\x00,\x00 \x00s\x00p\x00e\x00c\x00u\x00l\x00a\x00r\x00 \x00l\x00i\x00g\x00h\x00t\x00i\x00n\x00g\x00,\x00\n\x00\n\x00A\x00 \x00p\x00r\x00e\x00t\x00t\x00y\x00 \x00w\x00o\x00m\x00a\x00n\x00 \x00w\x00i\x00t\x00h\x00 \x00a\x00n\x00 \x00a\x00c\x00c\x00e\x00n\x00t\x00u\x00a\x00t\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00 \x00i\x00s\x00 \x00d\x00r\x00e\x00s\x00s\x00e\x00d\x00 \x00i\x00n\x00 \x00a\x00 \x00w\x00h\x00i\x00t\x00e\x00 \x00c\x00h\x00i\x00f\x00f\x00o\x00n\x00 \x00b\x00l\x00o\x00u\x00s\x00e\x00,\x00 \x00w\x00i\x00d\x00e\x00 \x00h\x00i\x00k\x00i\x00n\x00g\x00 \x00s\x00h\x00o\x00r\x00t\x00s\x00 \x00a\x00n\x00d\x00 \x00r\x00o\x00u\x00g\x00h\x00 \x00h\x00i\x00k\x00i\x00n\x00g\x00 \x00b\x00o\x00o\x00t\x00s\x00.\x00 \x00S\x00h\x00e\x00 \x00w\x00e\x00a\x00r\x00s\x00 \x00a\x00 \x00s\x00u\x00m\x00m\x00e\x00r\x00 \x00h\x00a\x00t\x00 \x00a\x00n\x00d\x00 \x00h\x00a\x00s\x00 \x00l\x00o\x00n\x00g\x00 \x00c\x00u\x00r\x00l\x00y\x00 \x00r\x00e\x00d\x00 \x00h\x00a\x00i\x00r\x00.\x00 \x00S\x00h\x00e\x00 \x00h\x00a\x00s\x00 \x00a\x00 \x00b\x00i\x00c\x00y\x00c\x00l\x00e\x00.\x00 \x00T\x00h\x00e\x00 \x00a\x00c\x00t\x00i\x00o\x00n\x00 \x00t\x00a\x00k\x00e\x00s\x00 \x00p\x00l\x00a\x00c\x00e\x00 \x00i\x00n\x00 \x00t\x00h\x00e\x00 \x00t\x00a\x00i\x00g\x00a\x00 \x00f\x00o\x00r\x00e\x00s\x00t\x00.\x00\n\x00\n\x00E\x00x\x00i\x00t\x00:\x00\n\x00-\x00 \x00C\x00h\x00a\x00r\x00a\x00c\x00t\x00e\x00r\x00:\x00\n\x00 \x00 \x00 \x00 \x00-\x00 \x00B\x00o\x00d\x00y\x00:\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00A\x00 \x00d\x00e\x00f\x00i\x00n\x00e\x00d\x00 \x00a\x00n\x00d\x00 \x00t\x00o\x00n\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00T\x00h\x00i\x00n\x00 \x00w\x00a\x00i\x00s\x00t\x00,\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00a\x00t\x00h\x00l\x00e\x00t\x00i\x00c\x00 \x00b\x00o\x00d\x00y\x00,\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00C\x00o\x00n\x00f\x00i\x00d\x00e\x00n\x00c\x00e\x00 \x00i\x00n\x00 \x00y\x00o\x00u\x00r\x00 \x00p\x00o\x00s\x00t\x00u\x00r\x00e\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00L\x00e\x00a\x00n\x00i\x00n\x00g\x00 \x00o\x00n\x00 \x00y\x00o\x00u\x00r\x00 \x00b\x00i\x00k\x00e\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00t\x00o\x00n\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00\n\x00-\x00 \x00H\x00a\x00i\x00r\x00:\x00\n\x00 \x00 \x00 \x00 \x00C\x00u\x00r\x00l\x00y\x00 \x00h\x00a\x00i\x00r\x00\n\x00 \x00 \x00 \x00 \x00C\x00u\x00r\x00l\x00s\x00 \x00f\x00r\x00a\x00m\x00i\x00n\x00g\x00 \x00h\x00e\x00r\x00 \x00f\x00a\x00c\x00e\x00\n\x00 \x00 \x00 \x00 \x00n\x00e\x00a\x00t\x00 \x00a\x00n\x00d\x00 \x00t\x00i\x00d\x00y\x00,\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00d\x00 \x00h\x00a\x00i\x00r\x00 \x00c\x00o\x00l\x00o\x00r\x00,\x00\n\x00 \x00 \x00 \x00 \x00S\x00u\x00b\x00t\x00l\x00e\x00 \x00h\x00i\x00g\x00h\x00l\x00i\x00g\x00h\x00t\x00s\x00 \x00t\x00h\x00a\x00t\x00 \x00a\x00d\x00d\x00 \x00d\x00e\x00p\x00t\x00h\x00 \x00t\x00o\x00 \x00t\x00h\x00e\x00 \x00s\x00t\x00y\x00l\x00e\x00\n\x00-\x00 \x00F\x00a\x00c\x00e\x00:\x00\n\x00 \x00 \x00 \x00 \x00E\x00u\x00r\x00o\x00p\x00e\x00a\x00n\x00 \x00f\x00a\x00c\x00e\x00 \x00t\x00y\x00p\x00e\x00\n\x00 \x00 \x00 \x00 \x00B\x00r\x00i\x00g\x00h\x00t\x00 \x00s\x00k\x00i\x00n\x00\n\x00 \x00 \x00 \x00 \x00F\x00r\x00e\x00c\x00k\x00l\x00e\x00s\x00\n\x00 \x00 \x00 \x00 \x00G\x00r\x00e\x00e\x00n\x00 \x00e\x00y\x00e\x00s\x00\n\x00-\x00 \x00L\x00o\x00c\x00a\x00t\x00i\x00o\x00n\x00:\x00\n\x00 \x00 \x00 \x00 \x00S\x00i\x00b\x00e\x00r\x00i\x00a\x00n\x00 \x00t\x00a\x00i\x00g\x00a\x00,\x00\n\x00 \x00 \x00 \x00 \x00S\x00u\x00n\x00l\x00i\x00g\x00h\x00t\x00 \x00f\x00i\x00l\x00t\x00e\x00r\x00i\x00n\x00g\x00 \x00t\x00h\x00r\x00o\x00u\x00g\x00h\x00 \x00t\x00h\x00e\x00 \x00n\x00e\x00e\x00d\x00l\x00e\x00s\x00\n\x00 \x00 \x00 \x00 \x00t\x00r\x00e\x00e\x00 \x00c\x00r\x00o\x00w\x00n\x00s\x00 \x00p\x00r\x00o\x00v\x00i\x00d\x00i\x00n\x00g\x00 \x00s\x00h\x00a\x00d\x00e\x00\n\x00 \x00 \x00 \x00 \x00C\x00a\x00l\x00m\x00 \x00a\x00n\x00d\x00 \x00s\x00e\x00r\x00e\x00n\x00e\x00 \x00e\x00n\x00v\x00i\x00r\x00o\x00n\x00m\x00e\x00n\x00t\x00\n\x00N\x00e\x00g\x00a\x00t\x00i\x00v\x00e\x00 \x00p\x00r\x00o\x00m\x00p\x00t\x00:\x00 \x00l\x00o\x00w\x00r\x00e\x00s\x00,\x00 \x00b\x00a\x00d\x00 \x00a\x00n\x00a\x00t\x00o\x00m\x00y\x00,\x00 \x00b\x00a\x00d\x00 \x00h\x00a\x00n\x00d\x00s\x00,\x00 \x00m\x00i\x00s\x00s\x00i\x00n\x00g\x00 \x00f\x00i\x00n\x00g\x00e\x00r\x00s\x00,\x00 \x00(\x00w\x00o\x00r\x00s\x00t\x00 \x00q\x00u\x00a\x00l\x00i\x00t\x00y\x00:\x001\x00.\x004\x00)\x00,\x00 \x00(\x00l\x00o\x00w\x00 \x00q\x00u\x00a\x00l\x00i\x00t\x00y\x00:\x001\x00.\x004\x00)\x00,\x00 \x00s\x00t\x00r\x00a\x00b\x00i\x00s\x00m\x00u\x00s\x00,\x00 \x00c\x00r\x00o\x00s\x00s\x00e\x00d\x00 \x00e\x00y\x00e\x00s\x00,\x00 \x00n\x00s\x00f\x00w\x00,\x00 \x00(\x00f\x00o\x00r\x00e\x00h\x00e\x00a\x00d\x00:\x001\x00.\x003\x00)\x00,\x00 \x00e\x00a\x00s\x00y\x00n\x00e\x00g\x00a\x00t\x00i\x00v\x00e\x00\n\x00S\x00t\x00e\x00p\x00s\x00:\x00 \x003\x000\x00,\x00 \x00S\x00a\x00m\x00p\x00l\x00e\x00r\x00:\x00 \x00D\x00P\x00M\x00+\x00+\x00 \x002\x00M\x00 \x00K\x00a\x00r\x00r\x00a\x00s\x00,\x00 \x00C\x00F\x00G\x00 \x00s\x00c\x00a\x00l\x00e\x00:\x00 \x001\x000\x00.\x000\x00,\x00 \x00S\x00e\x00e\x00d\x00:\x00 \x003\x003\x002\x008\x002\x005\x007\x004\x004\x003\x00,\x00 \x00S\x00i\x00z\x00e\x00:\x00 \x005\x001\x002\x00x\x007\x006\x008\x00,\x00 \x00M\x00o\x00d\x00e\x00l\x00 \x00h\x00a\x00s\x00h\x00:\x00 \x009\x001\x00d\x00c\x004\x008\x000\x006\x004\x000\x00,\x00 \x00M\x00o\x00d\x00e\x00l\x00:\x00 \x00s\x00h\x00a\x00m\x00p\x00o\x00o\x00M\x00i\x00x\x00_\x00v\x003\x000\x00,\x00 \x00S\x00e\x00e\x00d\x00 \x00r\x00e\x00s\x00i\x00z\x00e\x00 \x00f\x00r\x00o\x00m\x00:\x00 \x00-\x001\x00x\x00-\x001\x00,\x00 \x00D\x00e\x00n\x00o\x00i\x00s\x00i\x00n\x00g\x00 \x00s\x00t\x00r\x00e\x00n\x00g\x00t\x00h\x00:\x00 \x000'
# Your binary comment here

# Decode the binary comment as UTF-16LE and remove null bytes
human_readable_comment = binary_comment.decode('utf-8').replace('\x00', '')

# Print the human-readable comment
print("Human-Readable Comment:", human_readable_comment)


from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg") 
exif_data = image._getexif()


for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    binary_comment =f"{tag_name}: {value}"
print(binary_comment)
human_readable_comment = binary_comment.decode('utf-8').replace('\x00', '')
print(human_readable_comment)

from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
image = Image.open("/home/jack/Desktop/FlaskLearnFfmpeg/static/images/Halle_Berry/00018.jpg") 
exif_data = image._getexif()
image_description = exif_data.get(270, None)  # 270 corresponds to the "ImageDescription" tag
if image_description:
    binary_comment =f"Image Description: {image_description}"
# Decode the binary comment as UTF-8 and remove null bytes
human_readable_comment = binary_comment.decode('utf-8').replace('\x00', '')

# Print the human-readable comment
print("Human-Readable Comment:\n", human_readable_comment)


# Extract GPS data if available
gps_data = exif_data.get(34853, None)  # 34853 corresponds to the "GPSInfo" tag
if gps_data:
    print("GPS Data:")
    for tag, value in gps_data.items():
        tag_name = GPSTAGS.get(tag, tag)
        print(f"{tag_name}: {value}")


from PIL import Image
from PIL.ExifTags import TAGS

# Open the image
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg")

# Get the EXIF data
exif_data = image._getexif()

# Check if there is EXIF data in the image
if exif_data:
    print("EXIF Data:")
    for tag, value in exif_data.items():
        # Convert the tag number to its corresponding tag name
        tag_name = TAGS.get(tag, tag)
        binary_comment = f"{tag_name}: {value}"
        #binary_comment =f"Image Description: {image_description}"
        # Decode the binary comment as UTF-16LE and remove null bytes
        human_readable_comment = binary_comment.decode('utf-8').replace('\x00', '')

        # Print the human-readable comment
        print("Human-Readable Comment:", human_readable_comment)
else:
    print("No EXIF data found in the image.")


from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
data = []
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg") 
exif_data = image._getexif()
image_description = exif_data.get(270, None)  # 270 corresponds to the "ImageDescription" tag
if image_description:
    binary_comment =f"Image Description: {image_description}"
    data.append(binary_comment)
# Decode the binary comment as UTF-16LE and remove null bytes
human_readable_comment = binary_comment.decode('utf-8').replace('\x00', '')

# Print the human-readable comment
print("Human-Readable Comment:", human_readable_comment)


print (data)

from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
DATA = []
image = Image.open("21b680bd-bcfd-483c-bc2d-cc120f6f3b8b.jpeg") 
exif_data = image._getexif()
for tag, value in exif_data.items():
    tag_name = TAGS.get(tag, tag)
    print(f"{tag_name}: {value}")
    DATA.append(f"{tag_name}: {value}")

print(DATA[1])
results in:
UserComment: b'UNICODE\x00\x00H\x00D\x00R\x00,\x00 \x00D\x00S\x00L\x00R\x00,\x00 \x008\x00K\x00 \x00w\x00a\x00l\x00l\x00p\x00a\x00p\x00e\x00r\x00s\x00,\x00 \x00R\x00A\x00W\x00 \x00p\x00h\x00o\x00t\x00o\x00s\x00 \x00(\x00c\x00o\x00m\x00p\x00l\x00e\x00x\x00 \x00d\x00e\x00t\x00a\x00i\x00l\x00s\x00:\x00 \x001\x00.\x003\x00)\x00,\x00 \x00u\x00l\x00t\x00r\x00a\x00-\x00c\x00o\x00m\x00p\x00l\x00e\x00x\x00 \x00l\x00a\x00n\x00d\x00s\x00c\x00a\x00p\x00e\x00s\x00,\x00 \x00p\x00o\x00l\x00i\x00s\x00h\x00e\x00d\x00,\x00 \x00s\x00p\x00e\x00c\x00u\x00l\x00a\x00r\x00 \x00l\x00i\x00g\x00h\x00t\x00i\x00n\x00g\x00,\x00\n\x00\n\x00A\x00 \x00p\x00r\x00e\x00t\x00t\x00y\x00 \x00w\x00o\x00m\x00a\x00n\x00 \x00w\x00i\x00t\x00h\x00 \x00a\x00n\x00 \x00a\x00c\x00c\x00e\x00n\x00t\x00u\x00a\x00t\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00 \x00i\x00s\x00 \x00d\x00r\x00e\x00s\x00s\x00e\x00d\x00 \x00i\x00n\x00 \x00a\x00 \x00w\x00h\x00i\x00t\x00e\x00 \x00c\x00h\x00i\x00f\x00f\x00o\x00n\x00 \x00b\x00l\x00o\x00u\x00s\x00e\x00,\x00 \x00w\x00i\x00d\x00e\x00 \x00h\x00i\x00k\x00i\x00n\x00g\x00 \x00s\x00h\x00o\x00r\x00t\x00s\x00 \x00a\x00n\x00d\x00 \x00r\x00o\x00u\x00g\x00h\x00 \x00h\x00i\x00k\x00i\x00n\x00g\x00 \x00b\x00o\x00o\x00t\x00s\x00.\x00 \x00S\x00h\x00e\x00 \x00w\x00e\x00a\x00r\x00s\x00 \x00a\x00 \x00s\x00u\x00m\x00m\x00e\x00r\x00 \x00h\x00a\x00t\x00 \x00a\x00n\x00d\x00 \x00h\x00a\x00s\x00 \x00l\x00o\x00n\x00g\x00 \x00c\x00u\x00r\x00l\x00y\x00 \x00r\x00e\x00d\x00 \x00h\x00a\x00i\x00r\x00.\x00 \x00S\x00h\x00e\x00 \x00h\x00a\x00s\x00 \x00a\x00 \x00b\x00i\x00c\x00y\x00c\x00l\x00e\x00.\x00 \x00T\x00h\x00e\x00 \x00a\x00c\x00t\x00i\x00o\x00n\x00 \x00t\x00a\x00k\x00e\x00s\x00 \x00p\x00l\x00a\x00c\x00e\x00 \x00i\x00n\x00 \x00t\x00h\x00e\x00 \x00t\x00a\x00i\x00g\x00a\x00 \x00f\x00o\x00r\x00e\x00s\x00t\x00.\x00\n\x00\n\x00E\x00x\x00i\x00t\x00:\x00\n\x00-\x00 \x00C\x00h\x00a\x00r\x00a\x00c\x00t\x00e\x00r\x00:\x00\n\x00 \x00 \x00 \x00 \x00-\x00 \x00B\x00o\x00d\x00y\x00:\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00A\x00 \x00d\x00e\x00f\x00i\x00n\x00e\x00d\x00 \x00a\x00n\x00d\x00 \x00t\x00o\x00n\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00T\x00h\x00i\x00n\x00 \x00w\x00a\x00i\x00s\x00t\x00,\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00a\x00t\x00h\x00l\x00e\x00t\x00i\x00c\x00 \x00b\x00o\x00d\x00y\x00,\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00C\x00o\x00n\x00f\x00i\x00d\x00e\x00n\x00c\x00e\x00 \x00i\x00n\x00 \x00y\x00o\x00u\x00r\x00 \x00p\x00o\x00s\x00t\x00u\x00r\x00e\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00L\x00e\x00a\x00n\x00i\x00n\x00g\x00 \x00o\x00n\x00 \x00y\x00o\x00u\x00r\x00 \x00b\x00i\x00k\x00e\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00t\x00o\x00n\x00e\x00d\x00 \x00b\x00o\x00d\x00y\x00\n\x00-\x00 \x00H\x00a\x00i\x00r\x00:\x00\n\x00 \x00 \x00 \x00 \x00C\x00u\x00r\x00l\x00y\x00 \x00h\x00a\x00i\x00r\x00\n\x00 \x00 \x00 \x00 \x00C\x00u\x00r\x00l\x00s\x00 \x00f\x00r\x00a\x00m\x00i\x00n\x00g\x00 \x00h\x00e\x00r\x00 \x00f\x00a\x00c\x00e\x00\n\x00 \x00 \x00 \x00 \x00n\x00e\x00a\x00t\x00 \x00a\x00n\x00d\x00 \x00t\x00i\x00d\x00y\x00,\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00d\x00 \x00h\x00a\x00i\x00r\x00 \x00c\x00o\x00l\x00o\x00r\x00,\x00\n\x00 \x00 \x00 \x00 \x00S\x00u\x00b\x00t\x00l\x00e\x00 \x00h\x00i\x00g\x00h\x00l\x00i\x00g\x00h\x00t\x00s\x00 \x00t\x00h\x00a\x00t\x00 \x00a\x00d\x00d\x00 \x00d\x00e\x00p\x00t\x00h\x00 \x00t\x00o\x00 \x00t\x00h\x00e\x00 \x00s\x00t\x00y\x00l\x00e\x00\n\x00-\x00 \x00F\x00a\x00c\x00e\x00:\x00\n\x00 \x00 \x00 \x00 \x00E\x00u\x00r\x00o\x00p\x00e\x00a\x00n\x00 \x00f\x00a\x00c\x00e\x00 \x00t\x00y\x00p\x00e\x00\n\x00 \x00 \x00 \x00 \x00B\x00r\x00i\x00g\x00h\x00t\x00 \x00s\x00k\x00i\x00n\x00\n\x00 \x00 \x00 \x00 \x00F\x00r\x00e\x00c\x00k\x00l\x00e\x00s\x00\n\x00 \x00 \x00 \x00 \x00G\x00r\x00e\x00e\x00n\x00 \x00e\x00y\x00e\x00s\x00\n\x00-\x00 \x00L\x00o\x00c\x00a\x00t\x00i\x00o\x00n\x00:\x00\n\x00 \x00 \x00 \x00 \x00S\x00i\x00b\x00e\x00r\x00i\x00a\x00n\x00 \x00t\x00a\x00i\x00g\x00a\x00,\x00\n\x00 \x00 \x00 \x00 \x00S\x00u\x00n\x00l\x00i\x00g\x00h\x00t\x00 \x00f\x00i\x00l\x00t\x00e\x00r\x00i\x00n\x00g\x00 \x00t\x00h\x00r\x00o\x00u\x00g\x00h\x00 \x00t\x00h\x00e\x00 \x00n\x00e\x00e\x00d\x00l\x00e\x00s\x00\n\x00 \x00 \x00 \x00 \x00t\x00r\x00e\x00e\x00 \x00c\x00r\x00o\x00w\x00n\x00s\x00 \x00p\x00r\x00o\x00v\x00i\x00d\x00i\x00n\x00g\x00 \x00s\x00h\x00a\x00d\x00e\x00\n\x00 \x00 \x00 \x00 \x00C\x00a\x00l\x00m\x00 \x00a\x00n\x00d\x00 \x00s\x00e\x00r\x00e\x00n\x00e\x00 \x00e\x00n\x00v\x00i\x00r\x00o\x00n\x00m\x00e\x00n\x00t\x00\n\x00N\x00e\x00g\x00a\x00t\x00i\x00v\x00e\x00 \x00p\x00r\x00o\x00m\x00p\x00t\x00:\x00 \x00l\x00o\x00w\x00r\x00e\x00s\x00,\x00 \x00b\x00a\x00d\x00 \x00a\x00n\x00a\x00t\x00o\x00m\x00y\x00,\x00 \x00b\x00a\x00d\x00 \x00h\x00a\x00n\x00d\x00s\x00,\x00 \x00m\x00i\x00s\x00s\x00i\x00n\x00g\x00 \x00f\x00i\x00n\x00g\x00e\x00r\x00s\x00,\x00 \x00(\x00w\x00o\x00r\x00s\x00t\x00 \x00q\x00u\x00a\x00l\x00i\x00t\x00y\x00:\x001\x00.\x004\x00)\x00,\x00 \x00(\x00l\x00o\x00w\x00 \x00q\x00u\x00a\x00l\x00i\x00t\x00y\x00:\x001\x00.\x004\x00)\x00,\x00 \x00s\x00t\x00r\x00a\x00b\x00i\x00s\x00m\x00u\x00s\x00,\x00 \x00c\x00r\x00o\x00s\x00s\x00e\x00d\x00 \x00e\x00y\x00e\x00s\x00,\x00 \x00n\x00s\x00f\x00w\x00,\x00 \x00(\x00f\x00o\x00r\x00e\x00h\x00e\x00a\x00d\x00:\x001\x00.\x003\x00)\x00,\x00 \x00e\x00a\x00s\x00y\x00n\x00e\x00g\x00a\x00t\x00i\x00v\x00e\x00\n\x00S\x00t\x00e\x00p\x00s\x00:\x00 \x003\x000\x00,\x00 \x00S\x00a\x00m\x00p\x00l\x00e\x00r\x00:\x00 \x00D\x00P\x00M\x00+\x00+\x00 \x002\x00M\x00 \x00K\x00a\x00r\x00r\x00a\x00s\x00,\x00 \x00C\x00F\x00G\x00 \x00s\x00c\x00a\x00l\x00e\x00:\x00 \x001\x000\x00.\x000\x00,\x00 \x00S\x00e\x00e\x00d\x00:\x00 \x003\x003\x002\x008\x002\x005\x007\x004\x004\x003\x00,\x00 \x00S\x00i\x00z\x00e\x00:\x00 \x005\x001\x002\x00x\x007\x006\x008\x00,\x00 \x00M\x00o\x00d\x00e\x00l\x00 \x00h\x00a\x00s\x00h\x00:\x00 \x009\x001\x00d\x00c\x004\x008\x000\x006\x004\x000\x00,\x00 \x00M\x00o\x00d\x00e\x00l\x00:\x00 \x00s\x00h\x00a\x00m\x00p\x00o\x00o\x00M\x00i\x00x\x00_\x00v\x003\x000\x00,\x00 \x00S\x00e\x00e\x00d\x00 \x00r\x00e\x00s\x00i\x00z\x00e\x00 \x00f\x00r\x00o\x00m\x00:\x00 \x00-\x001\x00x\x00-\x001\x00,\x00 \x00D\x00e\x00n\x00o\x00i\x00s\x00i\x00n\x00g\x00 \x00s\x00t\x00r\x00e\x00n\x00g\x00t\x00h\x00:\x00 \x000'    

for data in DATA:
    print(data)
    data.decode('utf-8').replace('\x00', '')
    print("-----")
ExifOffset: 26
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[10], line 3
      1 for data in DATA:
      2     print(data)
----> 3     data.decode('utf-8').replace('\x00', '')
      4     print("-----")

AttributeError: 'str' object has no attribute 'decode'
    
    

human_readable_comment = data.decode('utf-8').replace('\x00', '')

for data in DATA:
    print(data)
    data = data.replace('\x00', '')
    print("-----")


# Assuming your data is stored in the variable DATA
data = DATA[1]

# Split the string into lines
lines = data.split('\n')

# Initialize an empty list to store human-readable lines
human_readable_lines = []

# Iterate through the lines and filter out non-human-readable lines
for line in lines:
    if not line.startswith('-') and not line.startswith('Exit:'):
        human_readable_lines.append(line)

# Join the human-readable lines into a single string
human_readable = '\n'.join(human_readable_lines)

# Print the human-readable information
#print(human_readable)
# Assuming your human-readable data is stored in the variable 'human_readable'
utf8_decoded = human_readable.encode('utf-8')

# Convert the UTF-8 bytes back to a string
#utf8_decoded_str = utf8_decoded.decode('utf-8')

# Print the UTF-8 decoded string
print(utf8_decoded_str)





==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/circle_blur_mouth.ipynb
Code Content:
import moviepy.editor as mp
from PIL import Image, ImageDraw, ImageFilter
from skimage.filters import gaussian
import numpy as np

# Define the function to apply blur with a circle mask
def circle_obj_blur(im1):
    im1 = im1.resize((600, 600), Image.BICUBIC)
    im2 = Image.new("RGBA", im1.size, (0, 0, 0, 0))
    mask = Image.new("L", im1.size, 0)
    draw = ImageDraw.Draw(mask)
    
    #draw.ellipse((225, 255, 377, 377), fill=255)
    #draw.ellipse((225, 295, 377, 427), fill=255)
    #draw.ellipse((235, 305, 357, 407), fill=255)
    #draw.ellipse((245, 285, 357, 387), fill=255)
    draw.ellipse((275, 335, 357, 387), fill=255)
    im = Image.composite(im1, im2, mask)
    mask_blur = mask.filter(ImageFilter.GaussianBlur(10))
    im = Image.composite(im1, im2, mask_blur)
    return im

# Load the video clip
clip = mp.VideoFileClip("/home/jack/Desktop/YOUTUBE/resources/output.mkv")

# Initialize an empty list to store the new frames
new_frames = []

# Loop through each frame, apply the function and add the new frame to the list
for frame in clip.iter_frames():
    pil_frame = Image.fromarray(frame)
    processed_frame = circle_obj_blur(pil_frame)
    new_frames.append(np.asarray(processed_frame))  # Convert PIL image to numpy array


from PIL import Image, ImageDraw
# Create a new image with a white background
image = Image.new('RGB', (500, 500), color='white')
# Create a draw object
draw = ImageDraw.Draw(image)
# Draw a red dot at position (100, 100)
dot_size = 4
dot_pos = (100 - dot_size//2, 100 - dot_size//2, 100 + dot_size//2, 100 + dot_size//2)
draw.rectangle(dot_pos, fill='red')
i = 6
draw.ellipse((100, 100, 400-i, 300-i), outline="red")
i=35
draw.ellipse((100, 100, 400-i, 300-i), outline="blue")
h= 10
draw.ellipse((100+h, 100+h, 400-i, 300-i), outline="black")
image

import matplotlib.pyplot as plt
import numpy as np

# Load the image as a numpy array
#img = plt.imread("path/to/image.jpg")

# Display the image
plt.imshow(new_frames[3])
plt.show()


import imageio

# Create the gif using the new_frames list
imageio.mimsave('resources/NEW-new_gif.gif', new_frames, fps=25)


!ffmpeg -i XXXnew_gif.gif XXXnew_gif.mp4

!ls XXXnew_gif.mp4



import moviepy.editor as mp
from PIL import Image, ImageDraw, ImageFilter

def circle_obj_blur(im1):
    im1 = im1.resize((600,600), Image.BICUBIC)
    im2 = Image.new("RGBA",im1.size,(0,0,0,0))
    mask = Image.new("L", im1.size, 0)
    draw = ImageDraw.Draw(mask)
    draw.ellipse((25, 25, 575, 575), fill=255)
    #draw.ellipse((100, 100, 412, 412), fill=250)
    im = Image.composite(im1, im2, mask)
    mask_blur = mask.filter(ImageFilter.GaussianBlur(15))
    im = Image.composite(im1, im2, mask_blur)
    return im

clip = mp.VideoFileClip("/home/jack/Desktop/YOUTUBE/resources/output.mkv")
print(clip.fps)
new_frames = []
cnt = 0
for frame in clip.iter_frames():
    cnt = cnt +1
    pil_frame = Image.fromarray(frame)
    processed_frame = circle_obj_blur(pil_frame)
    new_frames.append(processed_frame)
print(cnt)
print(len(new_frames))
#new_clip = mp.ImageSequenceClip(new_frames, fps=clip.fps)
#new_clip.write_videofile("/home/jack/Desktop/YOUTUBE/resources/Newblur.mp4")


new_clip = mp.ImageSequenceClip(new_frames, fps=clip.fps)
new_clip.write_videofile("/home/jack/Desktop/YOUTUBE/resources/Newblur.mp4")


import moviepy.editor as mp
from PIL import Image, ImageDraw, ImageFilter
from skimage.filters import gaussian

# Define the function to apply blur with a circle mask
def circle_obj_blur(im1):
    im1 = im1.resize((600, 600), Image.BICUBIC)
    im2 = Image.new("RGBA", im1.size, (0, 0, 0, 0))
    mask = Image.new("L", im1.size, 0)
    draw = ImageDraw.Draw(mask)
    draw.ellipse((25, 25, 575, 575), fill=255)
    im = Image.composite(im1, im2, mask)
    mask_blur = mask.filter(ImageFilter.GaussianBlur(15))
    im = Image.composite(im1, im2, mask_blur)
    return im

# Load the video clip
#clip = mp.VideoFileClip("/home/jack/Desktop/YOUTUBE/resources/sample.mp4")
clip = mp.VideoFileClip("/home/jack/Desktop/YOUTUBE/resources/output.mkv")
# Initialize an empty list to store the new frames
new_frames = []

# Loop through each frame, apply the function and add the new frame to the list
for frame in clip.iter_frames():
    pil_frame = Image.fromarray(frame)
    processed_frame = circle_obj_blur(pil_frame)
    new_frames.append(processed_frame)

# Create a new video clip with the processed frames
new_clip = mp.ImageSequenceClip(new_frames, fps=clip.fps)

# Write the new video clip to a file
new_clip.write_videofile("/home/jack/Desktop/YOUTUBE/resources/Newblur.mp4")
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[14], line 30
     27     new_frames.append(processed_frame)
     29 # Create a new video clip with the processed frames
---> 30 new_clip = mp.ImageSequenceClip(new_frames, fps=clip.fps)
     32 # Write the new video clip to a file
     33 new_clip.write_videofile("/home/jack/Desktop/YOUTUBE/resources/Newblur.mp4")

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/io/ImageSequenceClip.py:84, in ImageSequenceClip.__init__(self, sequence, fps, durations, with_mask, ismask, load_images)
     82    size = imread(sequence[0]).shape
     83 else:
---> 84    size = sequence[0].shape
     86 for image in sequence:
     87     image1=image

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/PIL/Image.py:519, in Image.__getattr__(self, name)
    512     warnings.warn(
    513         "Image categories are deprecated and will be removed in Pillow 10 "
    514         "(2023-07-01). Use is_animated instead.",
    515         DeprecationWarning,
    516         stacklevel=2,
    517     )
    518     return self._category
--> 519 raise AttributeError(name)

AttributeError: shape

import moviepy.editor as mp
from PIL import Image, ImageDraw, ImageFilter

def circle_obj_blur(im1):
    im1 = im1.resize((600,600), Image.BICUBIC)
    im2 = Image.new("RGBA",im1.size,(0,0,0,0))
    mask = Image.new("L", im1.size, 0)
    draw = ImageDraw.Draw(mask)
    draw.ellipse((25, 25, 575, 575), fill=255)
    #draw.ellipse((100, 100, 412, 412), fill=250)
    im = Image.composite(im1, im2, mask)
    mask_blur = mask.filter(ImageFilter.GaussianBlur(15))
    im = Image.composite(im1, im2, mask_blur)
    return im

clip = mp.VideoFileClip("/home/jack/Desktop/YOUTUBE/resources/output.mkv")

new_frames = []
for frame in clip.iter_frames():
    pil_frame = Image.fromarray(frame)
    processed_frame = circle_obj_blur(pil_frame)
    new_frames.append(processed_frame)

new_clip = mp.ImageSequenceClip(new_frames, fps=clip.fps)
new_clip.write_videofile("/home/jack/Desktop/YOUTUBE/resources/Newblur.mp4")


from skimage.filters import gaussian
from moviepy.editor import VideoFileClip, clips_array, concatenate_videoclips
import matplotlib.pyplot as plt
import numpy as np
import PIL

def blur(image):
    image = image.convert("RGBA")
    """ Returns a blurred (radius=4 pixels) version of the image """    
    return gaussian(np.array(image).astype(float), sigma=4)

image = PIL.Image.open("/home/jack/Desktop/StoryMaker/static/images/eyes/00009.jpg")
im = blur(image)
im

from PIL import Image,ImageFilter,ImageDraw
def circle_obj_blur(im1):
    w,h =im1.size
    ww,hh =w*1.25,h*1.25
    print(ww,hh)
    im1 = im1.resize((int(ww),int(hh)), Image.BICUBIC)
    im2 = Image.new("RGBA",im1.size,(0,0,0,0))
    mask = Image.new("L", im1.size, 0)
    draw = ImageDraw.Draw(mask)
    #draw.ellipse((75, 75, 875, 875), fill=255)
    draw.ellipse((50, 50, 512, 512), fill=250)
    im = Image.composite(im1, im2, mask)
    mask_blur = mask.filter(ImageFilter.GaussianBlur(15))
    im = Image.composite(im1, im2, mask_blur)
    return im
im1 = Image.open("/home/jack/Desktop/StoryMaker/static/images/eyes/00009.jpg")
circle_obj_blur(im1)



from skimage.filters import gaussian
from moviepy.editor import VideoFileClip, clips_array, concatenate_videoclips
import matplotlib.pyplot as plt

import PIL
def blur(image):
    image = image.convert("RGBA")
    """ Returns a blurred (radius=4 pixels) version of the image """    
    return gaussian(image.astype(float), sigma=4)

image = PIL.Image.open("/home/jack/Desktop/YOUTUBE/resources/Ai_Generated_Image.jpg")
im = blur(image)
im = im.resize((400,400),PIL.Image.BICUBIC)
plt.imshow(im)
plt.show()

AttributeError  
---> 12 im = blur(image)
return gaussian(image.astype(float), sigma=4)
AttributeError: astype

from moviepy.video.compositing.concatenate import concatenate_videoclips
from skimage.filters import gaussian
from moviepy.editor import VideoFileClip, clips_array, concatenate_videoclips

def blur(image):
    """ Returns a blurred (radius=4 pixels) version of the image """    
    return gaussian(image.astype(float), sigma=4)

# open video and resize to 460
clip1 = VideoFileClip("cat1.mp4").subclip(1,2).resize(width=200)
clip2 = VideoFileClip("cat1.mp4").subclip(2,4).resize(width=200)
clip3 = VideoFileClip("cat1.mp4").subclip(4,6).resize(width=200)

# blur clip 2
clip2_blurred = clip2.fl_image( blur )

# final 
final_clip = concatenate_videoclips([clip1, clip2_blurred, clip3])
final_clip.write_gif("blurdemo.gif")

An ellipse is a closed curve that looks like a stretched circle. To draw an ellipse, you need to specify the position and dimensions of its bounding box. The bounding box is a rectangle that surrounds the ellipse, and its width and height determine the size of the ellipse. The top-left corner of the bounding box specifies the position of the ellipse.

To draw an ellipse using the Python pillow library, you first need to create a Draw object from an image. Here's an example:

python
Copy code
from PIL import Image, ImageDraw

# Create a new image with a white background
image = Image.new('RGB', (500, 500), color='white')

# Create a draw object
draw = ImageDraw.Draw(image)
In this example, we create a new image with a white background and a size of 500x500 pixels. Then, we create a Draw object from the image using ImageDraw.Draw().

To draw an ellipse, you can use the ellipse() method of the Draw object. The ellipse() method takes a tuple of four values representing the bounding box of the ellipse, and an optional fill parameter that specifies the color to fill the ellipse with. Here's an example:

css
Copy code
# Draw a white ellipse with a black outline
draw.ellipse((100, 100, 400, 300), outline=0)
In this example, we draw a white ellipse with a black outline. The bounding box of the ellipse is specified as (100, 100, 400, 300), which means that the top-left corner of the bounding box is at position (100, 100), and the width and height of the bounding box are 300 pixels and 200 pixels, respectively.

To locate an ellipse, you can adjust the position of its bounding box. For example, to move an ellipse 20 pixels down, you can add 20 to the y-coordinate of the top-left corner of the bounding box. Here's an example:

python
Copy code
# Draw a white ellipse 20 pixels down from the previous one
draw.ellipse((100, 120, 400, 320), outline=0)
In this example, we moved the ellipse 20 pixels down by adding 20 to the y-coordinate of the top-left corner of the bounding box.

I hope this helps! Let me know if you have any more questions.




Regenerate response

from PIL import Image, ImageDraw

# Create a new image with a white background
image = Image.new('RGB', (500, 500), color='white')

# Create a draw object
draw = ImageDraw.Draw(image)

# Draw a red dot at position (100, 100)
dot_size = 4
dot_pos = (100 - dot_size//2, 100 - dot_size//2, 100 + dot_size//2, 100 + dot_size//2)
draw.rectangle(dot_pos, fill='red')

for i in range(0,100,5):
    draw.ellipse((100, 100, 400-i, 300-i), outline=0)
image

from PIL import Image, ImageDraw
# Create a new image with a white background
image = Image.new('RGB', (500, 500), color='white')
# Create a draw object
draw = ImageDraw.Draw(image)
# Draw a red dot at position (100, 100)
dot_size = 4
dot_pos = (100 - dot_size//2, 100 - dot_size//2, 100 + dot_size//2, 100 + dot_size//2)
draw.rectangle(dot_pos, fill='red')
i = 6
draw.ellipse((100, 100, 400-i, 300-i), outline="red")
i=35
draw.ellipse((100, 100, 400-i, 300-i), outline="blue")
h= 10
draw.ellipse((100+h, 100+h, 400-i, 300-i), outline="black")
image

    #draw.ellipse((225, 255, 377, 377), fill=255)
    #draw.ellipse((225, 295, 377, 427), fill=255)
    #draw.ellipse((235, 305, 357, 407), fill=255)
    #draw.ellipse((245, 285, 357, 387), fill=255)
    draw.ellipse((265, 305, 357, 400), fill=255)

import cv2
import numpy as np

# Load images
img1 = cv2.imread('image1.jpg')
img2 = cv2.imread('image2.jpg')

# Define corresponding points (can use STASM or manual markings)
points1 = np.array([[x1, y1], [x2, y2], ...])
points2 = np.array([[x1, y1], [x2, y2], ...])

# Create morphing function
morph_func = cv2.createThinPlateSplineShapeTransformer()

# Set corresponding points for the morphing function
src_pts = np.expand_dims(points1, axis=0)
dst_pts = np.expand_dims(points2, axis=0)
morph_func.estimateTransformation(dst_pts, src_pts)

# Generate intermediate frames
num_frames = 10
for i in range(num_frames):
    t = (i + 1) / (num_frames + 1)
    inter_points = (1 - t) * points1 + t * points2
    inter_pts = np.expand_dims(inter_points, axis=0)
    inter_frame = morph_func.warpImage(img1, inter_pts)

    # Save intermediate frame as image or add to video
    cv2.imwrite('inter_frame{}.jpg'.format(i), inter_frame)


import cv2
import stasm

# Load image
img = cv2.imread('image.jpg')

# Initialize STASM
stasm.init()

# Detect facial landmarks
landmarks = stasm.search_single(img)

# Display landmarks on the image
for landmark in landmarks:
    cv2.circle(img, (int(landmark[0]), int(landmark[1])), 2, (0, 255, 0), -1)

# Display the image with landmarks
cv2.imshow('Facial Landmarks', img)
cv2.waitKey(0)
cv2.destroyAllWindows()


import cv2
import stasm
from matplotlib import pyplot as plt

# Load image
img = cv2.imread('image.jpg')

# Initialize STASM
stasm.init()

# Detect facial landmarks
landmarks = stasm.search_single(img)

# Display landmarks on the image
for landmark in landmarks:
    cv2.circle(img, (int(landmark[0]), int(landmark[1])), 2, (0, 255, 0), -1)

# Display the image with landmarks using matplotlib
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.show()


import facemorpher
# Get a list of image paths in a folder
imgpaths = facemorpher.list_imgpaths('DarkStorm/')

# To morph, supply an array of face images:
facemorpher.morpher(imgpaths, plot=True)

import facemorpher
import glob
# Get a list of image paths in a folder
#imgpaths = facemorpher.list_imgpaths('DarkStorm/')
imgpaths = facemorpher.list_imgpaths('/home/jack/Desktop/HDD500/collections/newdownloads/')
# To morph, supply an array of face images:
#facemorpher.morpher(imgpaths, plot=True)
#images = glob.glob('DarkStorm/*.jpg')
# To average, supply an array of face images:
#facemorpher.averager(['image1.png', 'image2.png'], plot=True)
#facemorpher.averager(images, plot=True)
facemorpher.averager(imgpaths, plot=True)

from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler

from diffusers import StableDiffusionPipeline
import torch

from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]  
    
image.save("astronaut_rides_horse.png")


from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]  
    
image.save("astronaut_rides_horse.png")

from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler

model_id = "stabilityai/stable-diffusion-2-1"

# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]
    
image.save("astronaut_rides_horse.png")








==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Imports_moviepy-ffmpeg-Copy1.ipynb
Code Content:
import numpy as np
import cv2
import glob 
import random
import datetime
from PIL import Image

def motionvid():
    #filename = random.choice(glob.glob("*.jpg"))
    #basedir="/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/"
    basedir="/home/jack/Desktop/StoryMaker/static/goddess/"
    filename = random.choice(glob.glob(basedir+"*.jpg"))
    im = Image.open(filename).convert("RGB")
    im = im.resize((640,640),Image.BICUBIC)
    im.save("TEMPimg.jpg")
    # Read the image
    img = cv2.imread("TEMPimg.jpg")
    

    # Reshape the image to a 2D array of pixels
    pixel_values = img.reshape((-1, 3))
    # Reshape the image to a 2D array of pixels
    #pixel_values = img.reshape((-1, 3))

    # Convert pixel values to float
    pixel_values = np.float32(pixel_values)

    # Define the number of clusters (colors) to use
    k = 26

    # Define criteria and apply kmeans()
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

    # Convert the centers back to uint8 and reshape the labels back to the original image shape
    centers = np.uint8(centers)
    res = centers[labels.flatten()]
    res2 = res.reshape((img.shape))

    # Apply the wave effect to the image
    h, w = img.shape[:2]
    wave_x = 2 * w
    wave_y = h
    amount_x = 10
    amount_y = 5
    num_frames = 300
    delay = 50
    border_color = (0, 0, 0)
    x = np.arange(w, dtype=np.float32)
    y = np.arange(h, dtype=np.float32)
    frames = []
    for i in range(num_frames):
        phase_x = i * 360 / num_frames
        phase_y = phase_x
        x_sin = amount_x * np.sin(2 * np.pi * (x / wave_x + phase_x / 360)) + x
        map_x = np.tile(x_sin, (h, 1))
        y_sin = amount_y * np.sin(2 * np.pi * (y / wave_y + phase_y / 360)) + y
        map_y = np.tile(y_sin, (w, 1)).transpose()
        result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=border_color)
        frames.append(result)

    # Save the frames as an MP4 video file
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    now = datetime.datetime.now()
    #vidname = "newvid/"+now.strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3] + "new.mp4"
    vidname = "newvid/text.mp4"
    print(vidname)
    out = cv2.VideoWriter(vidname, fourcc, 25.0, (w, h))
    for frame in frames:
        out.write(frame)
    out.release()


import numpy as np
import cv2
import glob 
import random
import datetime
from PIL import Image

def motionvid():
    #filename = random.choice(glob.glob("*.jpg"))
    #basedir="/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/"
    #basedir="/home/jack/Desktop/StoryMaker/static/goddess/"
    basedir="/home/jack/Desktop/StoryMaker/static/goddess/ai-generations_files/"
    filename = random.choice(glob.glob(basedir+"*.jpg"))
    im = Image.open(filename).convert("RGB")
    im = im.resize((512,768),Image.BICUBIC)
    im.save("TEMPimg.jpg")
    # Read the image
    img = cv2.imread("TEMPimg.jpg")
    # Reshape the image to a 2D array of pixels
    pixel_values = img.reshape((-1, 3))
    # Convert pixel values to float
    pixel_values = np.float32(pixel_values)
    # Define the number of clusters (colors) to use
    k = 26
    # Define criteria and apply kmeans()
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

    # Convert the centers back to uint8 and reshape the labels back to the original image shape
    centers = np.uint8(centers)
    res = centers[labels.flatten()]
    res2 = res.reshape((img.shape))

    # Apply the wave effect to the image
    h, w = img.shape[:2]
    #wave_x = 2 * w
    wave_x = 3 * w
    wave_y = h
    #amount_x = 10
    #amount_y = 5
    amount_x = 65
    amount_y = 75
    num_frames = 1000
    delay = 1
    border_color = (0, 0, 0)
    x = np.arange(w, dtype=np.float32)
    y = np.arange(h, dtype=np.float32)
    frames = []
    for i in range(num_frames):
        phase_x = i * 360 / num_frames
        phase_y = phase_x
        x_sin = amount_x * np.sin(2 * np.pi * (x / wave_x + phase_x / 360)) + x
        map_x = np.tile(x_sin, (h, 1))
        y_sin = amount_y * np.sin(2 * np.pi * (y / wave_y + phase_y / 360)) + y
        map_y = np.tile(y_sin, (w, 1)).transpose()
        result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=border_color)
        frames.append(result)

    # Save the frames as an MP4 video file
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    now = datetime.datetime.now()
    #vidname = "newvid/"+now.strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3] + "new.mp4"
    vidname = "newvid/text2.mp4"
    print(vidname)
    out = cv2.VideoWriter(vidname, fourcc, 25.0, (w, h))
    for frame in frames:
        out.write(frame)
    out.release()


motionvid()
!vlc newvid/text2.mp4

!pwd

!vlc newvid/text.mp4

# Good working with fade transition
import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=1), 
                               clip2.fx(vfx.fadein, duration=1)], 
                              size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshowT.mp4')


!pwd

from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip

def add_title_image(video_path, title_image_path, output_path):
    # Load the video file and title image
    video_clip = VideoFileClip(video_path)
    title_image = ImageClip(title_image_path)

    # Set the duration of the title image
    title_duration = video_clip.duration
    title_image = title_image.set_duration(title_duration)

    # Position the title image at the center and resize it to fit the video dimensions
    title_image = title_image.set_position(("center", "center"))
    title_image = title_image.resize(video_clip.size)

    # Create a composite video clip with the title image overlay
    composite_clip = CompositeVideoClip([video_clip, title_image])

    # Set the audio of the composite clip to the original video's audio
    composite_clip = composite_clip.set_audio(video_clip.audio)

    # Export the final video with the title image
    composite_clip.write_videofile(output_path)

# Example usage
video_path = "static/alice/Final_End.mp4"
title_image_path = "static/current_project/Images/Title_Image.png"
output_path = "static/alice/Titled_Final_End.mp4"

add_title_image(video_path, title_image_path, output_path)


/home/jack/Downloads/saved_pages/mine-new/0001_Shot.jpg

https://github.com/Zulko/moviepy
https://github.com/fujiawei-dev/ffmpeg-generator 
https://gfycat.com/create
https://man.archlinux.org/man/ffmpeg-filters.1.en
https://trac.ffmpeg.org/wiki/Capture/ALSA

https://www.mltframework.org/
https://ffmpeg.org/ffmpeg-filters.html#concat
https://github.com/mltframework/mlt/releases
https://copyprogramming.com/howto/nlmeans-ffmpeg-is-it-really-so-slow
https://snyk.io/advisor/npm-package/fluent-ffmpeg/functions/fluent-ffmpeg

!cp /home/jack/Downloads/saved_pages/mine-new/Title.png .

!ls *.mp4

import random
TITLE = random.choice(["Title.jpg","Title.png","title.jpg"])
TITLE

import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips, ImageClip, vfx

# Set the duration of the transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 15)

# Load the title image as an ImageClip object and set its duration
TITLE = random.choice(["Title.jpg","Title.png","title.jpg"])
title_image = ImageClip(TITLE).set_duration(2)

# Load each selected file as a VideoFileClip object and add a transition
clips = [title_image]
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition_duration = min(transition_duration, clip.duration)
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),
                                         clip.set_start(transition_duration)])
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("animated-joined_02a.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 6)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition_duration = min(transition_duration, clip.duration)
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),
                                         clip.set_start(transition_duration)])
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("animation_out_A.mp4")


from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "shortsoutput01.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="400" height="400" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


from vidpy import Clip, Composition
from vidpy import Clip, Composition

#clip1 = Clip('animated2.mp4', offset=1.5)# start playing clip one after 1.5 seconds
#clip2 = Clip('animated.mp4')
#clip2.set_offset(5) # start clip2 after 5 seconds

#clip1 = Clip('0030short.mp4')
#clip2 = Clip('0046short.mp4')

clip1 = Clip('0030short.mp4')
clip2 = Clip('0046short.mp4', offset=3)
#clip2.set_offset(65)
# play videos on top of each other
composition = Composition([clip1, clip2])
composition.save('compose01.mp4')

from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "compose01.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="400" height="400" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


!pwd



!vlc newvid/2023-07-15_19-56-27-193new.mp4

for i in range(0,50):
    motionvid()

!ls newvid

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

# Get a list of all the MP4 files in the junk directory
file_list = glob.glob("newvid/*new.mp4")

# Choose 20 random files from the file list
selected_files = random.sample(file_list, 15)

# Create a list of VideoFileClip objects from the selected files
clip_list = [VideoFileClip(filename) for filename in selected_files]

# Concatenate the video clips together into a single clip
final_clip = concatenate_videoclips(clip_list)

# Write the final clip to a file named JOINED.mp4
final_clip.write_videofile("newvid/start.mp4")


!ls /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/junk

basedir="/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/"
filename = random.choice(glob.glob(basedir+"junk/*__.png"))

ffmpeg -i input.lowfps.hevc -filter "minterpolate='fps=120'" output.120fps.hevc


!locate *.mp4 |grep collections

!ffmpeg -i /mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4 \
-filter:v "minterpolate='fps=120'" -y output.120fps.mp4

!ls *.jpg

import subprocess

# Define the command as a list of strings
command = ['ffmpeg', '-y', '-r', '0.3', '-stream_loop', '1', '-i', '2023-04-13-10-15-59.jpg', '-r', '0.3', '-stream_loop', '2', '-i', '2023-04-13-15-21-31.jpg', '-filter_complex', '[0][1]concat=n=2:v=1:a=0[v];[v]minterpolate=fps=24:scd=none,trim=3:7,setpts=PTS-STARTPTS', '-pix_fmt', 'yuv420p', '-y','test02.mp4']

# Run the command using subprocess.Popen
process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# Get the output and error messages (if any)
stdout, stderr = process.communicate()

# Print the output and error messages
print(stdout.decode('utf-8'))
print(stderr.decode('utf-8'))


!ffmpeg -y -r 0.3 -stream_loop 1 -i 2023-04-13-10-15-59.jpg -r 0.3 -stream_loop 2 -i \ 2023-04-13-15-21-31.jpg -filter_complex "[0][1]concat=n=2:v=1:a=0[v]; \[v]minterpolate=fps=24:scd=none,trim=3:7,setpts=PTS-STARTPTS" -pix_fmt yuv420p \ test02.mp4

from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "output.120fps.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="400" height="400" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


from IPython.display import HTML
from base64 import b64encode

# Replace 'myvideo.mp4' with the path to your own video file
#video_path = "/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4"
video_path = "output.120fps.mp4"
# Read the video file and encode it as base64
video_data = open(video_path, 'rb').read()
base64_data = b64encode(video_data).decode('ascii')

# Embed the video in an HTML5 video element
video_html = f"""
<video width="640" height="640" controls>
  <source src="data:video/mp4;base64,{base64_data}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


from IPython.display import HTML

# Replace 'myvideo.mp4' with the path to your own video file
video_path = '/mnt/HDD500/collections/square_videos/2023-04-03_20-47-11_output.mp4'

# Embed the video player in an HTML object
video_html = f"""
<video width=500 controls>
    <source src="{video_path}" type="video/mp4">
</video>
"""

# Display the HTML object in the notebook
HTML(video_html)


ffmpeg -i input.lowfps.hevc -filter "minterpolate='fps=120'" output.120fps.hevc


ffmpeg -y -fflags +genpts -r 30 -i $input01 -vf "setpts=100*PTS,minterpolate=fps=24:scd=none" -pix_fmt yuv420p "test01.mp4"

We received your request for an API key from us here at Gfycat.  Please find your credentials below:

App name: experimental
Client ID: 2_dVjMKA
Client Secret: zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw

Some cool capabilities of the API include:

     Pulling in trending GIFs and categories of GIFs
     Pulling in GIFs via search from Gfycat
     Allowing you or your users to upload and create GIFs
     Allowing you or your users to upload or pull GIFs from their or other users accounts on Gfycat

!ls *.gif

from gfycat.client import GfycatClient
import requests

client_id = "2_dVjMKA"
client_secret = "zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw"

# Construct the API endpoint URL for testing credentials
test_url = f"https://api.gfycat.com/v1/me?client_id={client_id}&client_secret={client_secret}"

# Send a GET request to the test URL
response = requests.get(test_url)

# Check the response status code
if response.status_code == 200:
    print("Credentials are valid!")
else:
    print("Invalid credentials. Please check your client ID and secret.")


from gfycat.client import GfycatClient

client = GfycatClient("2_dVjMKA","zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw")

# Example request
client.upload_from_file('animation.gif')

---------------------------------------------------------------------------
GfycatClientError                         Traceback (most recent call last)
Cell In[25], line 6
      3 client = GfycatClient("2_dVjMKA","zH2l3rvGIshCSbPLXm8LM15J4vGlUNQlkVe9uRzwfhAnUDjTkCDf9gA9qJysRDIw")
      5 # Example request
----> 6 client.upload_from_file('animation.gif')

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/gfycat/client.py:61, in GfycatClient.upload_from_file(self, filename)
     58 r = requests.post(FILE_UPLOAD_ENDPOINT, data=data, files=files)
     60 if r.status_code != 200:
---> 61     raise GfycatClientError('Error uploading the GIF', r.status_code)
     63 info = self.uploaded_file_info(key)
     64 while 'timeout' in info.get('error', '').lower():

GfycatClientError: (403) Error uploading the GIF


!pwd

import numpy as np
import cv2
from PIL import Image
import glob 1  
import random
import datetime


#img = cv2.imread("/home/jack/Desktop/HDD500/collections/newdownloads/512x512/2023-04-13-11-26-06.jpg")
#import cv2
#import numpy as np

# Load the image
filename = random.choice(glob.glob("*.jpg"))
img = cv2.imread(filename)

# Reshape the image to a 2D array of pixels
pixel_values = img.reshape((-1, 3))

# Convert pixel values to float
pixel_values = np.float32(pixel_values)

# Define the number of clusters (colors) to use
k = 26

# Define criteria and apply kmeans()
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# Convert the centers back to uint8 and reshape the labels back to the original image shape
centers = np.uint8(centers)
res = centers[labels.flatten()]
res2 = res.reshape((img.shape))

img = res2
# get dimensions
h, w = img.shape[:2]

# set wavelength
wave_x = 2*w
wave_y = h

# set amount, number of frames and delay
amount_x = 10
amount_y = 5
num_frames = 100
delay = 50
border_color = (128,128,128)

# create X and Y ramps
x = np.arange(w, dtype=np.float32)
y = np.arange(h, dtype=np.float32)

frames = []
# loop and change phase
for i in range(0,num_frames):

    # compute phase to increment over 360 degree for number of frames specified so makes full cycle
    phase_x = i*360/num_frames
    phase_y = phase_x

    # create sinusoids in X and Y, add to ramps and tile out to fill to size of image
    x_sin = amount_x * np.sin(2 * np.pi * (x/wave_x + phase_x/360)) + x
    map_x = np.tile(x_sin, (h,1))

    y_sin = amount_y * np.sin(2 * np.pi * (y/wave_y + phase_y/360)) + y
    map_y = np.tile(y_sin, (w,1)).transpose()

    # do the warping using remap
    result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_CUBIC, borderMode = cv2.BORDER_CONSTANT, borderValue=border_color)
        
    # show result
    #cv2.imshow('result', result)
    #cv2.waitKey(delay)

    # convert to PIL format and save frames
    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    pil_result = Image.fromarray(result)
    frames.append(pil_result)

# write animated gif from frames using PIL
frames[0].save('animation.gif',save_all=True, append_images=frames[1:], optimize=False, duration=delay, loop=0)


!pwd

import numpy as np
import cv2
from PIL import Image

img = cv2.imread("/home/jack/Desktop/HDD500/collections/newdownloads/512x512/2023-04-13-11-27-05.jpg")

# Reshape the image to a 2D array of pixels
pixel_values = img.reshape((-1, 3))

# Convert pixel values to float
pixel_values = np.float32(pixel_values)

# Define the number of clusters (colors) to use
k = 26

# Define criteria and apply kmeans()
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# Convert the centers back to uint8 and reshape the labels back to the original image shape
centers = np.uint8(centers)
res = centers[labels.flatten()]
res2 = res.reshape((img.shape))

img = res2
# get dimensions
h, w = img.shape[:2]

# set wavelength
wave_x = 2*w
wave_y = h

# set amount, number of frames and delay
amount_x = 10
amount_y = 5
num_frames = 100
delay = 50
border_color = (128,128,128)

# create X and Y ramps
x = np.arange(w, dtype=np.float32)
y = np.arange(h, dtype=np.float32)

frames = []
# loop and change phase
for i in range(0,num_frames):

    # compute phase to increment over 360 degree for number of frames specified so makes full cycle
    phase_x = i*360/num_frames
    phase_y = phase_x

    # create sinusoids in X and Y, add to ramps and tile out to fill to size of image
    x_sin = amount_x * np.sin(2 * np.pi * (x/wave_x + phase_x/360)) + x
    map_x = np.tile(x_sin, (h,1))

    y_sin = amount_y * np.sin(2 * np.pi * (y/wave_y + phase_y/360)) + y
    map_y = np.tile(y_sin, (w,1)).transpose()

    # do the warping using remap
    result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_CUBIC, borderMode = cv2.BORDER_CONSTANT, borderValue=border_color)
    # convert to PIL format and save frames
    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    pil_result = Image.fromarray(result)
    frames.append(pil_result)

# write animated gif from frames using PIL
frames[0].save('animation2.gif',save_all=True, append_images=frames[1:], optimize=False, duration=delay, loop=0)


import numpy as np
import cv2
import glob 
import random
import datetime
#img = cv2.imread("/home/jack/Desktop/HDD500/collections/newdownloads/512x512/2023-04-13-11-26-06.jpg")
#import cv2
#import numpy as np

# Load the image
filename = random.choice(glob.glob("*.jpg"))
# Read the image
img = cv2.imread(filename)

# Reshape the image to a 2D array of pixels
pixel_values = img.reshape((-1, 3))

# Convert pixel values to float
pixel_values = np.float32(pixel_values)

# Define the number of clusters (colors) to use
k = 26

# Define criteria and apply kmeans()
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# Convert the centers back to uint8 and reshape the labels back to the original image shape
centers = np.uint8(centers)
res = centers[labels.flatten()]
res2 = res.reshape((img.shape))

# Apply the wave effect to the image
h, w = img.shape[:2]
wave_x = 2 * w
wave_y = h
amount_x = 10
amount_y = 5
num_frames = 100
delay = 50
border_color = (128, 128, 128)
x = np.arange(w, dtype=np.float32)
y = np.arange(h, dtype=np.float32)
frames = []
for i in range(num_frames):
    phase_x = i * 360 / num_frames
    phase_y = phase_x
    x_sin = amount_x * np.sin(2 * np.pi * (x / wave_x + phase_x / 360)) + x
    map_x = np.tile(x_sin, (h, 1))
    y_sin = amount_y * np.sin(2 * np.pi * (y / wave_y + phase_y / 360)) + y
    map_y = np.tile(y_sin, (w, 1)).transpose()
    result = cv2.remap(img.copy(), map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=border_color)
    frames.append(result)

# Save the frames as an MP4 video file
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
now = datetime.datetime.now()
vidname = now.strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3] + ".mp4"
out = cv2.VideoWriter(vidname, fourcc, 25.0, (w, h))
for frame in frames:
    out.write(frame)
out.release()


from vidpy import config
config.MELT_BINARY = '/path/to/melt'

!which melt

from vidpy import Clip, Composition
# start playing clip one after 1.5 seconds
clip1 = Clip('animated2.mp4', offset=1.5)

clip2 = Clip('animated.mp4')
clip2.set_offset(5) # start clip2 after 5 seconds

composition = Composition([clip1, clip2])
composition.save('output2.mp4')

from ffmpeg import _filters
print(_filters.__doc__)

from ffmpeg import _filters
help(_filters)

import ffmpeg
help(ffmpeg)

import ffmpeg
help(ffmpeg)

import ffmpeg
dir(ffmpeg)

import subprocess

import numpy as np

from ffmpeg import constants, FFprobe, input, settings
from tests import data

from ffmpeg import zoompan
help(zoompan)

from ffmpeg import input_file



import ffmpeg

input_file = '/home/jack/Desktop/HDD500/to-vid/building/01145.jpg'
output_file = 'test.mp4'
zoompan_filter = 'zoompan=z=\'min(zoom+0.0015,1.5)\':d=700:x=\'if(gte(zoom,1.5),x,x+1/a)\':y=\'if(gte(zoom,1.5),y,y+1)\':s=640x640'

(
    ffmpeg
    .input(input_file, loop=1)
    .filter(zoompan_filter, duration=10)
    .output(output_file, vcodec='libx264', pix_fmt='yuv420p')
    .run()
)


!pwd

# Works Good 

!ffmpeg -i /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/2023-04-20_00-06-43-983new.mp4 -vf "zoompan=z='min(max(zoom,pzoom)+0.0025,1.5)':d=300:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'" -t 180 -y /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/test3.mp4


!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/test3.mp4 -vf "zoompan=z='min(max(zoom,pzoom)+0.0025,1.5)':d=125:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'" -y /home/jack/Desktop/HDD500/to-vid/building/test2.mp4


!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/test3.mp4 -vf "zoompan=z='if(between(in_time,0,1),2,1)':d=1:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'" -y /home/jack/Desktop/HDD500/to-vid/building/test2.mp4

!mkdir steampunk

# Works Good 
!ffmpeg -i /home/jack/Desktop/StoryMaker/static/images/steampunk/personal-feed_files/00009.jpg -vf "zoompan=z='min(zoom+0.0015,1.5)':d=60000:x='if(gte(zoom,1.5),x,x+1/a)':y='if(gte(zoom,1.5),y,y+1)':s=768x512" -t 180 -y steampunk/test3.mp4

!vlc steampunk/test3.mp4

# Works Good 
!ffmpeg -i /home/jack/Desktop/StoryMaker/static/images/steampunk/personal-feed_files/00009.jpg -vf "zoompan=z='min(zoom+0.0015,1.5)':d=700:x='if(gte(zoom,1.5),x,x+1/a)':y='if(gte(zoom,1.5),y,y+1)':s=768x512" -y steampunk/test4.mp4

# Works Good 
!ffmpeg -i /home/jack/Desktop/HDD500/to-vid/building/01145.jpg -vf "zoompan=z='min(zoom+0.0015,1.5)':d=700:x='if(gte(zoom,1.5),x,x+1/a)':y='if(gte(zoom,1.5),y,y+1)':s=640x640" -y /home/jack/Desktop/HDD500/to-vid/building/test3.mp4

import ffmpeg
dir(ffmpeg)
['Error',
 'Stream',
 '__all__',
 '__builtins__',
 '__cached__',
 '__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__',
 '_ffmpeg',
 '_filters',
 '_probe',
 '_run',
 '_utils',
 '_view',
 'colorchannelmixer',
 'compile',
 'concat',
 'crop',
 'dag',
 'drawbox',
 'drawtext',
 'filter',
 'filter_',
 'filter_multi_output',
 'get_args',
 'hflip',
 'hue',
 'input',
 'merge_outputs',
 'nodes',
 'output',
 'overlay',
 'overwrite_output',
 'probe',
 'run',
 'run_async',
 'setpts',
 'trim',
 'unicode_literals',
 'vflip',
 'view',
 'zoompan']

import subprocess

import numpy as np

from ffmpeg import constants, FFprobe, input, settings
from tests import data

settings.CUDA_ENABLE = False


def ffmpeg_input_process(src):
    return input(src).output(constants.PIPE, format="rawvideo",
                             pixel_format="rgb24").run_async(pipe_stdout=True)


def ffmpeg_output_process(dst, width, height):
    return input(constants.PIPE, format="rawvideo", pixel_format="rgb24",
                 width=width, height=height).output(dst, pixel_format="yuv420p"). \
        run_async(pipe_stdin=True)


def read_frame_from_stdout(process: subprocess.Popen, width, height):
    frame_size = width * height * 3
    input_bytes = process.stdout.read(frame_size)

    if not input_bytes:
        return

    assert len(input_bytes) == frame_size

    return np.frombuffer(input_bytes, np.uint8).reshape([height, width, 3])


def process_frame_simple(frame):
    # deep dream
    return frame * 0.3


def write_frame_to_stdin(process: subprocess.Popen, frame):
    process.stdin.write(frame.astype(np.uint8).tobytes())


def run(src, dst, process_frame):
    width, height = FFprobe(src).video_scale

    input_process = ffmpeg_input_process(src)
    output_process = ffmpeg_output_process(dst, width, height)

    while True:
        input_frame = read_frame_from_stdout(input_process, width, height)

        if input_frame is None:
            break

        write_frame_to_stdin(output_process, process_frame(input_frame))

    input_process.wait()

    output_process.stdin.close()
    output_process.wait()


if __name__ == '__main__':
    run("slideshow.mp4", "processed_slideshow.mp4", process_frame)


!ls process_frame.mp4

import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 12)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(f) for f in selected_files]

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("12output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the transition in seconds
transition_duration = 1

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 15)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),
                                         clip.set_start(transition_duration)],
                                        duration=transition_duration)
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("15trans_output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the transition in seconds
transition_duration = 1
# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 14)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        transition_duration = min(transition_duration, clip.duration)
        transition = CompositeVideoClip([clips[-1].set_end(clips[-1].duration-transition_duration),clip.set_start(transition_duration)])
        clips.append(transition)
        clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("15output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, concatenate_videoclips, CompositeVideoClip

# Set the duration of the cross-fade transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 8)

# Load each selected file as a VideoFileClip object and add a transition
clips = []
for i, file in enumerate(selected_files):
    clip = VideoFileClip(file)
    if i > 0:
        # Determine the duration of the transition
        transition_duration = min(transition_duration, clip.duration, clips[i-1].duration)
        # Create a cross-fade transition
        transition = CompositeVideoClip([clips[i-1].set_end(clips[i-1].duration-transition_duration),
                                         clip.set_start(transition_duration)])
        clips.append(transition)
    clips.append(clip)

# Concatenate the clips into one video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile("blend-shorts-8-output.mp4")


import os
import random
from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the cross-fade transition in seconds
transition_duration = 2

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 8)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(file) for file in selected_files]

# Add cross-fade transitions between the clips
transitions = [None] * (len(clips) - 1)
for i in range(len(transitions)):
    transition_out = clips[i].crossfadeout(transition_duration)
    transition_in = clips[i+1].crossfadein(transition_duration)
    transition = CompositeVideoClip([transition_out, transition_in])
    transitions[i] = transition

# Concatenate the clips and transitions into one video
final_clip = concatenate_videoclips([clips[0]] + transitions + clips[1:])

# Write the final video to a file
final_clip.write_videofile("test-output.mp4")


from PIL import Image
import os
from moviepy.editor import ImageClip, concatenate_videoclips

# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"
# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Print the list of filtered images
#print(random.sample(filtered_images,20))

# Sort the filtered images alphabetically
filtered_images =random.sample(filtered_images,30)

# Sort the filtered images alphabetically
print(len(filtered_images))

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(filtered_images)-1):
    clip1 = ImageClip(filtered_images[i], duration=1)
    clip2 = ImageClip(filtered_images[i+1], duration=1)
    cross_fade = clip2.crossfadein(1)
    clips.append(clip1.crossfadeout(1).set_duration(2) )
    clips.append(cross_fade.set_duration(2))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/splatteroutput.mp4", fps=24, audio=False)


from PIL import Image
import os
import random
from moviepy.editor import ImageClip, concatenate_videoclips, CompositeVideoClip

# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"

# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Sort the filtered images alphabetically
filtered_images.sort()

# Randomly select 30 images from the filtered list
selected_images = random.sample(filtered_images, 30)

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(selected_images)-1):
    clip1 = ImageClip(selected_images[i], duration=1)
    clip2 = ImageClip(selected_images[i+1], duration=1)
    cross_fade = CompositeVideoClip([clip1, clip2.set_start(1).crossfadein(1)])
    clips.append(cross_fade.set_duration(2))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/longsplatteroutput2.mp4", fps=24, audio=False)


from PIL import Image
import os
import random
from moviepy.editor import ImageClip, concatenate_videoclips

# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"

# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Sort the filtered images alphabetically
filtered_images.sort()

# Randomly select 30 images from the filtered list
selected_images = random.sample(filtered_images, 30)

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(selected_images)-1):
    clip1 = ImageClip(selected_images[i], duration=1)
    clip2 = ImageClip(selected_images[i+1], duration=1)
    cross_fade = clip2.crossfadein(1)
    clips.append(clip1.crossfadeout(1).set_duration(2) )
    clips.append(cross_fade.set_duration(2))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/longsplatteroutput3.mp4", fps=24, audio=False)


# Good Video Slideshow no Effects
from PIL import Image
import os
import random
from moviepy.editor import ImageClip, concatenate_videoclips

from moviepy.editor import VideoFileClip, CompositeVideoClip, concatenate_videoclips
# Set the directory path to search for images
directory = os.getcwd()
directory = "/home/jack/Downloads/saved_pages/mine-new"
# Get a list of all image files in the directory
image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png') or f.endswith('Shot.jpg')]

# Filter the images by size
filtered_images = []
for file_path in image_files:
    with Image.open(file_path) as img:
        if img.size == (512, 1024):
            filtered_images.append(file_path)

# Print the list of filtered images
#print(random.sample(filtered_images,20))

# Sort the filtered images alphabetically
filtered_images =random.sample(filtered_images,30)

# Create a list of clips from the images with a crossfade transition
clips = []
for i in range(len(filtered_images)-1):
    clip1 = ImageClip(filtered_images[i], duration=1)
    clip2 = ImageClip(filtered_images[i+1], duration=1)
    cross_fade = clip2.crossfadein(1)
    clips.append(clip1.crossfadeout(1).set_duration(1) )
    clips.append(cross_fade.set_duration(1))

# Concatenate the clips into a single video
final_clip = concatenate_videoclips(clips)

# Write the final video to a file
final_clip.write_videofile(directory+"/Shot01.mp4", fps=24, audio=False)


import os
import random
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the cross-fade transition in seconds
transition_duration = 1

# Set the duration of the title image in seconds
title_duration = 2

# Load the title image as an ImageClip object
title_image = ImageClip("Title.jpg", duration=title_duration)

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('animated.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 7)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(file) for file in selected_files]

# Add cross-fade transitions between the clips
transitions = [None] * (len(clips) - 1)
for i in range(len(transitions)):
    transition_out = clips[i].crossfadeout(transition_duration)
    transition_in = clips[i+1].crossfadein(transition_duration)
    transition = CompositeVideoClip([transition_out, transition_in])
    transitions[i] = transition

# Concatenate the title image and the final video
title_video = concatenate_videoclips([title_image])
final_video = concatenate_videoclips([title_video, clips[0]] + transitions + clips[1:])

# Write the final video to a file
final_video.write_videofile("test-output.mp4")


#WORKS GREAT
import os
import random
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips

# Set the duration of the cross-fade transition in seconds
transition_duration = 1

# Set the duration of the title image in seconds
title_duration = 2

# Load the title image as an ImageClip object
title_image = ImageClip('title.jpg', duration=title_duration)

# Get a list of all the .mp4 files in the current directory
mp4_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith('hot.mp4')]

# Choose 10 random files from the list
selected_files = random.sample(mp4_files, 6)

# Load each selected file as a VideoFileClip object
clips = [VideoFileClip(file) for file in selected_files]

# Add cross-fade transitions between the clips
transitions = [None] * (len(clips) - 1)
for i in range(len(transitions)):
    transition_out = clips[i].crossfadeout(transition_duration)
    transition_in = clips[i+1].crossfadein(transition_duration)
    transition = CompositeVideoClip([transition_out, transition_in])
    transitions[i] = transition

# Calculate the transition for the last two clips separately
last_transition_out = clips[-2].crossfadeout(transition_duration)
last_transition_in = clips[-1].crossfadein(transition_duration)
last_transition = CompositeVideoClip([last_transition_out, last_transition_in])

# Concatenate the title image, the clips, and the transitions into one video
title_video = concatenate_videoclips([title_image])
final_video = concatenate_videoclips([title_video, clips[0]])
for i in range(len(transitions)):
    final_video = concatenate_videoclips([final_video, transitions[i], clips[i+1]])
final_video = concatenate_videoclips([final_video, last_transition, clips[-1]])

# Write the final video to a file
final_video.write_videofile('Post_to_YouTube.mp4')


from moviepy.editor import *

# Load video
video = VideoFileClip("video.mp4")

# Load sound
sound = AudioFileClip("sound.mp3")

# Set sound duration to match video duration
sound = sound.set_duration(video.duration)

# Fade in sound for 1 second and out for 2 seconds
sound = sound.fadein(1).fadeout(2)

# Combine video and sound
video_with_sound = video.set_audio(sound)

# Write output video with sound
video_with_sound.write_videofile("output.mp4")


!ls ../../Music/Music_for_Creators.mp4

!ls ../../

import random
from moviepy.editor import *
from moviepy.audio.fx.all import *

# Load video 
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Get a random start time for the sound between 15 and 25 minutes
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in sound for 1 second and out for 2 seconds
#sound = sound.fx(afx.fade_in, 1).fx(afx.fade_out, 2)
sound = sound.fx(afx.audio_fadein, 1).fx(afx.audio_fadeout, 2)

# Set the end time of the sound to match the duration of the video
sound = sound.set_end(video.duration)

# Add audio to video
video = video.set_audio(sound)

# Write the output video file
video.write_videofile("animated-with_sound_and_transition.mp4")


import random
from moviepy.editor import *
from moviepy.editor import VideoFileClip, AudioFileClip
# Load video 
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Get a random start time for the sound between 15 and 25 minutes
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in sound for 1 second and out for 2 seconds
sound = sound.fadein(1).fadeout(2)

# Set the end time of the sound to match the duration of the video
sound = sound.set_end(video.duration)

# Add audio to video
video = video.set_audio(sound)

# Write the output video file
video.write_videofile("output.mp4")
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[41], line 17
     14 sound = sound.subclip(start_time)
     16 # Fade in sound for 1 second and out for 2 seconds
---> 17 sound = sound.fadein(1).fadeout(2)
     19 # Set the end time of the sound to match the duration of the video
     20 sound = sound.set_end(video.duration)

AttributeError: 'AudioFileClip' object has no attribute 'fadein'


from moviepy.editor import *
from moviepy.editor import VideoFileClip, AudioFileClip
import random

# Load video and audio files
# Load video
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Get a random start time between 15 and 25 minutes into the sound clip
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in audio for 1 second and out for 2 seconds
sound = sound.audio_fadein(1).audio_fadeout(2)

# Add audio to video
video = video.set_audio(sound)

# Write the output video file
video.write_videofile("output.mp4")




from moviepy.editor import *
import random

# Load video
video = VideoFileClip("animated-no-sound-no-transition.mp4")

# Load sound
sound = AudioFileClip("../../Music/Music_for_Creators.mp4")

# Set sound duration to match video duration
sound = sound.set_duration(video.duration)

# Get a random start time between 15 and 25 minutes from the beginning of the sound file
start_time = random.uniform(900, 1500)

# Trim sound to start at the random time
sound = sound.subclip(start_time)

# Fade in sound for 1 second and out for 2 seconds
sound = sound.fadein(1).fadeout(2)

# Combine video and sound
video_with_sound = video.set_audio(sound)

# Write output video with sound
video_with_sound.write_videofile("DEMO_for_youtube.mp4")


# %load /usr/local/bin/SuperEffect
#!/bin/bash

# Print instructions for the user
echo "This script will process a video file located in the current directory."
echo "Please ensure that the 'start.mp4' file exists in this directory before proceeding."
echo "Press ENTER to continue, or CTRL+C to cancel."
read

# Check if start.mp4 exists in the current directory
if [ ! -f "start.mp4" ]; then
  echo "Error: 'start.mp4' file not found in current directory."
  exit 1
fi

# Process the video file
ffmpeg -i "$(pwd)/start.mp4" -crf 10 -vf 'minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1' ---"$(pwd)/TEMPP.mp4"
sleep 1000
ffmpeg -i "$(pwd)/TEMPP.mp4" -vf mpdecimate,setpts=N/FRAME_RATE/TB -map:v 0 -y "$(pwd)/SUPER_EFFECT_Output.mkv"

# Remove the temporary file
rm "$(pwd)/TEMPP.mp4"


melt
Usage: melt [options] [producer [name=value]* ]+
Options:
  -attach filter[:arg] [name=value]*       Attach a filter to the output
  -attach-cut filter[:arg] [name=value]*   Attach a filter to a cut
  -attach-track filter[:arg] [name=value]* Attach a filter to a track
  -attach-clip filter[:arg] [name=value]*  Attach a filter to a producer
  -audio-track | -hide-video               Add an audio-only track
  -blank frames                            Add blank silence to a track
  -consumer id[:arg] [name=value]*         Set the consumer (sink)
  -debug                                   Set the logging level to debug
  -filter filter[:arg] [name=value]*       Add a filter to the current track
  -getc                                    Get keyboard input using getc
  -group [name=value]*                     Apply properties repeatedly
  -help                                    Show this message
  -jack                                    Enable JACK transport synchronization
  -join clips                              Join multiple clips into one cut
  -mix length                              Add a mix between the last two cuts
  -mixer transition                        Add a transition to the mix
  -null-track | -hide-track                Add a hidden track
  -profile name                            Set the processing settings
  -progress                                Display progress along with position
  -query                                   List all of the registered services
  -query "consumers" | "consumer"=id       List consumers or show info about one
  -query "filters" | "filter"=id           List filters or show info about one
  -query "producers" | "producer"=id       List producers or show info about one
  -query "transitions" | "transition"=id   List transitions, show info about one
  -query "profiles" | "profile"=id         List profiles, show info about one
  -query "presets" | "preset"=id           List presets, show info about one
  -query "formats"                         List audio/video formats
  -query "audio_codecs"                    List audio codecs
  -query "video_codecs"                    List video codecs
  -remove                                  Remove the most recent cut
  -repeat times                            Repeat the last cut
  -repository path                         Set the directory of MLT modules
  -serialise [filename]                    Write the commands to a text file
  -silent                                  Do not display position/transport
  -split relative-frame                    Split the last cut into two cuts
  -swap                                    Rearrange the last two cuts
  -track                                   Add a track
  -transition id[:arg] [name=value]*       Add a transition
  -verbose                                 Set the logging level to verbose
  -timings                                 Set the logging level to timings
  -version                                 Show the version and copyright
  -video-track | -hide-audio               Add a video-only track
For more help: <https://www.mltframework.org/>


# %load /usr/local/bin/SuperEffect
#!/bin/bash

# Print instructions for the user
echo "This script will process a video file located in the current directory."
echo "Please ensure that the 'start.mp4' file exists in this directory before proceeding."
echo "Press ENTER to continue, or CTRL+C to cancel."
read

# Check if start.mp4 exists in the current directory
if [ ! -f "start.mp4" ]; then
  echo "Error: 'start.mp4' file not found in current directory."
  exit 1
fi

# Process the video file
ffmpeg -i "$(pwd)/start.mp4" -crf 10 -vf \ 'minterpolate=fps=60:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1' -t 25 \ "$(pwd)/TEMPP.mp4"
sleep 1000
ffmpeg -i "$(pwd)/TEMPP.mp4" -vf mpdecimate,setpts=N/FRAME_RATE/TB -map:v 0 -y "$(pwd)/SUPER_EFFECT_Output.mkv"

# Remove the temporary file
rm "$(pwd)/TEMPP.mp4"


ffmpeg \
-loop 1 -t 3 -i img001.jpg \
-loop 1 -t 3 -i img002.jpg \
-loop 1 -t 3 -i img003.jpg \
-loop 1 -t 3 -i img004.jpg \
-loop 1 -t 3 -i img005.jpg \
-filter_complex \
"[0][1]xfade=transition=circlecrop:duration=0.5:offset=2.5[f0]; \
[f0][2]xfade=transition=smoothleft:duration=0.5:offset=5[f1]; \
[f1][3]xfade=transition=pixelize:duration=0.5:offset=7.5[f2]; \
[f2][4]xfade=transition=hblur:duration=0.5:offset=10[f3]" \
-map "[f3]" -r 25 -pix_fmt yuv420p -vcodec libx264 output-swipe-custom.mp4

import os
import random
from moviepy.editor import *
#from moviepy.video import transitions
from moviepy.video.compositing import transitions
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip

def zoom_inn(clip, duration):
    """Zooms in on a clip over a given duration"""
    zoomed_clips = []
    zoom_factor = 1.5 # Increase this value to zoom in more
    for i in range(2):
        zoomed_clips.append(clip.zoom(zoom_factor ** i))
    transition = CompositeVideoClip([zoomed_clips[0], zoomed_clips[1].set_start(duration)], size=clip.size)
    return transition.set_duration(duration)

def zoom_in(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_in = clip.zoom(zoom_factor)
    zoomed_in = zoomed_in.set_position(('center', 'center'))
    zoomed_in = zoomed_in.set_duration(duration)
    return zoomed_in.margin(-w/2, -h/2, -w/2, -h/2)


def zoom_out(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_out = clip.zoom(zoom_factor)
    zoomed_out = zoomed_out.set_position(('center', 'center'))
    zoomed_out = zoomed_out.set_duration(duration)
    return zoomed_out


def get_random_transition():
    transition_list = [
        transitions.crossfadein,
        transitions.crossfadeout,
        transitions.slide_in(random.choice(["left", "right", "top", "bottom"])),
        lambda: transitions.slide_out(random.choice(["left", "right", "top", "bottom"]))
  
    ]
    return random.choice(transition_list)


# Set the path to the directory containing the input images
image_dir = '/home/jack/Desktop/HDD500/to-vid/building'

# Set the output video parameters
duration = 1 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])

# Define a function to choose a random transition

# Define a function to create a video clip from an image with a random transition
def create_clip(filename):
    transition = get_random_transition()
    image_clip = ImageClip(filename).set_duration(duration)
    return image_clip.fx(transition, duration=duration)

# Create a list of video clips for each image with a random transition
clips = [create_clip(filename) for filename in image_files]

# Concatenate the video clips to create the final slideshow
slideshow = concatenate_videoclips(clips)

# Set the output video parameters and write the video file
slideshow = slideshow.resize(size).set_fps(fps)
slideshow.write_videofile(image_dir+'/slideshow.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.compositing import transitions
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip

def zoom_inn(clip, duration):
    """Zooms in on a clip over a given duration"""
    zoomed_clips = []
    zoom_factor = 1.5 # Increase this value to zoom in more
    for i in range(2):
        zoomed_clips.append(clip.zoom(zoom_factor ** i))
    transition = CompositeVideoClip([zoomed_clips[0], zoomed_clips[1].set_start(duration)], size=clip.size)
    return transition.set_duration(duration)

def zoom_in(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_in = clip.zoom(zoom_factor)
    zoomed_in = zoomed_in.set_position(('center', 'center'))
    zoomed_in = zoomed_in.set_duration(duration)
    return zoomed_in.margin(-w/2, -h/2, -w/2, -h/2)


def zoom_out(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_out = clip.zoom(zoom_factor)
    zoomed_out = zoomed_out.set_position(('center', 'center'))
    zoomed_out = zoomed_out.set_duration(duration)
    return zoomed_out


def slide_out_random_direction():
    return transitions.slide_out(random.choice(["left", "right", "top", "bottom"]))

def get_random_transition():
    transition_list = [
        transitions.crossfadein,
        transitions.crossfadeout,
        lambda: transitions.slide_in(random.choice(["left", "right", "top", "bottom"]), duration=duration),
        lambda: transitions.slide_out(random.choice(["left", "right", "top", "bottom"]), duration=duration)
    ]
    return random.choice(transition_list)



# Set the path to the directory containing the input images
image_dir = '/home/jack/Desktop/HDD500/to-vid/building'

# Set the output video parameters
duration = 1 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])

# Define a function to create a video clip from an image with a random transition
def create_clip(filename):
    transition = get_random_transition()
    image_clip = ImageClip(filename).set_duration(duration)
    return image_clip.fl_image(lambda img: transition(img, duration=duration))



# Create a list of video clips for each image with a random transition
clips = [create_clip(filename) for filename in image_files]

# Concatenate the video clips to create the final slideshow
slideshow = concatenate_videoclips(clips)

# Set the output video parameters and write the video file
slideshow = slideshow.resize(size).set_fps(fps)
slideshow.write_videofile(os.path.join(image_dir, 'slideshow.mp4'))

import os
import random
from moviepy.editor import *
from moviepy.video.compositing import transitions
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip

def zoom_inn(clip, duration):
    """Zooms in on a clip over a given duration"""
    zoomed_clips = []
    zoom_factor = 1.5 # Increase this value to zoom in more
    for i in range(2):
        zoomed_clips.append(clip.zoom(zoom_factor ** i))
    transition = CompositeVideoClip([zoomed_clips[0], zoomed_clips[1].set_start(duration)], size=clip.size)
    return transition.set_duration(duration)

def zoom_in(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_in = clip.zoom(zoom_factor)
    zoomed_in = zoomed_in.set_position(('center', 'center'))
    zoomed_in = zoomed_in.set_duration(duration)
    return zoomed_in.margin(-w/2, -h/2, -w/2, -h/2)


def zoom_out(clip, duration):
    w, h = clip.size
    zoom_factor = 1.5
    zoomed_out = clip.zoom(zoom_factor)
    zoomed_out = zoomed_out.set_position(('center', 'center'))
    zoomed_out = zoomed_out.set_duration(duration)
    return zoomed_out


def slide_out_random_direction():
    return transitions.slide_out(random.choice(["left", "right", "top", "bottom"]))

"""def get_random_transition():
    transition_list = [
        transitions.crossfadein,
        transitions.crossfadeout,
        lambda: transitions.slide_in(random.choice(["left", "right", "top", "bottom"]), duration=duration),
        lambda: transitions.slide_out(random.choice(["left", "right", "top", "bottom"]), duration=duration)
    ]
    return random.choice(transition_list)
"""
def get_random_transition():
    transition_list = [
        zoom_inn,
        zoom_in,
        zoom_out
    ]
    return random.choice(transition_list)



# Set the path to the directory containing the input images
image_dir = '/home/jack/Desktop/HDD500/to-vid/building'

# Set the output video parameters
duration = 1 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])

# Define a function to create a video clip from an image with a random transition
def create_clips(filename):
    transition = get_random_transition()
    image_clip = ImageClip(filename).set_duration(duration)
    return image_clip.fl_image(lambda img: transition(img, duration=duration))
def create_clip(filename):
    transition = get_random_transition()
    image_clip = VideoFileClip(filename, audio=False).set_duration(duration)
    return image_clip.fl_image(lambda img: transition(img, duration=duration))



# Create a list of video clips for each image with a random transition
clips = [create_clip(filename) for filename in image_files]

# Concatenate the video clips to create the final slideshow
slideshow = concatenate_videoclips(clips)

# Set the output video parameters and write the video file
slideshow = slideshow.resize(size).set_fps(fps)
slideshow.write_videofile(os.path.join(image_dir, 'slideshow.mp4'))

import os
from moviepy.editor import *
import random
import glob
# Set the output video parameters
duration = .5 # Duration of each image in seconds
fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_dir = '/home/jack/Desktop/HDD500/to-vid/building/'
image_files = random.sample(glob.glob(image_dir+'*.jpg'), 30)

# Define the crossfade transition
crossfade = lambda clip1, clip2: CompositeVideoClip([clip1, clip2.set_start(clip1.duration - duration)], size=size)

# Create a list of image clips with crossfades
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshow1.mp4')


import os
import random
import glob
import random
from moviepy.editor import *
from moviepy.video.compositing import *
# Set the output video parameters

fps = 25 # Frames per second
size = (640, 640) # Size of the output video

# Get a list of the image filenames in the directory
image_dir = '/home/jack/Desktop/HDD500/to-vid/building/*.jpg'
image_files = random.sample(glob.glob(image_dir), 40)
# Define the transitions
duration = 1 # Duration of each image in seconds
transitions = [transitions.crossfadein, transitions.crossfadeout]
random_transition = lambda clip1, clip2: random.choice(transitions)(clip1, clip2)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(os.path.join(image_files[i])).set_duration(1)
    
    if i > 0:
        # Add a random transition to the previous clip
        transition = random_transition(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshow2.mp4')


import moviepy
dir(moviepy.video.compositing.transitions)



import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),20)

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1, clip2.set_start(clip1.duration - duration)], size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshow.mp4')


/home/jack/Desktop/HDD500/to-vid/building/01145.jpg

image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),20)

print (image_files[4])

from moviepy.video.fx import *
dir(vfx)

# Good working with fade transition
import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=1), 
                               clip2.fx(vfx.fadein, duration=1)], 
                              size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshowT.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the transitions
def wipe(clip1, clip2, transition_type="right"):
    if transition_type == "right":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_x)
        mask = mask.crop(x1=0, y1=0, x2=mask.w*(clip2.start/clip2.duration), y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("right","top")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_x)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("left","top")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=0, x2=mask.w, y2=mask.h*(clip2.start/clip2.duration))
        mask = mask.set_position(("center","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","bottom")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    elif transition_type == "top":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(1-clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("center","bottom"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","top")).fx(vfx.mask_color, color=(0,0,0), mask=mask)])
    else:
        raise ValueError("Invalid transition type")

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a wipe transition to the previous clip
        transition = wipe(clips[-1], image_clip, "left")
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Apply a wipe transition
video = video.fx(vfx.wipe, horizontal=True, duration=1)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshoww.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)

# Define the wipe transition
def wipe(clip1, clip2, transition_type):
    # Create a mask clip for the transition
    mask = ColorClip(size, color=(255, 255, 255), duration=duration)
    if transition_type == "right":
        mask = mask.crop(x1=0, y1=0, x2=mask.w*clip2.start/clip2.duration, y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("right","top")).mask_video(mask)])
    elif transition_type == "left":
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right","top"))
        return CompositeVideoClip([clip1.set_position(("right","top")), clip2.set_position(("left","top")).mask_video(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=0, x2=mask.w, y2=mask.h*clip2.start/clip2.duration)
        mask = mask.set_position(("center","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","bottom")).mask_video(mask)])
    elif transition_type == "top":
        mask = ColorClip(size, color=(0,0,0), duration=duration)
        mask = mask.crop(x1=0, y1=mask.h*(1-clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("center","bottom"))
        return CompositeVideoClip([clip1, clip2.set_position(("center","top")).mask_video(mask)])
    else:
        return clip2

# Create a list of image clips with wipe transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a wipe transition to the previous clip
        transition = wipe(clips[-1], image_clip, "right")
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/HDD500/to-vid/building/slideshowW.mp4')


from moviepy.editor import *
from moviepy.video.fx import all as vfx

def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    mask = ColorClip(size, color=(0,0,0), duration=duration)
    if transition_type == "right":
        mask = mask.crop(x1=0, y1=0, x2=mask.w*clip2.start/clip2.duration, y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1, clip2.set_position(("right","top")).set_mask(mask)])
    elif transition_type == "left":
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left","top"))
        return CompositeVideoClip([clip1.set_mask(mask), clip2.set_position(("left","top"))])
    elif transition_type == "top":
        mask = mask.crop(x1=0, y1=mask.h*(1-clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("top","left"))
        return CompositeVideoClip([clip1.set_mask(mask), clip2.set_position(("left","top"))])
    elif transition_type == "bottom":
        mask = mask.crop(x1=0, y1=0, x2=mask.w, y2=mask.h*clip2.start/clip2.duration)
        mask = mask.set_position(("bottom","left"))
        return CompositeVideoClip([clip1.set_mask(mask), clip2.set_position(("left","top"))])
    else:
        raise ValueError("Invalid transition type")

def slideshow(image_files, duration, transition_type="right"):
    clips = []
    for i, image_file in enumerate(image_files):
        image_clip = ImageClip(image_file).set_duration(duration)
        if i == 0:
            clips.append(image_clip)
        else:
            transition = wipe(clips[-1], image_clip, transition_type)
            clips.append(transition)
        clips.append(image_clip)
    return concatenate_videoclips(clips)

image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'),30)
duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshowx.mp4", fps=24)


from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx
from PIL import Image
import random
import glob


def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    image_clip = ImageClip(Image.new("RGB", size, "black"), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1.set_position(("right", "top")), clip2.set_position(("left", "top")).set_mask(mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right", "top"))
        return CompositeVideoClip([clip1.set_position(("left", "top")), clip2.set_position(("right", "top")).set_mask(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("bottom","center"))
        return CompositeVideoClip([clip1.set_position(("top","center")), clip2.set_position(("bottom","center")).set_mask(mask)])


def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        clip = VideoFileClip(path, audio=False).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip

image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/to-vid/building/*.jpg'), 30)
duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshowx.mp4", fps=24)


import os
import random
import glob
from PIL import Image

from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx


def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    image_clip = ImageClip(Image.new("RGB", size, "black"), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1.set_position(("right", "top")), clip2.set_position(("left", "top")).set_mask(mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right", "top"))
        return CompositeVideoClip([clip1.set_position(("left", "top")), clip2.set_position(("right", "top")).set_mask(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("bottom","center"))
        return CompositeVideoClip([clip1.set_position(("top","center")), clip2.set_position(("bottom","center")).set_mask(mask)])


def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        with Image.open(path) as img:
            clip = ImageClip(img).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip


image_folder = '/home/jack/Desktop/HDD500/to-vid/building'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)

duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("slideshow.mp4", fps=24)


import os
import random
import glob
from moviepy.editor import VideoClip, VideoFileClip, clips_array

def wipe(clip1, clip2, transition_type):
    if transition_type == "right":
        return clips_array([[clip1.crossfadein(1), clip2.crossfadein(1).margin(10)]])
    elif transition_type == "left":
        return clips_array([[clip1.crossfadein(1).margin(10), clip2.crossfadein(1)]])
    elif transition_type == "bottom":
        return clips_array([[clip1.crossfadein(1).margin(10)], [clip2.crossfadein(1)]])

def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        img_clip = VideoFileClip(path).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(img_clip)
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = clips_array(clips, duration=duration*len(images_path))
    return final_clip

image_folder = 'static/images'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 5)

duration = 5
transition_type = "left"
slideshow_clip = slideshow(image_files, duration, transition_type)
slideshow_clip.write_videofile("steamslideshow.mp4", fps=24)




import os
import random
import glob
import cv2
from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx
import numpy as np

def wipe(clip1, clip2, transition_type):
    size = clip1(0).shape[:2][::-1]
    duration = clip1.duration
    image_clip = ImageClip(np.zeros((size[1], size[0], 3), dtype=np.uint8), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1, clip2.set_position(("left", "top")).set_mask(mask)])
    # Similar for other transition types...

def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        img = cv2.imread(path)
        clips.append(lambda t: img)
        print(f"Loaded image from {path}")
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip

image_folder = '/home/jack/Desktop/StoryMaker/static/images/steampunk/personal-feed_files'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)

duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("steamslideshow.mp4", fps=24)


import os
import random
import glob
from PIL import Image
import cv2
from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy.video.VideoClip import ColorClip
from moviepy.video.VideoClip import ImageClip
from moviepy.video import fx as vfx


def wipe(clip1, clip2, transition_type):
    size = clip1.size
    duration = clip1.duration
    image_clip = ImageClip(Image.new("RGB", size, "black"), duration=duration)
    if transition_type == "right":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("left", "top"))
        return CompositeVideoClip([clip1.set_position(("right", "top")), clip2.set_position(("left", "top")).set_mask(mask)])
    elif transition_type == "left":
        mask = ColorClip(size, color=(255, 255, 255), duration=duration)
        mask = mask.crop(x1=mask.w*(1-clip2.start/clip2.duration), y1=0, x2=mask.w, y2=mask.h)
        mask = mask.set_position(("right", "top"))
        return CompositeVideoClip([clip1.set_position(("left", "top")), clip2.set_position(("right", "top")).set_mask(mask)])
    elif transition_type == "bottom":
        mask = ColorClip(size, color=(0,0,0), duration=duration).fx(vfx.mirror_y)
        mask = mask.crop(x1=0, y1=mask.h*(clip2.start/clip2.duration), x2=mask.w, y2=mask.h)
        mask = mask.set_position(("bottom","center"))
        return CompositeVideoClip([clip1.set_position(("top","center")), clip2.set_position(("bottom","center")).set_mask(mask)])

def slideshow(images_path, duration=1, transition_type="right"):
    clips = []
    for path in images_path:
        img = cv2.imread(path)
        clips.append(lambda t: img)  # Create a frame generator function
        print(f"Loaded image from {path}")
    for i in range(1, len(clips)):
        transition = wipe(clips[i-1], clips[i], transition_type)
        clips.append(transition)
    final_clip = CompositeVideoClip(clips, duration=duration*len(images_path))
    return final_clip


image_folder = '/home/jack/Desktop/StoryMaker/static/images/steampunk/personal-feed_files'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)

duration = 5
transition_type = "left"
slideshow = slideshow(image_files, duration, transition_type)
slideshow.write_videofile("steamslideshow.mp4", fps=24)


steamslideshow.mp4

import os
from PIL import Image
from moviepy.video.VideoClip import ImageClip, ColorClip
from moviepy.video.compositing.concatenate import concatenate_videoclips

def slideshow(images_path, duration, transition_type):
    clips = []
    transition_duration = 1
    
    if transition_type == "fade":
        transition_clip = ColorClip((1280, 720), color=(0, 0, 0), duration=transition_duration)
        transition_clip = transition_clip.crossfadein(transition_duration)
    
    for path in images_path:
        with Image.open(path) as img:
            clip = ImageClip(img).resize(height=720)
        print(f"Loaded image from {path}")
        clips.append(clip)
        if transition_type == "fade":
            clips.append(transition_clip)

    final_clip = concatenate_videoclips(clips, method="compose")
    final_clip = final_clip.set_duration(duration)
    return final_clip

# Example usage
image_folder = '/home/jack/Desktop/HDD500/to-vid/building'
image_files = glob.glob(os.path.join(image_folder, '*.jpg'))
image_files = random.sample(image_files, 30)
duration = 10  # in seconds
transition_type = "fade"
slideshow_clip = slideshow(image_files, duration, transition_type)
slideshow_clip.write_videofile("slideshow.mp4", fps=24)


from moviepy.editor import VideoFileClip
from moviepy.video.fx.all import painting
import subprocess

# Load the video clip
clip = VideoFileClip("/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED01.mp4")

# Apply the painting effect
painted_clip = clip.fx(painting, saturation=1.4, black=0.006)

# Write the painted clip to a new file
painted_clip.write_videofile("/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED_painting.mp4")



num_frames=$(ffprobe -v error -select_streams v:0 -count_packets -show_entries stream=nb_read_packets -of csv=p=0 /home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED01.mp4)
echo $num_frames


# Get the number of frames in the painted clip using ffmpeg
command = ["ffmpeg", "-i", "/home/jack/Desktop/HDD500/collections/newdownloads/mine-new/newvid/JOINED01.mp4", "-v", "error", "-count_frames", "-select_streams", "v:0", "-show_entries", "stream=nb_frames", "-of", "default=nokey=1:noprint_wrappers=1"]
num_frames = int(subprocess.check_output(command))

print(f"The painted video has {num_frames} frames.")


# Good working with fade transition
import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
#image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/collections/newdownloads/512x512/*.jpg'),30)

image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=.25), 
                               clip2.fx(vfx.fadein, duration=.25)], 
                              size=size)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a crossfade transition to the previous clip
        transition = crossfade(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2-5.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
from moviepy.video.compositing.transitions import WipeTransition
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
#image_files = random.sample(glob.glob('/home/jack/Desktop/HDD500/collections/newdownloads/512x512/*.jpg'),30)

image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Define the transitions
def crossfade(clip1, clip2):
    return CompositeVideoClip([clip1.fx(vfx.fadeout, duration=1), 
                               clip2.fx(vfx.fadein, duration=1)], 
                              size=size)

def vertical_wipe(clip1, clip2):
    return WipeTransition(clip1, clip2, direction='vertical', duration=1)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a wipe transition to the previous clip
        transition = vertical_wipe(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2.mp4')


import os
import random
from moviepy.editor import *
from moviepy.video.fx import *
import glob

# Set the output video parameters
fps = 25 # Frames per second
size = (640, 640) # Size of the output video
duration = 1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = sorted(glob.glob('/home/jack/Desktop/monitor_project/*.jpg'))

# Define the wipe transition
def vertical_wipe(clip1, clip2):
    return wipe(clip1, clip2, transition='vertical', duration=1)

# Create a list of image clips with transitions
clips = []
for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    
    if i > 0:
        # Add a vertical wipe transition to the previous clip
        transition = vertical_wipe(clips[-1], image_clip)
        clips.append(transition)
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('/home/jack/Desktop/monitor_project/slideshowNEW2.mp4')


import moviepy.video.compositing.transitions
dir(moviepy.video.compositing.transitions)

from moviepy.video.compositing.transitions import slide_in

!ls /home/jack/Desktop/StoryMaker/steampunk/*.jpg

from moviepy.video.compositing.transitions import slide_in
from moviepy.video.fx import all
from moviepy.editor import *
import glob
import random
# Set the output video parameters
fps = 25 # Frames per second
size = (768, 512) # Size of the output video
duration =1 # Duration of each image in seconds

# Get a list of the image filenames in the directory
image_files = sorted(glob.glob('/home/jack/Desktop/StoryMaker/static/images/steampunk/NewFolder/*.jpg'))
#print(image_files)
# Create a list of image clips with transitions
clips = []

for i in range(len(image_files)):
    # Load the image and create a video clip
    image_clip = ImageClip(image_files[i]).set_duration(duration)
    direction = random.choice(['right','left','top','bottom'])
    
    if i > 0:
        # Add a vertical slide transition to the previous clip
        transition = slide_in(image_clip, duration=1, side=direction)
        clips.append(CompositeVideoClip([clips[-1], transition]).set_duration(1))
        
    clips.append(image_clip)

# Concatenate the clips to create the final video
video = concatenate_videoclips(clips)

# Set the output video parameters
video = video.set_fps(fps)
video = video.resize(size)

# Save the video
video.write_videofile('steampunk/slideshowSharp.mp4')


video = ""

!vlc steampunk/slideshowSharp.mp4

from moviepy.editor import concatenate_videoclips
from moviepy.video.compositing.transitions import (
    CrossfadeIn, CrossfadeOut, SlideIn, SlideOut, ZoomIn, ZoomOut, WarpIn, WarpOut
)

# Create instances of the transition classes
transition1 = CrossfadeIn()
transition2 = CrossfadeOut()
transition3 = SlideIn()
transition4 = SlideOut()
transition5 = ZoomIn()
transition6 = ZoomOut()
transition7 = WarpIn()
transition8 = WarpOut()

# Concatenate video clips with transitions
video_clips = [clip1, clip2, clip3]  # Replace with your actual video clips
transitions = [transition1, transition2, transition3, transition4, transition5, transition6, transition7, transition8]
final_clip = concatenate_videoclips(video_clips, transitions=transitions)


"""Requires scikit-image installed (for ``vfx.painting``)."""

from moviepy import *


# WE TAKE THE SUBCLIPS WHICH ARE 2 SECONDS BEFORE & AFTER THE FREEZE

charade = VideoFileClip("../../videos/charade.mp4")
tfreeze = convert_to_seconds(19.21)  # Time of the freeze, 19'21

clip_before = charade.subclip(tfreeze - 2, tfreeze)
clip_after = charade.subclip(tfreeze, tfreeze + 2)


# THE FRAME TO FREEZE

im_freeze = charade.to_ImageClip(tfreeze)
painting = charade.fx(vfx.painting, saturation=1.6, black=0.006).to_ImageClip(tfreeze)

txt = TextClip("Audrey", font="Amiri-regular", font_size=35)

painting_txt = (
    CompositeVideoClip([painting, txt.set_pos((10, 180))])
    .add_mask()
    .with_duration(3)
    .crossfadein(0.5)
    .crossfadeout(0.5)
)

# FADEIN/FADEOUT EFFECT ON THE PAINTED IMAGE

painting_fading = CompositeVideoClip([im_freeze, painting_txt])

# FINAL CLIP AND RENDERING

final_clip = concatenate_videoclips(
    [clip_before, painting_fading.with_duration(3), clip_after]
)

final_clip.write_videofile(
    "../../audrey.avi", fps=charade.fps, codec="mpeg4", audio_bitrate="3000k"
)

from mohttp://localhost:8888/notebooks/Imports_moviepy-ffmpeg.ipynb#viepy.editor import concatenate_videoclips
from moviepy.video.fx import fadein, fadeout, slide_in, slide_out, zoom_in, zoom_out, warp_in, warp_out


import moviepy.video
dir (moviepy.video)

from moviepy.video import fx,io,tools,VideoClip,compositing
dir(compositing)

from moviepy.video import fx,io,tools,VideoClip,compositing
dir (compositing.transitions)

from moviepy.video.compositing.transitions import crossfadeinout, crossfadeoutin, slide_in, slide_out, zoom_in, zoom_out, warp_in, warp_outfrom 

from moviepy.video import fx,io,tools,VideoClip,compositing
compositing.fadein

from moviepy.video import fx,io,tools,VideoClip,compositing
dir (compositing.CompositeVideoClip)

from moviepy.video import ImageClip, concatenate_videoclips
from moviepy.video.compositing.transitions import crossfadeinout, crossfadeoutin, slide_in, slide_out, zoom_in, zoom_out, warp_in, warp_out
import random

def get_random_transition(duration):
    transition_list = [
        crossfadeinout,
        crossfadeoutin,
        slide_in,
        lambda: slide_out(random.choice(["left", "right", "top", "bottom"])),
        zoom_in,
        zoom_out,
        warp_in,
        warp_out
    ]
    return random.choice(transition_list)(duration=duration)
image_files= random.sample(glob.glob("/home/jack/Desktop/StoryMaker/static/current_project/Misc/portrait/*.jpg"),10)
#image_files = ["image1.jpg", "image2.jpg", "image3.jpg", "image4.jpg"]
clips = []
for filename in image_files:
    clip = ImageClip(filename).set_duration(2)
    transition = get_random_transition(1)
    clips.append(clip.fx(transition))

final_clip = concatenate_videoclips(clips)
final_clip.write_videofile("slideshow.mp4", fps=25)


!vlc slideshow.mp4

/home/jack/Desktop/StoryMaker/newvid/text.mp4

"""Requires scikit-image installed (for ``vfx.painting``)."""

from moviepy import *


# WE TAKE THE SUBCLIPS WHICH ARE 2 SECONDS BEFORE & AFTER THE FREEZE

charade = VideoFileClip("/home/jack/Desktop/StoryMaker/newvid/text.mp4")
#tfreeze = convert_to_seconds(19.21)  # Time of the freeze, 19'21
tfreeze = 3  # Time of the freeze, 19'21

clip_before = charade.subclip(tfreeze - 2, tfreeze)
clip_after = charade.subclip(tfreeze, tfreeze + 2)


# THE FRAME TO FREEZE

im_freeze = charade.to_ImageClip(tfreeze)
painting = charade.fx(vfx.painting, saturation=1.6, black=0.006).to_ImageClip(tfreeze)

txt = TextClip("Audrey", font="Amiri-regular", font_size=35)

painting_txt = (
    CompositeVideoClip([painting, txt.set_pos((10, 180))])
    .add_mask()
    .with_duration(3)
    .crossfadein(0.5)
    .crossfadeout(0.5)
)

# FADEIN/FADEOUT EFFECT ON THE PAINTED IMAGE

painting_fading = CompositeVideoClip([im_freeze, painting_txt])

# FINAL CLIP AND RENDERING

final_clip = concatenate_videoclips(
    [clip_before, painting_fading.with_duration(3), clip_after]
)

final_clip.write_videofile(
    "freeze.avi", fps=charade.fps, codec="mpeg4", audio_bitrate="3000k"
)




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Story_Maker.ipynb
Code Content:
import datetime

from PIL import ImageGrab
import numpy as np
import cv2
from win32api import GetSystemMetrics

input_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey_FORMATED.txt'

words_per_line = 5

# Read the input file
with open(input_file_path, 'r') as file:
    content = file.read()

# Split the content into words
words = content.split()

# Format the text with 5 words per line
formatted_words = [' '.join(words[i:i+words_per_line]) for i in range(0, len(words), words_per_line)]
formatted_text = '\n'.join(formatted_words)

# Save the formatted text to the output file
with open(output_file_path, 'w') as file:
    file.write(formatted_text)

!pwd

import cv2
import numpy as np

text_file_path = 'static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt'
new_path = text_file_path[:-4]+"_A.mp4"
output_file_path = new_path.replace("formatted_text","text_video")
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = num_lines * 2  # Adjust the duration as needed
fps = int(num_lines / duration)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y = height + (line_num + 1) * font_size  # Scroll up by the font size multiplied by line number

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()
print(output_file_path)

!ls static/text_video/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED_A.mp4

#Works FIne
from moviepy.editor import ImageSequenceClip
import os
import random
def create_video_from_images(image_directory, output_file):
    # Get a list of image file names in the directory
    image_files = sorted([f for f in os.listdir(image_directory) if f.endswith(".jpg") or f.endswith(".png")])
    # Shuffle the image list
    random.shuffle(image_files)
    # Create a list of image paths
    image_paths = [os.path.join(image_directory, filename) for filename in image_files]

    # Create a clip from the image sequence 1/24
    clip = ImageSequenceClip(image_paths, durations=[1] * len(image_paths))

    # Write the video file
    clip.write_videofile(output_file, fps=24)

# Set the directory path where your images are located
image_directory = "/home/jack/Desktop/HDD500/0WORKSHOP-with-NOTEBOOKS/done/dystopianstreets_files"

# Set the output file path
output_file = "VIDEOS/dystopianstreets2-10.mp4"

# Create the video from the images
create_video_from_images(image_directory, output_file)


!vlc VIDEOS/dystopianstreets2-10.mp4

# Shuffle image_list

import random
import glob

DIR = "/home/jack/Desktop/animate/backgrounds/"
image_list = glob.glob(DIR + "*.jpg")

# Shuffle the image list
random.shuffle(image_list)

# Print the shuffled image list
print(len(image_list))


!rm /home/jack/Desktop/animate/*.jpg
import random
import os
import glob
from moviepy.editor import ImageSequenceClip
from PIL import Image
from time import sleep
import time
import datetime
#imagelist =sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/*.png"))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/quantized/*.jpg'))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/640x640-alien/*.jpg'))
#imagelist =sorted(glob.glob('build/*.jpg'))
DIR = "/home/jack/Desktop/HDD500/FLASK/static/milestones_resources/alien-skull/"
DIR = "/home/jack/Desktop/StoryMaker/static/new_resources/leonardo.ai_files/"
image_list = glob.glob(DIR + "*.jpg")

# Shuffle the image list
random.shuffle(image_list)

# Print the shuffled image list
print(len(image_list))
countdown=len(image_list)
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
imagesize = Image.open(image_list[1]).size
for i in range(0,len(image_list)-1): 
# Take two images for blending them together  
    imag1 = image_list[i]
    imag2 = image_list[i+1]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(0,125):
        inc = ic*.008
        sleep(.1)
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        current_time = datetime.datetime.now()
        filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + '.jpg'
        alphaBlended.save(f'{text}{filename}')
        if ic % 100 ==0:print(countdown-ic,end = " . ")

#from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
# Get the list of files sorted by creation time
imagelist = sorted(glob.glob('/home/jack/Desktop/animate/*.jpg'), key=os.path.getmtime)

# Create a clip from the images
clip = ImageSequenceClip(imagelist, fps=30)

# Write the clip to a video file using ffmpeg
current_time = datetime.datetime.now()
filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + 'june29.mp4'
clip.write_videofile('/home/jack/Desktop/animate/'+filename, fps=24, codec='libx265', preset='medium')

import glob

image_list =sorted(glob.glob('/home/jack/Desktop/HDD500/0WORKSHOP-with-NOTEBOOKS/build/*.jpg'))
len(image_list)

from PIL import Image
im = Image.open(image_list[20])
im.size
im

from PIL import Image
from time import sleep
import time
import datetime
import random
from time import sleep
# Function to change the image size

imagelist = random.sample(image_list, 68)
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
imagesize = Image.open(imagelist[1]).size
for i in range(0,len(image_list)-1): 
# Take two images for blending them together  
    imag1 = imagelist[i]
    imag2 = imagelist[i+1]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(0,125):
        inc = ic*.008
        sleep(.1)
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        current_time = datetime.datetime.now()
        filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + '.jpg'
        alphaBlended.save(f'{text}{filename}')
        if ic %20 ==0:print(ic, end = " . ")

import os
import glob
from PIL import Image
from time import sleep
import time
import datetime
from time import sleep
from moviepy.editor import ImageSequenceClip
#from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
# Get the list of files sorted by creation time
imagelist = sorted(glob.glob('/home/jack/Desktop/animate/*.jpg'), key=os.path.getmtime)

# Create a clip from the images
clip = ImageSequenceClip(imagelist, fps=30)

# Write the clip to a video file using ffmpeg
current_time = datetime.datetime.now()
filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + 'june25.mp4'
clip.write_videofile('/home/jack/Desktop/animate/'+filename, fps=24, codec='libx265', preset='medium')









from PIL import Image
import random
import glob
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
ImOb = [] 
# Take two images for blending them together  
#imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
#imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
DIR = "/home/jack/Desktop/TENSORFLOW/FACE_Swap/source_images/square/*crop.jpg"
image1 = Image.open(random.choice(glob.glob(DIR)))
DIR = "/home/jack/Desktop/TENSORFLOW/FACE_Swap/source_images/square/*crop.jpg"
image2 = Image.open(random.choice(glob.glob(DIR)))
print(image1," : ",image2)
#/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00405up.jpg
#/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00410up.jpg
file1 ="/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00405up.jpg"
file2 ="/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/00410up.jpg"

image1 = Image.open(file1)
image1 = Image.open(file2)

# Make the images of uniform size
image3 = changeImageSize(512, 768, image1)
image4 = changeImageSize(512, 768, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(0,500):
    inc = i*.02
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    ImOb.append(alphaBlended)
    output= f'{text}{i:05d}_.png'
    print(output)
    alphaBlended.save(output)

im = Image.open("static/assets/colorful.png")
im

OPEN_IMAGES = ImOb
#IMAGES = sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/animate/new*_.png"))
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
#opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(ImOb):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image.save(f"temp{i}.png")
#opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images = [Image.open(f"temp{i}.png") for i in range(len(OPEN_IMAGES))]
opened_images[0].save("/home/jack/Desktop/animate/RANDOM105A.gif", save_all=True, append_images=opened_images[1:], duration=200, loop=0)


from PIL import Image
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
 
# Take two images for blending them together  
imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf9.png"
imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
image1 = Image.open(imag1)
image2 = Image.open(imag2)

# Make the images of uniform size
image3 = changeImageSize(512, 512, image1)
image4 = changeImageSize(512, 512, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(0,1000):
    inc = i*.001
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    alphaBlended.save(f'{text}{i:05d}_.png')

find $pwd -name "*.png" -printf '%h\n' | sort | uniq -c | sort -nr | head -n 1

from PIL import Image
import glob
import random
from random import randint
thumb = random.choice(glob.glob("/home/jack/.cache/thumbnails/normal/*.png"))
Thum = Image.open(thumb)
Thum                      

def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1080,1080), (250,250,250))
for i in range(0,2000):
     if i< 500:DIR = "/home/jack/.cache/thumbnails/large/*.png"
     if i> 500:DIR = "/home/jack/.cache/thumbnails/normal/*.png"
     Thum = mkmoz(DIR,im)
     im.paste(Thum,((randint(0,im.size[0])-50),randint(0,im.size[1])))
im              

im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMoz2.png")
im = im.resize((500,500), Image.BICUBIC)
im

def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1080,1080), (250,250,250))
for i in range(0,2500):
     if i< 500:DIR = "/home/jack/.cache/thumbnails/large/*.png"
     if i> 500:DIR = "/home/jack/.cache/thumbnails/normal/*.png"
     Thum = mkmoz(DIR,im)
     im.paste(Thum,((randint(0,im.size[0])-50),randint(0,im.size[1]-50)))
im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMozmix01.png")
im = im.resize((500,500), Image.BICUBIC)
im        

def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1080,1080), (250,250,250))
for i in range(0,2500):
     if i< 500:DIR = "/home/jack/.cache/thumbnails/large/*.png"
     if i> 500:DIR = "/home/jack/.cache/thumbnails/normal/*.png"
     if i> 2495:DIR = "/home/jack/.cache/thumbnails/large/*.png"   
     Thum = mkmoz(DIR,im)
     im.paste(Thum,((randint(0,im.size[0])-50),randint(0,im.size[1]-50)))
im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMozmix05.png")
im = im.resize((500,500), Image.BICUBIC)
im        

#im.save("/home/jack/Desktop/TENSORFLOW/junk/ThumbNailMozmix02.png")
im = im.resize((500,500), Image.BICUBIC)
im        

/home/jack/Desktop/TENSORFLOW/clouds/clouds/00015last.png

from PIL import Image
import os
import random
from random import randint 
import glob 
def mkmoz(DIR,im):
    #im = Image.new("RGB", (1080,1080), (250,250,250))
    thumb = random.choice(glob.glob(DIR))
    Thum = Image.open(thumb)
    return Thum
im = Image.new("RGB", (1200,1200), (250,250,250))
for x in range(0,500):
    DIR = "/home/jack/Desktop/TENSORFLOW/clouds/clouds/*.png"
    text = "/home/jack/Desktop/TENSORFLOW/animate/"
    Thum = mkmoz(DIR,im)
    SIZE = randint(100,300)
    Thu = Thum.resize((SIZE,SIZE), Image.BICUBIC) 
    im.paste(Thu,((randint(0,im.size[0])),randint(0,im.size[1])),Thu)
    if x % 100 == 0:print(f'{text}{x:05d}gif.png')
    # Calculate the coordinates for the crop
    left = (im.width - 1080) / 2
    top = (im.height - 1080) / 2
    right = (im.width + 1080) / 2
    bottom = (im.height + 1080) / 2
    # Crop the image
    cropped_img = im.crop((left, top, right, bottom))
    # Save the cropped image
    cropped_img.save(f'{text}{x:05d}gif.png')
im              

import random
import glob
from PIL import Image
OPEN_IMAGES = []
IMAGES = glob.glob("/home/jack/Desktop/TENSORFLOW/animate/*gif.png")
#IMAGES = glob.glob("/home/jack/Desktop/TENSORFLOW/mkgif/*.png")
print(len(IMAGES))
for i in range(0,len(IMAGES)):
    im = Image.open(IMAGES[i])
    OPEN_IMAGES.append(im)
images = IMAGES
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(opened_images):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image.save(f"temp{i}.png")
opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/dreamlike-circles2.gif", save_all=True, append_images=opened_images[1:], duration=650, loop=0)
#from IPython.display import HTML
#HTML('<img src="/home/jack/Desktop/TENSORFLOW/mkgif/dreamlike-custom6.gif"/>')

for x in range(0,10):
    text = "/home/jack/Desktop/TENSORFLOW/junk/"
    print(f'{text}{x:05d}gif.png')


print(f'{text}{x:05d}.png')

#!/home/jack/miniconda3/envs/cloned_base/bin/python
import tkinter as tk
from tkinter import *
from PIL import ImageTk, Image
import sys
# Create the root window
root = tk.Tk()

#print(sys.argv[1])
#print(sys.argv[2])
#image1 = ImageTk.PhotoImage(Image.open(sys.argv[1]))
#image2 = ImageTk.PhotoImage(Image.open(sys.argv[2]))
imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
image1 = ImageTk.PhotoImage(Image.open(imag1))
image2 = ImageTk.PhotoImage(Image.open(imag2))


# Create the first image widget and pack it into the root window
img1 = tk.Label(root, image=image1)
img1.pack()

# Set the initial opacity of the first image widget
#img1.config(bg='black')
#img1['image'] = image1
img1.config(bg='black')
img1['image'] = image1

def update_opacity():
    global opacity
    if opacity <= 0:
        root.after_cancel(after_id)
    else:
        img1.config(alpha=opacity)
        opacity -= opacity * 0.1
        after_id = root.after(1000, update_opacity)


"""


# Create a function that will be called repeatedly to update the opacity of the first image
def update_opacity():
    global opacity
    if opacity <= 0:
        # Stop the repeating call to this function
        root.after_cancel(after_id)
    else:
        # use rgba to set the opacity
        img1.config(bg='rgba(0, 0, 0, {})'.format(opacity))
        #img1.config(bg=)  # Convert the opacity value to hexadecimal and set the color
        # Decrease the opacity for the next iteration
        opacity -= int(opacity * 0.1)
        # Schedule this function to be called again after 1 second
        after_id = root.after(1000, update_opacity)

        # Update the opacity of the image widget
        #img1.config(alpha=opacity)
        # Decrease the opacity for the next iteration
        #opacity -= opacity * 0.1
        # Schedule this function to be called again after 1 second
        #after_id = root.after(1000, update_opacity)
"""
# Set the initial value of the opacity
opacity = 1
# Start the repeating function to update the opacity of the first image
update_opacity()

# Create a function that will be called after 1 second to fade in the second image
def fade_in_image2():
    # Create the second image widget and pack it into the root window
    img2 = tk.Label(root, image=image2)
    img2.pack()

    # Set the initial opacity of the second image widget
    img2.config(bg='black')
    img2['image'] = image2

    # Create a function that will be called repeatedly to update the opacity of the second image
    def update_opacity():
        global opacity
        if opacity >= 1:
            # Stop the repeating call to this function
            root.after_cancel(after_id)
        else:
            # Update the opacity of the image widget
            img2.config(alpha=opacity)
            # Increase the opacity for the next iteration
            opacity += opacity * 0.1
            # Schedule this function to be called again after 1 second
            after_id = root.after(1000, update_opacity)

    # Set the initial value of the opacity
    opacity = 0
    # Start the repeating function to update the opacity of the second image
    update_opacity()

# Schedule the function to fade in the second image after 1 second
root.after(1000, fade_in_image2)

# Run the tkinter event loop
root.mainloop()


from PIL import Image

# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]

    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])

    newImage    = image.resize((newWidth, newHeight))
    return newImage
    
# Take two images for blending them together  
imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
image1 = Image.open(imag1)
image2 = Image.open(imag2)

# Make the images of uniform size
image3 = changeImageSize(512, 512, image1)
image4 = changeImageSize(512, 512, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")

# Display the images
#image5.show()
#image6.show()

# alpha-blend the images with varying values of alpha
#alphaBlended1 = Image.blend(image5, image6, alpha=.2)
alphaBlended2 = Image.blend(image5, image6, alpha=.4)

# Display the alpha-blended images
#alphaBlended1.show()
alphaBlended2.show()

!display /home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf9.png

!ls -sr /home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/

for i in range(0,250):
    inc = i*.04
    print(f'{inc}',end = " . ")



IMAGES = sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/animate/new*_.png"))
print(IMAGES)

/home/jack/Desktop/TENSORFLOW/animate/new00999_.png

!ls /home/jack/Desktop/TENSORFLOW/animate/

import random
import glob
from PIL import Image
OPEN_IMAGES = []
IMAGES = sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/animate/*gif.png"))
#IMAGES = glob.glob("/home/jack/Desktop/TENSORFLOW/mkgif/*.png")
print(len(IMAGES))
for i in range(0,len(IMAGES)):
    im = Image.open(IMAGES[i])
    OPEN_IMAGES.append(im)
images = IMAGES
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(opened_images):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image.save(f"temp{i}.png")
opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/testBlend1.gif", save_all=True, append_images=opened_images[1:], duration=1, loop=0)
#from IPython.display import HTML
#HTML('<img src="/home/jack/Desktop/TENSORFLOW/mkgif/dreamlike-custom6.gif"/>')

import random
import glob
from PIL import Image
from PIL import Image
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
ImOb = [] 
# Take two images for blending them together  
#imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
#imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"

image1 = Image.open(random.choice(glob.glob(DIR)))
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"
image2 = Image.open(random.choice(glob.glob(DIR)))
print(image1," : ",image2)
# Make the images of uniform size
image3 = changeImageSize(512, 768, image1)
image4 = changeImageSize(512, 768, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(1,250):
    inc = i*.004
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    alphaBlended = alphaBlended.convert("RGB") 
    ImOb.append(alphaBlended)
    
    alphaBlended.save(f'{text}{i:05d}.jpg')

OPEN_IMAGES = ImOb
#IMAGES = sorted(glob.glob("/home/jack/Desktop/animate/*.jpg"))
IMAGES = sorted(glob.glob("/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"))
images = random.sample(IMAGES,20)
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
#opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(ImOb):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image=opened_image.convert("RGB")
    opened_image.save(f"temp{i}.jpg")
#opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images = [Image.open(f"temp{i}.jpg") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/RANDOM106XX.gif", save_all=True, append_images=opened_images[1:], duration=200, loop=0)


from IPython.display import HTML
HTML('<img src="/home/jack/Desktop/TENSORFLOW/mkgif/memory1.gif"/>')

import os
import hashlib

# Define the directory path
directory = '/home/jack/Desktop/HDD500/collections/quantized'

# Create an empty dictionary to store the file hashes
hashes = {}

# Loop through the directory and compute the hash of each file
for root, dirs, files in os.walk(directory):
    for file in files:
        # Compute the hash of the file
        with open(os.path.join(root, file), 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        # Check if the hash already exists in the dictionary
        if file_hash in hashes:
            # If the hash already exists, delete the duplicate file
            os.remove(os.path.join(root, file))
            print(f"Removed duplicate file: {os.path.join(root, file)}")
        else:
            # If the hash doesn't exist, add it to the dictionary
            hashes[file_hash] = os.path.join(root, file)
            print(f"Added file: {os.path.join(root, file)}")


import os
from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
from moviepy.video.fx.all import unsharp_mask, resize

# Specify input and output paths
input_dir = '/home/jack/Desktop/HDD500/collections/quantized/'
output_path = '/home/jack/Desktop/HDD500/complete-videos/Archived_Quantized.mkv'

# Get a list of all the image files in the input directory
image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]

# Define clip duration and frame rate
duration = len(image_files)
fps = 1

# Create a list of resized and sharpened ImageClips from the image files
image_clips = [resize(unsharp_mask(ImageSequenceClip(f, fps=fps)), height=2*f.size[1]) for f in image_files]

# Concatenate the clips into a single video clip
video_clip = ImageSequenceClip(image_clips, fps=fps)

# Write the final video file
video_clip.write_videofile(output_path, codec='libx264', preset='medium')


!pwd


%%writefile Quantize_video.py
from moviepy.editor import ImageSequenceClip
from skimage import io, filters
import glob
# Load the images
image_files =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/quantized/*.jpg'))
output_path = '/home/jack/Desktop/HDD500/complete-videos/Archived_Quantized.mkv'



images = [io.imread(file) for file in image_files]

# Double the size of the images
images = [filters.resize(image, (image.shape[0]*2, image.shape[1]*2))
          for image in images]

# Sharpen the images
images = [filters.unsharp_mask(image) for image in images]

# Create a video clip from the images
clip = ImageSequenceClip(images, fps=1)

# Write the clip to a file
if __name__ =="__main__":
    clip.write_videofile(output_path)


import os
from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
from moviepy.video.fx.all import resize, sharpen

# Specify input and output paths
input_dir = '/home/jack/Desktop/HDD500/collections/quantized/'
output_path = '/home/jack/Desktop/HDD500/complete-videos/Archived_Quantized.mkv'

# Get a list of all the image files in the input directory
image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]

# Define clip duration and frame rate
duration = len(image_files)
fps = 1

# Create a list of resized and sharpened ImageClips from the image files
image_clips = [resize(unsharp_mask(ImageSequenceClip(f, fps=fps)), height=2*f.size[1]) for f in image_files]

# Concatenate the clips into a single video clip
video_clip = ImageSequenceClip(image_clips, fps=fps)

# Write the final video file
video_clip.write_videofile(output_path, codec='libx264', preset='medium')


from PIL import Image
from time import sleep
import random
# Function to change the image size
#sample_size = 80
#image_list = random.sample(imagelist, sample_size)
imagesize = Image.open(image_list[1]).size
print(imagesize)
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
for i in range(1,len(imagelist)): 
# Take two images for blending them together  
    imag1 = imagelist[i]
    imag2 = imagelist[i]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(1,125):
        inc = ic*.008
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        alphaBlended.save(f'{text}{i}{ic:05d}.jpg')

import os
import shutil
import time
# Create the experiment directory if it doesn't exist
if not os.path.exists('/home/jack/Desktop/animate/experiment'):
    os.makedirs('/home/jack/Desktop/animate/experiment')

# Get a sorted list of all PNG files in the current directory
png_files = sorted([f for f in os.listdir('.') if f.endswith('.jpg')])

# Loop through each PNG file and rename it with a zero-padded index
for i, filename in enumerate(png_files):
    # Get the creation time of the file
    creation_time = os.path.getctime(filename)
    
    # Create the new filename with zero-padded index
    new_filename = f'{i+1:05}.jpg'
    
    # Move the file to the experiment directory with the new filename
    shutil.copy(filename, f'/home/jack/Desktop/animate/experiment/{new_filename}')


!rm /home/jack/Desktop/animate/*.jpg

for ic in range(0,125):
    inc = ic*.008
    print(inc,end="-")   

!pwd

#Create a Gif by blending two images

import random
import glob
from PIL import Image
from PIL import Image
# Function to change the image size
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
ImOb = [] 
# Take two images for blending them together  
#imag1 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf4.png"
#imag2 = "/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/cf6.png"
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"

image1 = Image.open(random.choice(glob.glob(DIR)))
DIR = "/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"
image2 = Image.open(random.choice(glob.glob(DIR)))
print(image1," : ",image2)
# Make the images of uniform size
image3 = changeImageSize(512, 768, image1)
image4 = changeImageSize(512, 768, image2)

# Make sure images got an alpha channel
image5 = image3.convert("RGBA")
image6 = image4.convert("RGBA")
text = "/home/jack/Desktop/animate/"
for i in range(1,250):
    inc = i*.004
    #gradually increase opacity
    alphaBlended = Image.blend(image5, image6, alpha=inc)
    alphaBlended = alphaBlended.convert("RGB") 
    ImOb.append(alphaBlended)
    
    alphaBlended.save(f'{text}{i:05d}.jpg')

OPEN_IMAGES = ImOb
#IMAGES = sorted(glob.glob("/home/jack/Desktop/animate/*.jpg"))
IMAGES = sorted(glob.glob("/home/jack/Downloads/saved_pages/WORKSHOP-with-NOTEBOOKS/400x600/*.jpg"))
images = random.sample(IMAGES,20)
#XXXX #ImageBot
overlay = Image.open("/home/jack/Desktop/TENSORFLOW/assets/canvas_texture.png")
overlay2 = Image.open("/home/jack/Desktop/TENSORFLOW/assets/colorful.png")

SIZE= OPEN_IMAGES[4].size
overlay2 = overlay2.resize((SIZE),Image.BICUBIC)
overlay = overlay.resize((SIZE),Image.BICUBIC)
overlay.paste(overlay2,(0,0),overlay2)
#opened_images = [Image.open(img).convert("RGBA") for img in images]
for i, opened_image in enumerate(ImOb):
    opened_image.paste(overlay, (0, 0), overlay)
    opened_image=opened_image.convert("RGB")
    opened_image.save(f"temp{i}.jpg")
#opened_images = [Image.open(f"temp{i}.png") for i in range(len(images))]
opened_images = [Image.open(f"temp{i}.jpg") for i in range(len(images))]
opened_images[0].save("/home/jack/Desktop/TENSORFLOW/mkgif/RANDOM106XX.gif", save_all=True, append_images=opened_images[1:], duration=200, loop=0)


import os
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/output_video2.mp4'
output_images_dir = 'output_images'
fps = 25

# Create output directory for images
os.makedirs(output_images_dir, exist_ok=True)

# Use FFMPEG to extract frames from input video
os.system(f'ffmpeg -i {input_video} -vf fps={fps} {output_images_dir}/input_%04d.jpg')

# Create ImageSequenceClip object from images
clip = ImageSequenceClip(output_images_dir, fps=fps)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)

# Cleanup: delete extracted images
#os.system(f'rm -rf {output_images_dir}')


import os
import random
import glob
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/output_video4.mp4'
output_images_dir = 'output_images'
fps = 25

# Create output directory for images
os.makedirs(output_images_dir, exist_ok=True)

# Use FFMPEG to extract frames from input video
os.system(f'ffmpeg -i {input_video} -vf fps={fps} {output_images_dir}/input_%04d_.jpg')

# Select a random subset of images
image_list = random.sample(glob.glob(f"{output_images_dir}/*_.jpg"), 47)

# Create ImageSequenceClip object from selected images
clip = ImageSequenceClip(image_list, fps=fps)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)

# Cleanup: delete extracted images
#os.system(f'rm -rf {output_images_dir}')


!ls -d */

output_images_dir = 'processed'
image_list = random.sample(glob.glob(f"{output_images_dir}/*.jpg"), 47)


import os
import random
import glob
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
#input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/lexica-warrior_video4.mp4'

#output_images_dir = 'processed'
output_images_dir = 'lexica-warrior'
fps = 25

image_list = random.sample(glob.glob(f"{output_images_dir}/*.jpg"), 40)

# Create ImageSequenceClip object from selected images
clip = ImageSequenceClip(image_list, fps=fps)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)


!ls EXPERIMENT/input_video.mp4

!pwd

import os
import random
import glob
from moviepy.editor import ImageSequenceClip

# Set input/output file names and paths
#input_video = 'EXPERIMENT/output_video.mp4'
output_video = 'EXPERIMENT/lexica-warrior_one_persec.mp4'

#output_images_dir = 'processed'
output_images_dir = 'lexica-warrior'
fps = 24

image_list = random.sample(glob.glob(f"{output_images_dir}/*.jpg"), 40)

# Create ImageSequenceClip object from selected images
clip = ImageSequenceClip(image_list, fps=1)

# Write video file using Moviepy
clip.write_videofile(output_video, codec='libx264', fps=fps)



import os
import glob
from moviepy.editor import ImageSequenceClip
from PIL import Image
from time import sleep
import time
import datetime
#imagelist =sorted(glob.glob("/home/jack/Desktop/TENSORFLOW/SQUARES/cf_square/*.png"))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/quantized/*.jpg'))
#imagelist =sorted(glob.glob('/home/jack/Desktop/HDD500/collections/640x640-alien/*.jpg'))
#imagelist =sorted(glob.glob('build/*.jpg'))
DIR = "/home/jack/Desktop/animate/backgrounds/"
image_list = glob.glob(DIR + "*.jpg")

# Shuffle the image list
random.shuffle(image_list)

# Print the shuffled image list
print(len(image_list))
def changeImageSize(maxWidth, 
                    maxHeight, 
                    image):
    widthRatio  = maxWidth/image.size[0]
    heightRatio = maxHeight/image.size[1]
    newWidth    = int(widthRatio*image.size[0])
    newHeight   = int(heightRatio*image.size[1])
    newImage    = image.resize((newWidth, newHeight))
    return newImage
imagesize = Image.open(image_list[1]).size
for i in range(0,len(image_list)-1): 
# Take two images for blending them together  
    imag1 = image_list[i]
    imag2 = image_list[i+1]
    image1 = Image.open(imag1)
    image2 = Image.open(imag2)

    # Make the images of uniform size
    image3 = changeImageSize(imagesize[0],imagesize[1], image1)
    image4 = changeImageSize(imagesize[0],imagesize[1], image2)

    # Make sure images got an alpha channel
    image5 = image3.convert("RGBA")
    image6 = image4.convert("RGBA")
    text = "/home/jack/Desktop/animate/"
    for ic in range(0,125):
        inc = ic*.008
        sleep(.1)
        #gradually increase opacity
        alphaBlended = Image.blend(image5, image6, alpha=inc)
        alphaBlended = alphaBlended.convert("RGB")
        current_time = datetime.datetime.now()
        filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + '.jpg'
        alphaBlended.save(f'{text}{filename}')
        if ic %25 ==0:print(i,":",ic, end = " . ")

#from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
# Get the list of files sorted by creation time
imagelist = sorted(glob.glob('/home/jack/Desktop/animate/*.jpg'), key=os.path.getmtime)

# Create a clip from the images
clip = ImageSequenceClip(imagelist, fps=30)

# Write the clip to a video file using ffmpeg
current_time = datetime.datetime.now()
filename = current_time.strftime('%Y%m%d_%H%M%S%f')[:-3] + 'june25.mp4'
clip.write_videofile('/home/jack/Desktop/animate/'+filename, fps=24, codec='libx265', preset='medium')

from datetime import datetime
import logging
# Configure logging
logging.basicConfig(filename='logfile.log', level=logging.INFO,
                    format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
def Whatname():
    namelist=["Joe","Ralph","Julia","Norman","Bill", "Mudpie"]
    # Log the time entry
    logging.info(f"Time entry: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    name = input("What is your name?")
    if name in namelist:
        print("Your in the list")
    else:
        print("You are not in the List.")
        logging.info(f"{name} Tried to get in.")
Whatname()            
!cat logfile.log

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'

def make_frame(t):
    with open(text_file_path, 'r') as file:
        lines = file.readlines()
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, lines, fontsize=font_size, color=font_color)
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

video_clip = TextClip(make_frame, duration=duration)
video_clip = video_clip.set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30).set_duration(duration).set_mask(None)

background_clip = ColorClip((width, height), col=background_color).set_duration(duration)
final_clip = CompositeVideoClip([background_clip, video_clip])

final_clip.write_videofile(output_file_path, codec='libx264', fps=30)



from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'

# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color)
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

video_clip = TextClip('', method='custom')
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)


from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'

# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color)
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

video_clip = TextClip('', method='custom')
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)


!ls /home/jack/fonts

from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'
custom_font_path = '/home/jack/fonts/sans.ttf'  # Path to the custom font file


# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color, fontname='CustomFont')
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

# Register the custom font
plt.rcParams['font.family'] = 'CustomFont'
plt.rcParams['font.sans-serif'] = ['CustomFont', 'sans-serif']

video_clip = TextClip('', method='custom', fontfile=custom_font_path)
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/VideoClip.py:1137, in TextClip.__init__(self, txt, filename, size, color, bg_color, fontsize, font, stroke_color, stroke_width, method, kerning, align, interline, tempfilename, temptxt, transparent, remove_temp, print_cmd)
   1136 try:
-> 1137     subprocess_call(cmd, logger=None)
   1138 except (IOError, OSError) as err:

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/tools.py:54, in subprocess_call(cmd, logger, errorprint)
     53         logger(message='Moviepy - Command returned an error')
---> 54     raise IOError(err.decode('utf8'))
     55 else:

OSError: convert: unable to open image `custom:@/tmp/tmpz4kakgwq.txt': No such file or directory @ error/blob.c/OpenBlob/2874.
convert: no images defined `PNG32:/tmp/tmp0b14_sdy.png' @ error/convert.c/ConvertImageCommand/3258.


During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
Cell In[23], line 36
     33 plt.rcParams['font.family'] = 'CustomFont'
     34 plt.rcParams['font.sans-serif'] = ['CustomFont', 'sans-serif']
---> 36 video_clip = TextClip('', method='custom')
     37 video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)
     39 background_clip = ColorClip((width, height), color=background_color)

File ~/miniconda3/envs/cloned_base/lib/python3.9/site-packages/moviepy/video/VideoClip.py:1146, in TextClip.__init__(self, txt, filename, size, color, bg_color, fontsize, font, stroke_color, stroke_width, method, kerning, align, interline, tempfilename, temptxt, transparent, remove_temp, print_cmd)
   1138 except (IOError, OSError) as err:
   1139     error = ("MoviePy Error: creation of %s failed because of the "
   1140              "following error:\n\n%s.\n\n." % (filename, str(err))
   1141              + ("This error can be due to the fact that ImageMagick "
   (...)
   1144                 "ImageMagick binary in file conf.py, or that the path "
   1145                 "you specified is incorrect"))
-> 1146     raise IOError(error)
   1148 ImageClip.__init__(self, tempfilename, transparent=transparent)
   1149 self.txt = txt

OSError: MoviePy Error: creation of None failed because of the following error:

convert: unable to open image `custom:@/tmp/tmpz4kakgwq.txt': No such file or directory @ error/blob.c/OpenBlob/2874.
convert: no images defined `PNG32:/tmp/tmp0b14_sdy.png' @ error/convert.c/ConvertImageCommand/3258.
.

.This error can be due to the fact that ImageMagick is not installed on your computer, or (for Windows users) that you didn't specify the path to the ImageMagick binary in file conf.py, or that the path you specified is incorrect


!ls remarkable_journey  

import cv2
import numpy as np

# Create a blank image/frame
frame = np.zeros((height, width, 3), dtype=np.uint8)
frame.fill(background_color)

# Write the frame to a video file
video_writer = cv2.VideoWriter('remarkable_journey/test_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))
video_writer.write(canvas)
video_writer.release()


!pwd

text_file_path = 'static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt'
new_path = text_file_path[:-3]+"mp4"
output_file_path = new_path.replace("formatted_text","text_video")
print(final_path)

!ls static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt

import cv2
import numpy as np

text_file_path = 'static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt'
new_path = text_file_path[:-3]+"mp4"
output_file_path = new_path.replace("formatted_text","text_video")
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = num_lines * 2  # Adjust the duration as needed
fps = int(num_lines / duration)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y = height + (line_num + 1) * font_size  # Scroll up by the font size multiplied by line number

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print(output_file_path)


!ls static/text_video/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.mp4

import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = num_lines * 10  # Adjust the duration as needed
fps = int(num_lines / duration)
fps =5
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
#fourcc = cv2.VideoWriter_fourcc(*'mp4v')
#fourcc = cv2.VideoWriter_fourcc(*'XVID')
#video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

#video_writer = cv2.VideoWriter('remarkable_journey/test_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))
video_writer = cv2.VideoWriter('remarkable_journey/test_video3.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))
#video_writer.write(canvas)
#video_writer.release()

# Generate frames for the scrolling text
for line in text_content:
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    print(line)
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


!ls /home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4

from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})

from moviepy.editor import TextClip, CompositeVideoClip, ColorClip
from moviepy.video.io.bindings import mplfig_to_npimage
import matplotlib.pyplot as plt

text_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/FORMATED.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/Prepare_to_embark_on_an_extraordinary_journey.mp4'
duration = 480  # Duration of the video in seconds
width, height = 1280, 720  # Dimensions of the video
font_size = 30
font_color = 'white'
background_color = 'black'
custom_font_path = '/home/jack/fonts/sans.ttf'  # Path to the custom font file


# Read the formatted text
with open(text_file_path, 'r') as file:
    formatted_text = file.read()

def make_frame(t):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)
    plt.axis('off')
    plt.text(0, (t / duration) * height, formatted_text, fontsize=font_size, color=font_color, fontname='CustomFont')
    plt.tight_layout()
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0)
    fig.canvas.draw()
    frame = mplfig_to_npimage(fig)
    plt.close(fig)
    return frame

# Register the custom font
plt.rcParams['font.family'] = 'CustomFont'
plt.rcParams['font.sans-serif'] = ['CustomFont', 'sans-serif']
video_clip = TextClip('', method='custom', font='CustomFont=' + custom_font_path)
video_clip = video_clip.set_make_frame(make_frame).set_duration(duration).resize((width, height)).set_position(('center', 'center')).set_fps(30)

background_clip = ColorClip((width, height), color=background_color)
background_clip = background_clip.set_duration(duration)

final_clip = CompositeVideoClip([background_clip, video_clip]).set_duration(duration)
final_clip.write_videofile(output_file_path, codec='libx264', fps=30)

!ls /home/jack/Desktop/content/static/current_project/



#duration = num_lines * 10  # Adjust the duration as needed
fps = duration/num_lines * .2
print(fps)

import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = 20  # Centered horizontally
text_y = height + font_size*2  # Start just below the frame

video_writer = cv2.VideoWriter('remarkable_journey/test_video4.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

# Generate frames for the scrolling text
for line in text_content:
    # Create a copy of the canvas for each frame
    frame = canvas.copy()
    print(line)
    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


!mkdir remarkable_journey

import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey5.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()

    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y + line_num * font_size), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journey6.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height + font_size  # Start just below the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()

    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y += font_size  # Scroll down by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journeyX.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines * .2
print(fps)

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height  # Start at the bottom of the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Create a copy of the canvas for each frame
    frame = canvas.copy()

    # Render the text onto the frame
    cv2.putText(frame, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Update the position for the next frame
    text_y -= font_size  # Scroll up by the font size

    # Write the frame to the video
    video_writer.write(frame)

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


import cv2
import numpy as np

text_file_path = 'remarkable_journey/FORMATED.txt'
output_file_path = 'remarkable_journey/extraordinary_journeyX.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
duration = 10  # Adjust the duration as needed
fps = num_lines / duration

# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height  # Start at the bottom of the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Render the text onto the existing frame (canvas)
    cv2.putText(canvas, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Write the frame to the video
    video_writer.write(canvas)

    # Update the position for the next line
    text_y -= font_size  # Scroll up by the font size

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


!ls /home/jack/fonts

from PIL import Image, ImageDraw, ImageFont

text_file_path = 'remarkable_journey/FORMATED.txt'
output_image_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/image2.png'

canvas_width = 720
canvas_height = 5500
background_color = (0, 0, 0, 0)  # Transparent background
text_color = (255, 255, 255)  # White text color  # Black color
text_position = (15, 15)
#font_path = 'FreeMono.ttf'
font_path = '/home/jack/fonts/Exo-Black.ttf'
font_size = 20

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read()

# Create a new image with white background
canvas = Image.new('RGB', (canvas_width, canvas_height), background_color)

# Load the font
font = ImageFont.truetype(font_path, font_size)

# Create a draw object
draw = ImageDraw.Draw(canvas)

# Draw the text on the canvas
draw.text(text_position, text_content, fill=text_color, font=font)

# Save the image
canvas.save(output_image_path)


!ls remarkable_journey/image2.png

!display remarkable_journey/image2.png





input_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/voice.txt'
output_file_path = '/home/jack/Desktop/content/static/current_project/remarkable_journey/voiceFORMATED.txt'

words_per_line = 4

# Read the input file
with open(input_file_path, 'r') as file:
    content = file.read()

# Split the content into words
words = content.split()

# Format the text with 5 words per line
formatted_words = [' '.join(words[i:i+words_per_line]) for i in range(0, len(words), words_per_line)]
formatted_text = '\n'.join(formatted_words)

# Save the formatted text to the output file
with open(output_file_path, 'w') as file:
    file.write(formatted_text)

!ls static/text/

text_file_path = 'static/text/resourcesFORMATTED.txt'
print (text_file_path[:-3]+"png")

from PIL import Image, ImageDraw, ImageFont

text_file_path = '/home/jack/Desktop/StoryMaker/static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt'

output_image_path = text_file_path[:-3]+"png"

canvas_width = 720
canvas_height = 1200
background_color = (0, 0, 0, 0)  # Transparent background
text_color = (255, 255, 255)  # White text color  # Black color
text_position = (15, 15)
#font_path = 'FreeMono.ttf'
font_path = '/home/jack/fonts/Exo-Black.ttf'
font_size = 20

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read()

# Create a new image with white background
canvas = Image.new('RGB', (canvas_width, canvas_height), background_color)

# Load the font
font = ImageFont.truetype(font_path, font_size)

# Create a draw object
draw = ImageDraw.Draw(canvas)

# Draw the text on the canvas
draw.text(text_position, text_content, fill=text_color, font=font)

# Save the image
canvas.save(output_image_path)


!display static/images/resourcesFORMATTED.png



import cv2
import numpy as np

#text_file_path = 'remarkable_journey/FORMATED.txt'
#output_file_path = 'remarkable_journey/extraordinary_journeyX1.mp4'
# Video parameters
width, height = 1280, 720  # Dimensions of the video frame
font_size = 30
font_color = (255, 255, 255)  # White color (BGR format)
background_color = 0  # Scalar value for black color

# Read the text file
with open(text_file_path, 'r') as file:
    text_content = file.read().splitlines()

# Determine the duration and frame rate based on the text length
num_lines = len(text_content)
fps = duration/num_lines
print(fps)
# Create a blank image/frame
canvas = np.zeros((height, width, 3), dtype=np.uint8)
canvas.fill(background_color)

# Set the initial position of the text
text_x = int(width / 2)  # Centered horizontally
text_y = height  # Start at the bottom of the frame

# Initialize the video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))

# Generate frames for the scrolling text
for line_num, line in enumerate(text_content):
    # Render the text onto the existing frame (canvas)
    cv2.putText(canvas, line, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, font_color, 2)

    # Write the frame to the video
    video_writer.write(canvas)

    # Update the position for the next line
    text_y -= font_size  # Scroll up by the font size

# Release the video writer and finalize the video
video_writer.release()

print("Vertical scrolling video created successfully.")


from moviepy.editor import *

# Load the text file
text_file = open("/home/jack/Desktop/StoryMaker/static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt", "r")
text_content = text_file.read()
text_file.close()

# Define the video dimensions and scrolling duration
video_width, video_height = 768, 512
scroll_duration = 10

# Create a function to animate the text's vertical position
def scroll_down(t):
    y_pos = -int((len(text_content.split('\n')) * 20) * t / scroll_duration)
    return TextClip(text_content, fontsize=20, color='white', font='Arial', align='center', size=(video_width, video_height)).set_position(('center', y_pos))

# Create the scrolling effect clip
scroll_clip = VideoClip(scroll_down, duration=scroll_duration)

# Set the video dimensions explicitly
scroll_clip = scroll_clip.set_resolution((video_width, video_height))

# Write the scrolling clip to a file
scroll_clip.write_videofile("scrolling_video.mp4", codec="libx264", fps=30)


from moviepy.editor import *
import numpy as np

# Load the image
image = ImageClip("/home/jack/Desktop/StoryMaker/static/images/resourcesFORMATTED.png")

# Define the duration and desired video dimensions
scroll_duration = 5  # Adjust the duration as needed
video_width, video_height = 720, 720

# Calculate the height difference between the image and the desired video dimensions
height_difference = image.h - video_height

# Create a custom video clip class for the scrolling effect
class ScrollingClip(VideoClip):
    def __init__(self, image, duration, video_width, video_height):
        VideoClip.__init__(self)
        self.image = image
        self.duration = duration
        self.video_width = video_width
        self.video_height = video_height

    def make_frame(self, t):
        y_pos = -int(height_difference * t / self.duration)
        image_positioned = self.image.set_position(("center", y_pos))
        return image_positioned.get_frame(t)

# Create the scrolling effect clip
scroll_clip = ScrollingClip(image, scroll_duration, video_width, video_height)

# Write the scrolling clip to a file
scroll_clip.write_videofile(
    "scrolling_video.mp4",
    codec="libx264",
    fps=30,
    audio=False,
    threads=4,
    resolution=(video_width, video_height)
)


!vlc vertical_video.mp4




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/transparent-circle.ipynb
Code Content:
from PIL import Image
import glob
import random
import uuid
# Load the images you want to stitch
def stitched(IMAGE1,IMAGE2):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)

    # Get the dimensions of the images
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Determine the dimensions of the stitched image
    stitched_width = width1 + width2
    stitched_height = max(height1, height2)

    # Create a new blank image with the stitched dimensions
    stitched_image = Image.new('RGB', (stitched_width, stitched_height))

    # Paste the images onto the stitched image
    stitched_image.paste(image1, (0, 0))
    stitched_image.paste(image2, (width1, 0))

    # Save the stitched image
    filename = "static/images/giger/stitched"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)
    print (filename)
    return filename

IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
FileName = stitched(IMAGE1,IMAGE2)

!ls static/images/giger/stitchedcd793035-fac6-48cb-8065-dd7c3281c8a3.jpg

im = Image.open(FileName)
im

from PIL import Image

def blend_images(image1, image2, alpha):
    blended_image = Image.new('RGB', image1.size)

    for x in range(image1.width):
        for y in range(image1.height):
            pixel1 = image1.getpixel((x, y))
            pixel2 = image2.getpixel((x, y))

            blended_pixel = (
                int((1 - alpha) * pixel1[0] + alpha * pixel2[0]),
                int((1 - alpha) * pixel1[1] + alpha * pixel2[1]),
                int((1 - alpha) * pixel1[2] + alpha * pixel2[2])
            )

            blended_image.putpixel((x, y), blended_pixel)

    return blended_image



# Load the images you want to stitch
def stitched(IMAGE1,IMAGE2):

    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)

    # Adjust the blending factor (0.0 to 1.0)
    alpha = 0.5

    # Blend the images
    stitched_image = blend_images(image1, image2, alpha)

    # Save the blended image
    filename = "static/images/giger/stitched/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)
    return filename
IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
FileName = stitched(IMAGE1,IMAGE2)

im = Image.open(FileName)
im

from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,250):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)
    # Save the stitched image
    filename = "static/images/giger/stitched/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)



from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,250):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/stitched/long/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/stitched/long/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)

    # Save the stitched image
    filename = "static/images/giger/stitched/long/longest/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)





from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,2):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)

    # Save the stitched image
    filename = "static/images/giger/superlong"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)





!ls static/images/giger/superlong*

import numpy as np
from moviepy.editor import VideoClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/superlongd7c865c1-4663-4d56-a348-f0d2f77b553e.jpg"))
image_path = "static/images/giger/superlong3889669f-bc39-40b7-9518-6ec8eb9bb46c.jpg"
#image_clip = VideoFileClip(image_path, audio=False)

image = np.array(Image.open(image_path))

# Set parameters
output_size = (768, 512)  # Output video dimensions
#scroll_speed = 50  # Pixels per second
scroll_speed = 10  # Pixels per second
# Function to generate frames for the scrolling video
def make_frame(t):
    x_offset = int(t * scroll_speed)
    frame = image[:, x_offset:x_offset + output_size[0], :]
    return frame

# Create the scrolling video clip
duration = image.shape[1] / scroll_speed  # Total duration to scroll the entire image
scrolling_video = VideoClip(make_frame, duration=duration)

# Resize the video to the desired output size
scrolling_video = scrolling_video.resize(output_size)

# Save the scrolling video as scrolling_video.mp4
output_path = 'scrolling_videolongest11w.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=30)


!vlc scrolling_videolongest11w.mp4

from moviepy.editor import VideoClip, clips_array
from moviepy.video.io.VideoFileClip import VideoFileClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
image_clip = VideoFileClip(image_path, audio=False)

# Set the desired output dimensions (512x512)
output_width, output_height = 512, 512

# Calculate the scrolling duration based on your requirement (e.g., 4 minutes)
scrolling_duration = 4 * 60  # 4 minutes in seconds

# Function to create the scrolling effect
def scrolling_frame(t):
    x_offset = int((image_clip.w - output_width) * t / scrolling_duration)
    frame = image_clip.get_frame(t)[x_offset:x_offset + output_width, :, :]
    return frame

# Create the scrolling video clip
scrolling_video = VideoClip(scrolling_frame, duration=scrolling_duration)

# Resize the video to 512x512
scrolling_video = scrolling_video.resize(newsize=(output_width, output_height))

# Save the video as scrolling_video.mp4
output_path = 'scrolling_video.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=24)


import numpy as np
from moviepy.editor import VideoClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
#image_clip = VideoFileClip(image_path, audio=False)

image = np.array(Image.open(image_path))

# Set parameters
output_size = (512, 512)  # Output video dimensions
scroll_speed = 50  # Pixels per second

# Function to generate frames for the scrolling video
def make_frame(t):
    x_offset = int(t * scroll_speed)
    frame = image[:, x_offset:x_offset + output_size[0], :]
    return frame

# Create the scrolling video clip
duration = image.shape[1] / scroll_speed  # Total duration to scroll the entire image
scrolling_video = VideoClip(make_frame, duration=duration)

# Resize the video to the desired output size
scrolling_video = scrolling_video.resize(output_size)

# Save the scrolling video as scrolling_video.mp4
output_path = 'scrolling_video1.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=30)


!vlc scrolling_video1.mp4

image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/.jpg"))



stitched_image

!cp /home/jack/Desktop/StoryMaker/downloadz/00135.jpg start.jpg

from PIL import Image, ImageFilter

# Open the image
img = Image.open("/home/jack/Desktop/StoryMaker/downloadz/00135.jpg")

# Create a new image with the same size as the original
new_img = Image.new("RGBA", img.size)

# Paste the original image onto the new image
new_img.paste(img, (0, 0))

# Create a mask with a gradient from 255 to 0
mask = Image.new("L", img.size, 255)
for x in range(img.width - 50, img.width):
    alpha = int(255 * (x - (img.width - 50)) / 50)
    for y in range(img.height):
        mask.putpixel((x, y), alpha)

# Blur the mask
mask = mask.filter(ImageFilter.GaussianBlur(10))

# Apply the mask to the new image
new_img.putalpha(mask)

# Create a new mask with a gradient from 255 to 0 over the rightmost edge of the image
mask2 = Image.new("L", img.size, 255)
for y in range(img.height):
    alpha = int(255 * (y - (img.height - 1)) / (img.height - 1))
    mask2.putpixel((img.width - 1, y), alpha)

# Apply the new mask to the new image
new_img.putalpha(mask2)

# Save the new image as a PNG file
new_img.save("feather_transparent.png")


from PIL import Image, ImageDraw, ImageOps

def feather_image(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering
        mask = Image.new("L", input_image.size, 255)
        draw = ImageDraw.Draw(mask)
        for y in range(input_image.height):
            alpha = int(255 * (1 - (y - 450) / feather_width))
            draw.line([(0, y), (input_image.width, y)], fill=alpha)
        
        # Apply the mask to the output image
        output_image.putalpha(mask)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered.png"
    feather_width = 52
    feather_image(input_path, output_path, feather_width)


im = Image.open("feathered.png")
im

from PIL import Image, ImageDraw, ImageOps

def feather_image(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering
        mask = Image.new("L", input_image.size, 255)
        draw = ImageDraw.Draw(mask)
        for x in range(input_image.width - feather_width, input_image.width):
            alpha = int(255 * (1 - (x - (input_image.width - feather_width)) / feather_width))
            draw.line([(x, 0), (x, input_image.height)], fill=alpha)
        
        # Apply the mask to the output image
        output_image.putalpha(mask)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered_right.png"
    feather_width = 52
    feather_image(input_path, output_path, feather_width)


im = Image.open("feathered_right.png")
im

from PIL import Image, ImageDraw, ImageOps

def feather_left_side(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering the left side
        mask_left = Image.new("L", input_image.size, 255)
        #mask_left = Image.new("L", input_image.size), 255)
        #draw = ImageDraw.Draw(mask)
        draw_left = ImageDraw.Draw(mask_left)
        #for x in range(input_image.width - feather_width, input_image.width):
        #    alpha = int(255 * (1 - (x - (input_image.width - feather_width)) / feather_width))
        #    draw.line([(x, 0), (x, input_image.height)], fill=alpha)               
        for x in range(feather_width):
            alpha = int(255 * (1 - x / feather_width))
            draw_left.line([(x, 0), (x, input_image.height)], fill=alpha)
        
        # Apply the left mask to the output image
        output_image.paste((0, 0, 0, 0), (0, 0, feather_width, input_image.height))
        output_image.paste(ImageOps.colorize(mask_left, (0, 0, 0, 0), (0, 0, 0, 255)), (0, 0), mask_left)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Left side feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered_left.png"
    feather_width = 52
    feather_left_side(input_path, output_path, feather_width)


im = Image.open("feathered_left.png")
im

import numpy as np
import cv2
import matplotlib.pyplot as plt
import uuid
input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
giger = cv2.imread(input_path)
l_row, l_col, nb_channel = giger.shape
rows, cols = np.mgrid[:l_row, :l_col]
radius = np.sqrt((rows - l_row/2)**2 + (cols - l_col/2)**2)
alpha_channel = np.zeros((l_row, l_col))
#change .8 to .6
r_min, r_max = 1./3 * radius.max(), 0.6 * radius.max()
alpha_channel[radius < r_min] = 1
alpha_channel[radius > r_max] = 0
gradient_zone = np.logical_and(radius >= r_min, radius <= r_max)
alpha_channel[gradient_zone] = (r_max - radius[gradient_zone])/(r_max - r_min)
alpha_channel *= 255
feathered = np.empty((l_row, l_col, nb_channel + 1), dtype=np.uint8)
feathered[..., :3] = astro[:]
feathered[..., -1] = alpha_channel[:]

# Display the image without plot elements
plt.axis('off')
plt.imshow(feathered)
plt.show()

# Save the image without plot elements
output_path = "feathered_image.png"
plt.imsave(output_path, feathered[..., :3], format="png")




import numpy as np
from skimage import data
import cv2
input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
astro = cv2.imread(input_path)
l_row, l_col, nb_channel = astro.shape
rows, cols = np.mgrid[:l_row, :l_col]
radius = np.sqrt((rows - l_row/2)**2 + (cols - l_col/2)**2)
alpha_channel = np.zeros((l_row, l_col))
r_min, r_max = 1./3 * radius.max(), 0.8 * radius.max()
alpha_channel[radius < r_min] = 1
alpha_channel[radius > r_max] = 0
gradient_zone = np.logical_and(radius >= r_min, radius <= r_max)
alpha_channel[gradient_zone] = (r_max - radius[gradient_zone])/(r_max - r_min)
alpha_channel *= 255
feathered = np.empty((l_row, l_col, nb_channel + 1), dtype=np.uint8)
feathered[..., :3] = astro[:]
feathered[..., -1] = alpha_channel[:]

import matplotlib.pyplot as plt
plt.imshow(feathered)
plt.show()

import os
dirs = os.listdir("static")
print (dirs)

import os
dirs = os.listdir("static")
      

import os

# Specify the root directory you want to start walking from
root_directory = "static"

# Walk through all directories and subdirectories
for dirpath, dirnames, filenames in os.walk(root_directory):
    print("Current Directory:", dirpath)
    print("Subdirectories:", dirnames)
    print("Files:", filenames)
    print("=" * 30)

import os
import random
import glob 
# Specify the root directory you want to start walking from
root_directory = "static"
MP4 =[]
# Walk through all directories and subdirectories
for dirpath, dirnames, filenames in os.walk(root_directory):
    for filename in filenames:
        if filename.endswith(".mp4"):
            data = os.path.join(dirpath, filename)
            MP4.append(data)
            
            
for filename in MP4:
    if "woman" in filename:
        video = random.choice(glob.glob(filename))  
print(video)



print(len(MP4))

for filename in MP4:
    if "woman" in filename:
        print(filename)

import os
directories = os.listdir("static/images")
print (directories)

import re

def get_route_lines():
    with open('MemMakerWdb', 'r') as file:
        route_lines = [re.split(r'[,/]', line.strip())[1]
                       for line in file
                       if line.startswith('@app.route')]
    return route_lines
cnt = 0
for route in route_lines:
    if len(route)>2:
        cnt = cnt +1
        print(cnt,route)

import re
def get_route_lines():
    route_lines = []
    with open('MemMakerWdb', 'r') as file:
        for line in file:
            if line.startswith('@app.route'):
                line = line.replace('@app.route',"")
                line = line.replace('(',"")
                line = line.replace(')',"")
                line = line.replace('"','')
                line = line.replace('\'','')
                linez = re.split(r'[,/]', line)
                linez = linez[1]
                route_lines.append(linez.strip())
    return route_lines
route_lines = get_route_lines()
cnt = 0
for route in route_lines:
    cnt = cnt +1
    print(cnt,route)


<a href="{{ url_for('add_border') }}">Add a border to an image.</a><br />

inputs = open("templates/all_routes.html","w")
routes = open("static/text/routes.txt").readlines()
for route in route_lines:
    if "favicon.ico" not in route:
        route = route.replace("\n","")
        line ="<a href=\"{{ url_for('"+route+"') }}\">"+route+"</a><br />\n"
        inputs.write(line)

inputs.close()

read = open("templates/all_routes.html","r").readlines()
for line in read:
    print(line)

read = open("templates/all_routes.html","r").readlines()
for line in read:
    if '""' in line:
        print(line)




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/zoom.ipynb
Code Content:
!mkdir zooms

/home/jack/Desktop/HDD500/collections/Music/Blue_Mood-Robert_Munzinger.mp3

from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music

# Load video and audio clips
video_clip = VideoFileClip("steampunk/slideshowSharp.mp4")
audio_clip = AudioFileClip(music())

# Ensure the audio clip is the same duration as the video
if audio_clip.duration > video_clip.duration:
    audio_clip = audio_clip.subclip(0, video_clip.duration)
else:
    padding_duration = video_clip.duration - audio_clip.duration
    padding = AudioFileClip.silence(duration=padding_duration)
    audio_clip = concatenate_audioclips([audio_clip, padding])

# Define fade durations
fade_in_duration = 10  # seconds
fade_out_duration = 15  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Apply fade out effect to audio at the end
final_audio = faded_audio.audio_fadeout(fade_out_duration)

# Set audio of the video to the processed audio
video_with_audio = video_clip.set_audio(final_audio)

# Write the final video with audio to a file
output_path = "steampunk/slideshowSharp-music.mp4"
video_with_audio.write_videofile(output_path, codec="libx264")


!vlc steampunk/slideshowSharp-music.mp4

!ffmpeg -i /home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/steampunk.mp4 -vf "unsharp=5:5:1.0:5:5:0.0" -c:a copy /home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Sharpensteampunk.mp4


!pwd

from moviepy.editor import VideoFileClip

def sharpen_video(input_path, output_path, strength):
    # Load the video clip
    video_clip = VideoFileClip(input_path)

    # Define a custom video effect to apply sharpening
    def sharpen_frame(t):
        # Get the frame at time t
        frame = video_clip.get_frame(t)

        # Apply sharpening to the frame
        sharpened_frame = frame * (0.5 + strength)
        return sharpened_frame

    # Apply the custom effect to each frame of the video
    sharpened_clip = video_clip.fl_image(sharpen_frame)

    # Set the audio
    sharpened_clip = sharpened_clip.set_audio(video_clip.audio)

    # Write the processed video to a new file
    sharpened_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')

# Specify the input and output file paths
input_video_path = 'static/images/Elektra-Weapons/steampunk.mp4'
output_video_path = 'static/images/Elektra-Weapons/SSsteampunk-5-9nk.mp4'

# Call the sharpen_video function
sharpen_video(input_video_path, output_video_path, strength=0.9)  # Adjust strength as needed


from moviepy.editor import VideoFileClip

def sharpen_video(input_path, output_path, strength):
    # Load the video clip
    video_clip = VideoFileClip(input_path)

    # Define a custom video effect to apply sharpening
    def sharpen_frame(t):
        # Get the frame at time t
        frame = video_clip.get_frame(t)

        # Apply sharpening to the frame
        sharpened_frame = frame * (0.5 + strength)
        return sharpened_frame

    # Apply the custom effect to each frame of the video
    sharpened_clip = video_clip.fl(sharpen_frame)

    # Set the audio
    sharpened_clip = sharpened_clip.set_audio(video_clip.audio)

    # Write the processed video to a new file
    sharpened_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')

# Specify the input and output file paths
input_video_path = 'static/images/Elektra-Weapons/steampunk.mp4'
output_video_path = 'static/images/Elektra-Weapons/SSsteampunk-5-nkl.mp4'

# Call the sharpen_video function
sharpen_video(input_video_path, output_video_path, strength=0.9)  # Adjust strength as needed


# save this
from moviepy.editor import VideoFileClip

def sharpen_video(input_path, output_path, strength):
    # Load the video clip
    video_clip = VideoFileClip(input_path)

    # Define a custom video effect to apply sharpening
    def sharpen_frame(frame):
        # Apply sharpening to the frame
        #sharpened_frame = frame * (1 + strength)
        sharpened_frame = frame * (.5 + strength)
        return sharpened_frame

    # Apply the custom effect to each frame of the video
    sharpened_clip = video_clip.fl(lambda gf, t: sharpen_frame(gf(t)))

    # Set the audio
    sharpened_clip = sharpened_clip.set_audio(video_clip.audio)

    # Write the processed video to a new file
    sharpened_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')

# Specify the input and output file paths
input_video_path = 'static/images/Elektra-Weapons/steampunk.mp4'
output_video_path = 'static/images/Elektra-Weapons/SSsteampunk-5-9.mp4'

# Call the sharpen_video function
sharpen_video(input_video_path, output_video_path, strength=0.9)  # Adjust strength as needed


from moviepy.editor import VideoFileClip
from moviepy.video import fx

def sharpen_video(input_path, output_path, strength=1.0):
    # Load the video clip
    video_clip = VideoFileClip(input_path)

    # Apply the sharpening effect using the vfx.sharpen function
    sharpened_clip = video_clip.fx(fx.vfx.sharpen, strength)

    # Write the processed video to a new file
    sharpened_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')

# Specify the input and output file paths
input_video_path = 'static/images/Elektra-Weapons/steampunk.mp4'
output_video_path = 'static/images/Elektra-Weapons/SSsteampunk-5-n.mp4'

# Call the sharpen_video function
sharpen_video(input_video_path, output_video_path, strength=0.5)  # Adjust strength as needed


!vlc static/images/Elektra-Weapons/SSsteampunk-5-9.mp4



from moviepy.editor import VideoFileClip
from moviepy.video.fx import resize

def sharpen_video(input_path, output_path, strength=1.0):
    # Load the video clip
    video_clip = VideoFileClip(input_path)

    # Apply the sharpening effect
    sharpened_clip = video_clip.fx(resize, lambda t: 1 + strength)

    # Set the audio
    sharpened_clip = sharpened_clip.set_audio(video_clip.audio)

    # Write the processed video to a new file
    sharpened_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')

# Specify the input and output file paths
input_video_path = 'static/images/Elektra-Weapons/steampunk.mp4'
output_video_path = 'static/images/Elektra-Weapons/Ssharpened_video.mp4'

# Call the sharpen_video function
sharpen_video(input_video_path, output_video_path, strength=0.5)  # Adjust strength as needed


!vlc /home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Sharpensteampunk.mp4

from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music

# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End.mp4")
audio_clip = AudioFileClip(music())

# Ensure the audio clip is the same duration as the video
if audio_clip.duration > video_clip.duration:
    audio_clip = audio_clip.subclip(0, video_clip.duration)
else:
    padding_duration = video_clip.duration - audio_clip.duration
    padding = AudioFileClip.silence(duration=padding_duration)
    audio_clip = concatenate_audioclips([audio_clip, padding])

# Define fade durations
fade_in_duration = 10  # seconds
fade_out_duration = 15  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Apply fade out effect to audio at the end
final_audio = faded_audio.audio_fadeout(fade_out_duration)

# Set audio of the video to the processed audio
video_with_audio = video_clip.set_audio(final_audio)

# Write the final video with audio to a file
output_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End_Zoomed-test.mp4"
video_with_audio.write_videofile(output_path, codec="libx264")


from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music

# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End.mp4")
audio_clip = AudioFileClip(music())

# Ensure the audio clip is the same duration as the video
if audio_clip.duration > video_clip.duration:
    audio_clip = audio_clip.subclip(0, video_clip.duration)
else:
    padding_duration = video_clip.duration - audio_clip.duration
    padding = AudioFileClip.silence(duration=padding_duration)
    audio_clip = concatenate_audioclips([audio_clip, padding])

# Define fade durations
fade_in_duration = 15  # seconds
fade_out_duration = 10  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Apply fade out effect to audio at the end
final_audio = faded_audio.audio_fadeout(fade_out_duration)

# Set audio of the video to the processed audio
video_with_audio = video_clip.set_audio(final_audio)

# Load the image overlay
overlay_image = ImageClip("/home/jack/Desktop/StoryMaker/static/assets/Glitch_Art_Frame_512x768.png")

# Resize the overlay image to match the video dimensions
overlay_image = overlay_image.resize(width=video_with_audio.w, height=video_with_audio.h)

# Set the duration of the overlay image to match the video duration
overlay_image = overlay_image.set_duration(video_with_audio.duration)

# Composite the video with the overlay image
final_video = CompositeVideoClip([video_with_audio, overlay_image])

# Write the final video with audio and overlay to a file
output_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End_Zoomed_With_Overlayframed.mp4"
final_video.write_videofile(output_path, codec="libx264")


!vlc /home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End_Zoomed_With_Overlayframed.mp4

from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music

# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End.mp4")
audio_clip = AudioFileClip(music())

# Define fade durations
fade_in_duration = 10  # seconds
fade_out_duration = 15  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Calculate the duration for synchronization
audio_start_time = max(video_clip.duration - faded_audio.duration, 0)

# Set audio to start at the calculated point
synced_audio = faded_audio.subclip(audio_start_time)

# Apply fade out effect to audio at the end
final_audio = synced_audio.audio_fadeout(fade_out_duration)

# Set audio of the video to the processed audio
video_with_audio = video_clip.set_audio(final_audio)

# Get the minimum duration between video and audio
min_duration = min(video_with_audio.duration, synced_audio.duration)

# Set video and audio to desired duration
video_duration = 58  # seconds
video_with_audio = video_with_audio.subclip(0, min(video_duration, min_duration))

# Write the final video with audio to a file
output_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End_Zoomed.mp4"
video_with_audio.write_videofile(output_path, codec="libx264")


from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips, AudioClip
import numpy as np

make_frame = lambda t: np.sin(440 * 2 * np.pi * t)
clip = AudioClip(make_frame, duration=.5, fps=44100)
clip.preview()


# Plays the note A in mono (a sine wave of frequency 440 Hz)
import numpy as np

# Plays the note A in stereo (two sine waves of frequencies 440 and 880 Hz)
make_frame = lambda t: np.array([
    np.sin(440 * 2 * np.pi * t),
    np.sin(880 * 2 * np.pi * t)
]).T.copy(order="C")
clip = AudioClip(make_frame, duration=3, fps=44100)
clip.preview()

import numpy as np
from scipy.io import wavfile
import subprocess

# Generate the stereo audio clip
def make_frame(t):
    return np.array([
        np.sin(440 * 2 * np.pi * t),
        np.sin(880 * 2 * np.pi * t)
    ]).T.copy(order="C")

duration = 3  # seconds
fps = 44100
sample_rate = 44100
num_channels = 2
num_samples = int(sample_rate * duration)
audio_data = np.array([make_frame(t) for t in np.linspace(0, duration, num_samples)])

# Save the audio data as a WAV file
wavfile.write("zooms/tone.wav", sample_rate, audio_data)

# Convert the WAV file to MP3 using FFmpeg
subprocess.run(["ffmpeg", "-i", "zooms/tone.wav", "zooms/tone.mp3"])

print("Audio saved as tone.mp3")


# creates a small transparent overlay that as it get larger increates in opacity 
from PIL import Image
import subprocess
import uuid
def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    SIZE = bg.size
    bg = bg.resize((SIZE), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((SIZE), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(200):
        size = (int(fg_copy.width * (i+1)/200), int(fg_copy.height * (i+1)/200))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/200))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/200))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
    
    # Save the resulting images as a GIF animation
    result_images[0].save('gifs/zoom_effect2.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)

overlay_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
base_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
zoom_effect(base_image_path, overlay_image_path)
# Convert the WAV file to MP3 using FFmpeg
base_filename = str(uuid.uuid4())
subprocess.run(["ffmpeg", "-i", "gifs/zoom_effect2.gif", "gifs/"+base_filename+".mp4"])


from moviepy.editor import AudioFileClip

# Load the audio clip
audio_clip = AudioFileClip(music())

# Define fade durations
fade_in_duration = 10  # seconds
fade_out_duration = 15  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Calculate the duration for synchronization
audio_start_time = max(58 - faded_audio.duration, 20)

# Set audio to start at the calculated point
synced_audio = faded_audio.subclip(audio_start_time)

# Apply fade out effect to audio at the end
audio_clip = synced_audio.audio_fadeout(fade_out_duration)

# Trim the audio clip to exactly 58 seconds
final_audio_clip = audio_clip.subclip(0, 58)

print("Audio clip duration:", final_audio_clip.duration, "seconds")
final_audio_clip.preview()


from moviepy.editor import AudioFileClip
import random
from scipy.io import wavfile
import uuid
import glob
import random
# Function to find a random song in the Music directory
def music():
    MUSIC = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/Music/*.mp3"))
    return MUSIC

audio_clip = AudioFileClip(music())  # Replace with your audio file path

# Define fade durations
fade_in_duration = 10  # seconds
fade_out_duration = 10  # seconds

# Calculate the maximum possible start time to ensure the selected section fits within the audio clip
max_start_time = audio_clip.duration - (58 - fade_in_duration - fade_out_duration)

# Choose a random start time within the valid range
random_start_time = random.uniform(0, max_start_time)

# Calculate the end time based on the random start time and desired total duration
random_end_time = random_start_time + (58 - fade_out_duration)

# Extract the random section from the audio clip
random_section = audio_clip.subclip(random_start_time, random_end_time)

# Apply fade-in and fade-out effects
faded_audio = random_section.audio_fadein(fade_in_duration).audio_fadeout(fade_out_duration)

# Make sure the resulting audio clip is exactly 58 seconds long
final_audio_clip = faded_audio.subclip(0, 58)

# Print the duration of the final audio clip
print("Final audio clip duration:", final_audio_clip.duration, "seconds")

# Preview the final audio clip
final_audio_clip.preview()

base_filename = str(uuid.uuid4())
# Save the audio as a WAV file
clip=final_audio_clip
wavfile.write("zooms/"+base_filename+".wav", int(clip.fps), clip.to_soundarray())
import subprocess

# Convert the WAV file to MP3 using FFmpeg
subprocess.run(["ffmpeg", "-i", "zooms/"+base_filename+".wav", "zooms/"+base_filename+".mp3"])

print("Audio saved as zooms/"+base_filename+".wav")

clip.preview()

!vlc zooms/audio02b.mp3

from scipy.io import wavfile

# Save an audio clip as a WAV file
clip=final_audio_clip
wavfile.write("zooms/audio02.wav", int(clip.fps), clip.to_soundarray())
import subprocess

# Convert the WAV file to MP3 using FFmpeg
subprocess.run(["ffmpeg", "-i", "zooms/audio02.wav", "zooms/audio02.mp3"])

print("Audio saved as zooms/audio02.wav")


# working but not wanted
from moviepy.editor import VideoFileClip
import random

# Load the video clip
video_clip = VideoFileClip("/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End.mp4")

# Define the desired duration of the random section
desired_duration = 1  # seconds

# Calculate the maximum possible start time to ensure the selected section fits within the video
max_start_time = video_clip.duration - desired_duration

# Choose a random start time within the valid range
random_start_time = random.uniform(0, max_start_time)

# Extract the random section from the video
random_section = video_clip.subclip(random_start_time, random_start_time + desired_duration)

# Print the duration of the random section
print("Random section duration:", random_section.duration, "seconds")

# Preview the random section
random_section.preview()


from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips, AudioClip
audio_clip = AudioFileClip(music())

# Define fade durations
fade_in_duration = 10  # seconds
fade_out_duration = 15  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Calculate the duration for synchronization

#audio_start_time = max(video_clip.duration - faded_audio.duration, 0)
audio_start_time = max(58 - faded_audio.duration, 20)
# Set audio to start at the calculated point
synced_audio = faded_audio.subclip(audio_start_time)

# Apply fade out effect to audio at the end
audio_clip = synced_audio.audio_fadeout(fade_out_duration)
print("Audio clip duration:", audio_clip.duration, "seconds")
audio_clip.preview()

from moviepy.editor import AudioFileClip

# Load the audio clip
audio_clip = AudioFileClip("your_audio_file.mp3")  # Replace with your audio file path

# Print the duration of the audio clip
print("Audio clip duration:", audio_clip.duration, "seconds")


!vlc zooms/audio01.wav

print(audio_clip)

clip.fx(audio_fadein, "00:00:06")

from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music
print (music())
# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/abd0aebb-ce72-4a30-a233-14ee14f5b841random_video_58s.mp4")
clip = AudioFileClip(music())
clip.fx(audio_fadein, "00:00:10")
clip.preview()

clip = AudioFileClip(music())

clip.fx(audio_fadein, "00:00:26")
clip.preview()



import moviepy.audio.fx.all as afx
clip = AudioFileClip(music())

clip.fx(afx.audio_fadein, "00:00:26")

clip.preview()

from moviepy.editor import *
clip = AudioFileClip(music())
clip.fx(audio_fadein, "00:00:06")
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[43], line 3
      1 clip = AudioFileClip(music())
----> 3 clip.fx(audio_fadein, "00:00:06")

NameError: name 'audio_fadein' is not defined

from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music
print (music())
# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/abd0aebb-ce72-4a30-a233-14ee14f5b841random_video_58s.mp4")
clip = AudioFileClip(music())


# Define fade durations
fade_in_duration = 1  # seconds
fade_out_duration = 1  # seconds

# Apply fade in effect to audio
clip.fx(audio_fadein, "00:00:20")
clip.preview()


#faded_audio = audio_clip.fx.fadein(fade_in_duration)

# Calculate the duration for synchronization
#audio_start_time = max(video_clip.duration - faded_audio.duration, 0)

from moviepy.editor import VideoFileClip
from moviepy.audio.fx.all import audio_fadein

# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End.mp4")
audio_clip = AudioFileClip(music())

# Define fade durations
fade_in_duration = 5  # seconds
fade_out_duration = 1  # seconds

# Apply fade in effect to audio
faded_audio = audio_fadein(audio_clip, fade_in_duration)

# Trim the audio to match the video duration
trimmed_audio = faded_audio.subclip(0, video_clip.duration)

# Apply fade out effect to audio at the end
final_audio = trimmed_audio.audio_fadeout(trimmed_audio, fade_out_duration)

# Set audio of the video to the processed audio
video_with_audio = video_clip.set_audio(final_audio)

# Write the final video with audio to a file
output_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End_Zommed.mp4"
video_with_audio.write_videofile(output_path, codec="libx264")


from moviepy.editor import *
from moviepy.audio.fx.all import audio_fadein
from MUSIC import music
print (music())
# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End.mp4")
audio_clip = AudioFileClip(music())


# Define fade durations
fade_in_duration = 5  # seconds
fade_out_duration = 1  # seconds

# Apply fade in effect to audio
faded_audio = audio_clip.audio_fadein(fade_in_duration)

# Calculate the duration for synchronization
audio_start_time = max(video_clip.duration - faded_audio.duration, 0)

# Set audio to start at the calculated point
synced_audio = faded_audio.subclip(audio_start_time)

# Apply fade out effect to audio at the end
final_audio = synced_audio.audio_fadeout(fade_out_duration)

# Set audio of the video to the processed audio
video_with_audio = video_clip.set_audio(final_audio)

# Write the final video with audio to a file
output_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/Final_End_Zoomed.mp4"
video_with_audio.write_videofile(output_path, duration=58,codec="libx264")




!vlc output.mp4

from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips
from MUSIC import music
print (music())
# Load video and audio clips
video_clip = VideoFileClip("/home/jack/Desktop/abd0aebb-ce72-4a30-a233-14ee14f5b841random_video_58s.mp4")
audio_clip = AudioFileClip(music())

# Define fade durations
fade_in_duration = 1  # seconds
fade_out_duration = 1  # seconds

# Fade in audio
faded_audio = audio_clip.crossfadein(fade_in_duration)

# Calculate the point at which to start audio to synchronize with the video
audio_start_time = max(video_clip.duration - faded_audio.duration, 0)

# Trim audio to match video duration
trimmed_audio = faded_audio.subclip(audio_start_time)

# Concatenate video with trimmed audio
video_with_audio = video_clip.set_audio(trimmed_audio)

# Fade out audio at the end of the video
final_audio = video_with_audio.audio.crossfadeout(fade_out_duration)

# Set the audio of the video to the faded-out audio
final_video = video_with_audio.set_audio(final_audio)

# Write the final video with audio to a file
final_video.write_videofile("/home/jack/Desktop/StoryMaker/VIDEOS/uuidoutput.mp4", codec="libx264")


%%writefile /home/jack/hidden/MUSIC.py
import glob
import random
def music():
    MUSIC = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/Music/*.mp3"))
    return MUSIC



!pwd

from MUSIC import music
print (music())

!rm *.png
!rm *.jpg
!rm start/*.png

from PIL import Image

def zoom_effect(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    # Start with the original image
    current_image = original_image.copy()

    for i in range(num_images):
        # Enlarge the current image by 1%
        new_size = (int(w * 1.01), int(h * 1.01))
        enlarged_image = current_image.resize(new_size, Image.BICUBIC)
        sm_size = ((int((w/w) * 1.01)+1+w), (int(h/h * 1.01))+1+h)
        print(sm_size)
        shrunken_image = current_image.resize(sm_size, Image.BICUBIC)        
        # Calculate the top-left coordinates to crop back to the original size
        x_offset = (new_size[0] - w) // 2
        y_offset = (new_size[1] - h) // 2

        # Crop back to the original size
        cropped_image = enlarged_image.crop((x_offset, y_offset, x_offset + w, y_offset + h))
        shrunken_image.save("start/"+str(i)+".png")
        # Save the current image with zoom effect
        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.jpg"
        cropped_image = cropped_image.convert("RGB")
        cropped_image.save(output_path)

        # Set the current image to the cropped image for the next iteration
        current_image = cropped_image.copy()

# Example usage
image_path = "start/00001.jpg"
num_images = 150  # Change this to the desired number of images in the sequence

zoom_effect(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity_and_zoom(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        opacity = int(255 * inc)

        # Resize the original image to the desired size
        resized_original = original_image.resize((int(w * inc), int(h * inc)), Image.BICUBIC)

        # Create a copy of the original image for each overlay
        overlay_image = original_image.copy()

        # Calculate the top-left coordinates to center the overlay image on the original image
        x_offset = (w - overlay_image.width) // 2
        y_offset = (h - overlay_image.height) // 2

        # Create a new transparent image to hold the overlay
        overlay = Image.new("RGBA", original_image.size, (0, 0, 0, 0))

        # Paste the overlay image onto the transparent image at the calculated coordinates
        overlay.paste(overlay_image, (x_offset, y_offset))

        # Set the opacity for the overlay
        overlay.putalpha(opacity)

        # Create the final image by alpha compositing the original image and the overlay
        final_image = Image.alpha_composite(resized_original, overlay)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "start/00001.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity_and_zoom(image_path, num_images)


!ls *.jpg

from PIL import Image

def overlay_images_with_opacity_and_zoom(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        opacity = int(255 * inc)

        # Resize the original image to the desired size
        resized_original = original_image.resize((int(w * inc), int(h * inc)), Image.BICUBIC)

        # Create a copy of the original image for each overlay
        overlay_image = original_image.copy()

        # Calculate the top-left coordinates to center the overlay image on the original image
        x_offset = (w - overlay_image.width) // 2
        y_offset = (h - overlay_image.height) // 2

        # Create a new transparent image to hold the overlay
        overlay = Image.new("RGBA", original_image.size, (0, 0, 0, 0))

        # Paste the overlay image onto the transparent image at the calculated coordinates
        overlay.paste(overlay_image, (x_offset, y_offset))

        # Set the opacity for the overlay
        overlay.putalpha(opacity)

        # Create the final image by alpha compositing the original image and the overlay
        final_image = Image.alpha_composite(resized_original, overlay)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "d1dfcf75-8cab-4d35-8f01-5c771ee74d83.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity_and_zoom(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity(image_path, num_images, save_interval=10):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        # Resize the original image to the desired size
        resized_original = original_image.resize(size, Image.BICUBIC)

        # Create a new transparent image with the same size as the original image
        overlay_image = Image.new("RGBA", (w, h), (0, 0, 0, 0))
        overlay_image.paste(resized_original, ((w - size[0]) // 2, (h - size[1]) // 2))

        # Set opacity for the overlay image
        overlay_image.putalpha(opacity)

        # Create the final image by overlaying the transparent image onto the original image
        final_image = Image.alpha_composite(original_image, overlay_image)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

        # Save the image at specified intervals to free up memory
        if (i + 1) % save_interval == 0:
            final_image.close()

    # Save the last image to ensure it is not left open
    final_image.close()

# Example usage
image_path = "start/00001.jpg"
num_images = 100  # Change this to the desired number of images in the sequence
save_interval = 10  # Change this to adjust how often images are saved

overlay_images_with_opacity(image_path, num_images, save_interval)


!rm *.jpg

from PIL import Image

def overlay_images_with_opacity_and_zoom(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        opacity = int(255 * inc)

        # Create a copy of the original image for each overlay
        overlay_image = original_image.copy()

        # Calculate the size of the overlay image based on the increase factor (inc)
        overlay_w, overlay_h = int(w * inc), int(h * inc)

        # Calculate the top-left coordinates to center the overlay image on the original image
        x_offset = (w - overlay_w) // 2
        y_offset = (h - overlay_h) // 2

        # Create a new transparent image to hold the overlay
        overlay = Image.new("RGBA", original_image.size, (0, 0, 0, 0))

        # Paste the overlay image onto the transparent image at the calculated coordinates
        overlay.paste(overlay_image, (x_offset, y_offset))

        # Set the opacity for the overlay
        overlay.putalpha(opacity)

        # Create the final image by alpha compositing the original image and the overlay
        final_image = Image.alpha_composite(original_image, overlay)

        # Create a zoomed version of the background image
        zoomed_image = original_image.resize((int(w / inc), int(h / inc)), Image.BICUBIC)
        x_zoom_offset = (w - zoomed_image.width) // 2
        y_zoom_offset = (h - zoomed_image.height) // 2

        # Paste the zoomed image onto the final image at the calculated coordinates
        final_image.paste(zoomed_image, (x_zoom_offset, y_zoom_offset))

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "d1dfcf75-8cab-4d35-8f01-5c771ee74d83.jpg"
num_images = 100  # Change this to the desired number of images in the sequence

overlay_images_with_opacity_and_zoom(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        # Resize the original image to the desired size
        resized_original = original_image.resize(size, Image.BICUBIC)

        # Create a new transparent image with the same size as the original image
        overlay_image = Image.new("RGBA", (w, h), (0, 0, 0, 0))
        overlay_image.paste(resized_original, ((w - size[0]) // 2, (h - size[1]) // 2))

        # Set opacity for the overlay image
        overlay_image.putalpha(opacity)

        # Create the final image by overlaying the transparent image onto the original image
        final_image = Image.alpha_composite(original_image, overlay_image)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "00001.jpg"
num_images = 100  # Change this to the desired number of images in the sequence

overlay_images_with_opacity(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        # Create a copy of the original image for each overlay
        overlay_image = original_image.copy()

        # Resize and set opacity for the overlay image
        overlay_image = overlay_image.resize(size, Image.BICUBIC)
        overlay_image.putalpha(opacity)

        # Create the final image by overlaying the resized image onto the original image
        final_image = Image.alpha_composite(original_image, overlay_image)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "00001.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        # Resize the original image to the desired size
        resized_original = original_image.resize(size, Image.BICUBIC)

        # Create a new image with an alpha channel for overlaying
        overlay_image = Image.new("RGBA", size, (0, 0, 0, 0))
        overlay_image.paste(resized_original, (0, 0))

        # Set opacity for the overlay image
        overlay_image.putalpha(opacity)

        # Create the final image by overlaying the resized image onto the original image
        final_image = Image.alpha_composite(original_image, overlay_image)
        DIR = "zooms/"
        file_number = str(i + 1).zfill(5)
        output_path = f"{DIR}{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00007.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity(image_path, num_images)


!mkdir zooms

!pwd

from PIL import Image

def overlay_images_with_opacity(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        # Create a copy of the original image for each overlay
        overlay_image = original_image.copy()

        # Resize and set opacity for the overlay image
        overlay_image = overlay_image.resize(size, Image.BICUBIC)
        overlay_image.putalpha(opacity)

        # Create the final image by overlaying the resized image onto the original image
        final_image = Image.alpha_composite(original_image, overlay_image)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "00001.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        overlay_image = original_image.resize(size, Image.BICUBIC)
        overlay_image.putalpha(opacity)

        final_image = Image.alpha_composite(original_image, overlay_image)

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "00001.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity(image_path, num_images)


from PIL import Image

def overlay_images_with_opacity(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        size = (int(w * inc), int(h * inc))
        opacity = int(255 * inc)

        overlay_image = original_image.resize(size, Image.BICUBIC)
        overlay_image.putalpha(opacity)

        final_image = Image.new("RGBA", original_image.size, (0, 0, 0, 0))
        x_offset = (w - size[0]) // 2
        y_offset = (h - size[1]) // 2
        final_image.paste(overlay_image, (x_offset, y_offset))

        file_number = str(i + 1).zfill(5)
        output_path = f"{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "00001.jpg"
num_images = 10  # Change this to the desired number of images in the sequence

overlay_images_with_opacity(image_path, num_images)


!ls leonardo.ai_files/00001.jpg

leonardo.ai_files/00001.jpg

from PIL import Image

def overlay_centered(larger_image_path, output_path, inc):
    # Open the larger and smaller images
    larger_image = Image.open(larger_image_path).convert("RGBA")
    w, h = larger_image.size
    smaller_image = larger_image.resize((int(w - inc), int(h - inc)), Image.BICUBIC)

    # Get dimensions of both images
    width_larger, height_larger = larger_image.size
    width_smaller, height_smaller = smaller_image.size

    # Calculate the center coordinates
    center_x = (width_larger - width_smaller) // 2
    center_y = (height_larger - height_smaller) // 2

    # Overlay the smaller image on the larger image
    larger_image.paste(smaller_image, (center_x, center_y), smaller_image)

    # Save the final image
    larger_image.save(output_path)

larger_image_path = "leonardo.ai_files/00001.jpg"
for Inc in range(0, 100):
    inc = Inc + 0.5
    output_path = str(inc) + "test.png"
    print(inc)
    overlay_centered(larger_image_path, output_path, inc)


from PIL import Image
import os
import math

def overlay_centered(larger_image_path, inc, image_num):
    # Open the larger and smaller images
    larger_image = Image.open(larger_image_path).convert("RGBA")
    w, h = larger_image.size
    smaller_image = larger_image.resize((int(w + inc), int(h + inc)), Image.BICUBIC)

    # Get dimensions of both images
    width_larger, height_larger = larger_image.size
    width_smaller, height_smaller = smaller_image.size

    # Calculate the center coordinates
    center_x = (width_larger - width_smaller) // 2
    center_y = (height_larger - height_smaller) // 2

    # Overlay the smaller image on the larger image
    larger_image.paste(smaller_image, (center_x, center_y), smaller_image)

    # Save the final image
    output_directory = "start/"
    ext = ".png"
    save_path = os.path.join(output_directory, f"{image_num:05d}{ext}")
    larger_image.save(save_path)


larger_image_path = "/home/jack/Desktop/learn_flask/static/abstract_beauty/00001.jpg"
image_num = 1
for inc in range(1, 1000):
    print(inc)
    overlay_centered(larger_image_path, inc, image_num)
    image_num += 1


!cp /home/jack/Desktop/learn_flask/zoom.ipynb .

import os
import fnmatch

def search_in_ipynb_files(root_directory, term):
    for dirpath, dirnames, filenames in os.walk(root_directory):
        for filename in fnmatch.filter(filenames, "*.ipynb"):
            file_path = os.path.join(dirpath, filename)
            with open(file_path, "r", encoding="utf-8") as file:
                lines = file.readlines()
                for line_number, line in enumerate(lines, 1):
                    if term in line:
                        print(f"Found '{term}' in file: {file_path}, line: {line_number}")
                        print("XXXXXXX",line.strip())  # Print the whole line

# Example usage
root_directory = "/home/jack/Desktop"
term_to_search = "composit"
search_in_ipynb_files(root_directory, term_to_search)


import os
def overlay_centered(larger_image_path,inc):
    # Open the larger and smaller images
    larger_image = Image.open(larger_image_path).convert("RGBA")
    w,h = larger_image.size
    smaller_image = larger_image.resize((int(w*inc),int(h*inc)),Image.BICUBIC)

    # Get dimensions of both images
    width_larger, height_larger = larger_image.size
    width_smaller, height_smaller = smaller_image.size

    # Calculate the center coordinates
    center_x = (width_larger - width_smaller) // 2
    center_y = (height_larger - height_smaller) // 2

    # Overlay the smaller image on the larger image
    larger_image.paste(smaller_image, (center_x, center_y), smaller_image)
    ext = ".png"
    output_directory = "start/"
    
    zfill = int(inc)
    save_path = os.path.join(output_directory, f"{zfill(5)}{ext}")
    larger_image.save(save_path)   
    
    
    
    
    
    # Save the final image
    #larger_image.save("start/"+str(inc)+"test.png")
    
    
larger_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg" 
larger_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg" 
for inc in range(1,100):
    inc = inc*.1
    print(inc)
    
    overlay_centered(larger_image_path,inc)  


!ls start



from PIL import Image

def overlay_centered(larger_image_path, smaller_image_path, output_path):
    # Open the larger and smaller images
    larger_image = Image.open(larger_image_path).convert("RGBA")
    smaller_image = Image.open(smaller_image_path).convert("RGBA")

    # Get dimensions of both images
    width_larger, height_larger = larger_image.size
    width_smaller, height_smaller = smaller_image.resize((400,400),Image.BICUBIC)

    # Calculate the center coordinates
    center_x = (width_larger - width_smaller) // 2
    center_y = (height_larger - height_smaller) // 2

    # Overlay the smaller image on the larger image
    larger_image.paste(smaller_image, (center_x, center_y), smaller_image)

    # Save the final image
    larger_image.save(output_path)

# Example usage
larger_image_path = "/home/jack/Desktop/learn_flask/static/abstract_beauty/00001.jpg"
smaller_image_path = "/home/jack/Desktop/learn_flask/static/abstract_beauty/00004.jpg"
output_path = "output_image.png"

overlay_centered(larger_image_path, smaller_image_path, output_path)


!display output_image.png

from PIL import Image
import os
import ffmpeg

def overlay_images(base_image_path, overlay_image_path, output_directory, num_steps):
    base_image = Image.open(base_image_path).convert("RGBA")
    overlay_image = Image.open(overlay_image_path).convert("RGBA")

    width, height = base_image.size
    step_size = 1.0 / num_steps

    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = int(i * (255 * step_size))
        size = (int(width * (1.0 + i * step_size)), int(height * (1.0 + i * step_size)))

        # Resize the overlay image
        resized_overlay = overlay_image.resize(size, Image.BICUBIC)

        # Create a new image with the same size as the base image
        new_image = Image.new("RGBA", (width, height), (0, 0, 0, 0))

        # Overlay the resized overlay image on the new image with the calculated opacity
        new_image = Image.alpha_composite(new_image, resized_overlay)

        # Overlay the new image on the base image with the calculated opacity
        final_image = Image.alpha_composite(base_image, new_image)

        # Save the image with the current step number as the filename
        output_path = os.path.join(output_directory, f"{i:03d}.png")
        final_image.save(output_path)

    return output_directory

def create_overlay_video(image_directory, output_video_path, fps):
    input_stream = ffmpeg.input(os.path.join(image_directory, "%03d.png"), framerate=fps)
    ffmpeg.output(input_stream, output_video_path).run()

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_directory = "output_images"
output_video_path = "output_video.mp4"
num_steps = 200
fps = 30

# Create the overlay images
overlay_images(base_image_path, overlay_image_path, output_directory, num_steps)

# Create the video from the overlay images
create_overlay_video(output_directory, output_video_path, fps)


!pwd

from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = VideoFileClip(base_image_path, target_resolution=(720, 1280))
    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    width, height = base_clip.w, base_clip.h
    step_size = 1.0 / num_steps

    clips = []
    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        size = (width * (1.0 + i * step_size), height * (1.0 + i * step_size))

        # Resize the overlay image
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([base_clip, resized_overlay_clip], use_bgclip=True)

        clips.append(composite_clip)

    # Concatenate all clips to create the final video
    final_clip = concatenate_videoclips(clips)

    # Write the video to a file
    final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)

# Paths and parameters
base_image_path = "/home/jack/Desktop/learn_flask/static/screaming/vid-from-images.mp4"
overlay_image_path = "/home/jack/Desktop/learn_flask/static/screaming/00069.jpg"
output_video_path = "output_video.mp4"
num_steps = 50
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)


from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips
import numpy as np

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = VideoFileClip(base_image_path, target_resolution=(720, 1280))
    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    width, height = base_clip.w, base_clip.h
    step_size = 1.0 / num_steps

    clips = []
    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        size = (width * (1.0 + i * step_size), height * (1.0 + i * step_size))

        # Resize the overlay image
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Print the duration of the resized overlay clip for debugging
        print("Clip", i, "Duration:", resized_overlay_clip.duration)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([base_clip, resized_overlay_clip], use_bgclip=True)

        clips.append(composite_clip)

    try:
        # Concatenate all clips to create the final video
        final_clip = concatenate_videoclips(clips)
    except Exception as e:
        print("Error while concatenating clips:", e)
        return

    try:
        # Write the video to a file
        final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)
    except Exception as e:
        print("Error while writing the output video:", e)
        return

    print("Overlay video created successfully!")

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_video_path = "xooms/output_video.mp4"
num_steps = 50
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)


from moviepy.editor import ImageClip, CompositeVideoClip, concatenate_videoclips

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = ImageClip(base_image_path)
    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    width, height = base_clip.w, base_clip.h
    step_size = 1.0 / num_steps

    clips = []
    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        size = (int(width * (1.0 + i * step_size)), int(height * (1.0 + i * step_size)))

        # Resize the overlay image
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([base_clip, resized_overlay_clip], use_bgclip=True)

        clips.append(composite_clip)

    try:
        # Concatenate all clips to create the final video
        final_clip = concatenate_videoclips(clips)
    except Exception as e:
        print("Error while concatenating clips:", e)
        return

    try:
        # Write the video to a file
        final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)
    except Exception as e:
        print("Error while writing the output video:", e)
        return

    print("Overlay video created successfully!")

# Rest of the script remains the same

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_video_path = "zooms/output_video.mp4"
num_steps = 50
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)




from moviepy.editor import ImageClip, CompositeVideoClip, concatenate_videoclips

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = ImageClip(base_image_path)
    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    width, height = base_clip.w, base_clip.h
    step_size = 1.0 / num_steps

    clips = []
    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        size = (int(width * (1.0 + i * step_size)), int(height * (1.0 + i * step_size)))

        # Resize the overlay image
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([base_clip.copy(), resized_overlay_clip], use_bgclip=True)
        composite_clip = composite_clip.set_duration(base_clip.duration)

        clips.append(composite_clip)

    try:
        # Concatenate all clips to create the final video
        final_clip = concatenate_videoclips(clips)
    except Exception as e:
        print("Error while concatenating clips:", e)
        return

    try:
        # Write the video to a file
        final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)
    except Exception as e:
        print("Error while writing the output video:", e)
        return

    print("Overlay video created successfully!")

# Rest of the script remains the same

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_video_path = "zooms/output_video.mp4"
num_steps = 50
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)


!mkdir gifs

from PIL import Image, ImageSequence

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    SIZE = bg.size
    bg = bg.resize((SIZE), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((SIZE), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(200):
        size = (int(fg_copy.width * (i+1)/200), int(fg_copy.height * (i+1)/200))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/200))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/200))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
        return result_images
    # Save the resulting images as a GIF animation
    result_images[0].save('gifs/zoom_effect2.gif', save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)
#zoom_effect(bg_file, fg_file)
overlay_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
base_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
zoom_effect(base_image_path, overlay_image_path)


from PIL import Image, ImageSequence
from moviepy.editor import ImageSequenceClip

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    SIZE = bg.size
    bg = bg.resize((SIZE), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((SIZE), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(200):
        size = (int(fg_copy.width * (i+1)/200), int(fg_copy.height * (i+1)/200))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/200))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/200))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
        return result_images

def create_mp4_from_images(images_list, output_file, fps):
    clip = ImageSequenceClip(images_list, fps=fps)
    clip.write_videofile(output_file, codec="libx264", fps=fps)

bg_file_path = "/home/jack/Desktop/learn_flask/static/screaming/00069.jpg"
fg_file_path = "/home/jack/Desktop/learn_flask/static/screaming/00069.jpg"


output_mp4_file = "output_video.mp4"
frames_per_second = 30

images_list = zoom_effect(bg_file_path, fg_file_path)
create_mp4_from_images(images_list, output_mp4_file, frames_per_second)


from PIL import Image, ImageSequence
from moviepy.editor import ImageSequenceClip

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    SIZE = bg.size
    bg = bg.resize((SIZE), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((SIZE), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(200):
        size = (int(fg_copy.width * (i+1)/200), int(fg_copy.height * (i+1)/200))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/200))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/200))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
        return result_images

def create_mp4_from_images(images_list, output_file, fps):
    clip = ImageSequenceClip(images_list, fps=fps)
    clip.write_videofile(output_file, codec="libx264", fps=fps)

bg_file_path = "/home/jack/Desktop/learn_flask/static/screaming/00069.jpg"
fg_file_path = "/home/jack/Desktop/learn_flask/static/screaming/00069.jpg"
output_mp4_file = "output_video.mp4"
frames_per_second = 30

images_list = zoom_effect(bg_file_path, fg_file_path)
create_mp4_from_images(images_list, output_mp4_file, frames_per_second)


from PIL import Image, ImageSequence
from moviepy.editor import ImageSequenceClip
import numpy as np

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    SIZE = bg.size
    bg = bg.resize((SIZE), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((SIZE), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(200):
        size = (int(fg_copy.width * (i+1)/200), int(fg_copy.height * (i+1)/200))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/200))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/200))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
    
    return result_images  # Move the return statement outside the for loop

def create_mp4_from_images(images_list, output_file, fps):
    # Convert PIL Image objects to NumPy arrays
    image_arrays = [np.array(image) for image in images_list]
    
    # Create the video clip from the NumPy arrays
    clip = ImageSequenceClip(image_arrays, fps=fps)
    
    # Write the video to the output file
    clip.write_videofile(output_file, codec="libx264", fps=fps)

bg_file_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
fg_file_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_mp4_file = "output_video2.mp4"
frames_per_second = 30

images_list = zoom_effect(bg_file_path, fg_file_path)
create_mp4_from_images(images_list, output_mp4_file, frames_per_second)


!vlc output_video2

from PIL import Image, ImageSequence
from moviepy.editor import ImageSequenceClip
import numpy as np

def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    SIZE = bg.size
    bg = bg.resize((SIZE), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((SIZE), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(200):
        size = (int(fg_copy.width * (i+1)/200), int(fg_copy.height * (i+1)/200))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/200))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/200))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
        return result_images

def create_mp4_from_images(images_list, output_file, fps):
    # Convert PIL Image objects to NumPy arrays
    image_arrays = [np.array(image) for image in images_list]
    
    # Create the video clip from the NumPy arrays
    clip = ImageSequenceClip(image_arrays, fps=fps)
    
    # Write the video to the output file
    clip.write_videofile(output_file, codec="libx264", fps=fps)

bg_file_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
fg_file_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_mp4_file = "output_video.mp4"
frames_per_second = 30

images_list = zoom_effect(bg_file_path, fg_file_path)
create_mp4_from_images(images_list, output_mp4_file, frames_per_second)


from moviepy.editor import ImageClip, CompositeVideoClip, concatenate_videoclips

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = ImageClip(base_image_path)
    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    base_duration = num_steps / fps
    base_clip = base_clip.set_duration(base_duration)

    width, height = base_clip.w, base_clip.h
    step_size = 1.0 / num_steps

    clips = []
    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        size = (int(width * (1.0 - i * step_size)), int(height * (1.0 - i * step_size)))  # Inverse size calculation

        # Resize both clips to have the same size
        resized_base_clip = base_clip.resize(size)
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([resized_base_clip, resized_overlay_clip], use_bgclip=True)

        # Set the duration of the composite clip to match the base clip
        composite_clip = composite_clip.set_duration(base_duration)

        clips.append(composite_clip)

    try:
        # Concatenate all clips to create the final video
        final_clip = concatenate_videoclips(clips, method="compose")
        print("Concatenation successful!")
    except Exception as e:
        print("Error while concatenating clips:", e)
        return

    try:
        # Write the video to a file in MP4 format
        final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)
        print("Video writing successful!")
    except Exception as e:
        print("Error while writing the output video:", e)
        return

    print("Overlay video created successfully!")

# Rest of the script remains the same

# Paths and parameters
base_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_video_path = "output_video.mp4"  # Change the extension to ".mp4"
num_steps = 50
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)


!vlc output_video.mp4



from moviepy.editor import ImageClip, CompositeVideoClip, concatenate_videoclips

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = ImageClip(base_image_path)

    # Print the attributes of the base clip
    print("Base Clip Attributes:", dir(base_clip))

    # Set the duration of the base clip to the number of steps
    base_duration = (num_steps / fps)*.05
    base_clip = base_clip.set_duration(base_duration)

    # Print the updated duration of the base clip
    print("Base Clip Duration (Updated):", base_clip.duration)

    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    width, height = base_clip.w, base_clip.h
    step_size = 1 / num_steps
    #step_size = .25 / num_steps

    clips = []
    for i in range(1,num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        print(i)
        if i ==0:
            size = (1, 1)
        else:
            Size = (int(width * (1.0 + i * step_size))/num_steps, int(height * (1.0 + i * step_size))/num_steps)
            print(Size)
        size = (i*10),(i*10)
        # Resize the overlay image
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([base_clip.copy(), resized_overlay_clip], use_bgclip=True)

        # Set the duration of the composite clip to match the base clip
        composite_clip = composite_clip.set_duration(base_duration)

        clips.append(composite_clip)

    try:
        # Concatenate all clips to create the final video
        final_clip = concatenate_videoclips(clips, method="compose")
        print("Concatenation successful!")
    except Exception as e:
        print("Error while concatenating clips:", e)
        return

    try:
        # Write the video to a file in MP4 format
        final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)
        print("Video writing successful!")
    except Exception as e:
        print("Error while writing the output video:", e)
        return

    print("Overlay video created successfully!")

# Rest of the script remains the same

# Paths and parameters
base_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
from PIL import Image
import os
import ffmpeg

def overlay_images(base_image_path, overlay_image_path, output_directory, num_steps):
    base_image = Image.open(base_image_path).convert("RGBA")
    overlay_image = Image.open(overlay_image_path).convert("RGBA")

    width, height = base_image.size
    step_size = 1.0 / num_steps

    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = int(i * (255 * step_size))
        size = (int(width * (1.0 + i * step_size)), int(height * (1.0 + i * step_size)))

        # Resize the overlay image
        resized_overlay = overlay_image.resize(size, Image.BICUBIC)

        # Create a new image with the same size as the base image
        new_image = Image.new("RGBA", (width, height), (0, 0, 0, 0))

        # Overlay the resized overlay image on the new image with the calculated opacity
        new_image = Image.alpha_composite(new_image, resized_overlay)

        # Overlay the new image on the base image with the calculated opacity
        final_image = Image.alpha_composite(base_image, new_image)

        # Save the image with the current step number as the filename
        output_path = os.path.join(output_directory, f"{i:03d}.png")
        final_image.save(output_path)

    return output_directory

def create_overlay_video(image_directory, output_video_path, fps):
    input_stream = ffmpeg.input(os.path.join(image_directory, "%03d.png"), framerate=fps)
    ffmpeg.output(input_stream, output_video_path).run()

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path =  "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
from PIL import Image
import os
import ffmpeg

def overlay_images(base_image_path, overlay_image_path, output_directory, num_steps):
    base_image = Image.open(base_image_path).convert("RGBA")
    overlay_image = Image.open(overlay_image_path).convert("RGBA")

    width, height = base_image.size
    step_size = 1.0 / num_steps

    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = int(i * (255 * step_size))
        size = (int(width * (1.0 + i * step_size)), int(height * (1.0 + i * step_size)))

        # Resize the overlay image
        resized_overlay = overlay_image.resize(size, Image.BICUBIC)

        # Create a new image with the same size as the base image
        new_image = Image.new("RGBA", (width, height), (0, 0, 0, 0))

        # Overlay the resized overlay image on the new image with the calculated opacity
        new_image = Image.alpha_composite(new_image, resized_overlay)

        # Overlay the new image on the base image with the calculated opacity
        final_image = Image.alpha_composite(base_image, new_image)

        # Save the image with the current step number as the filename
        output_path = os.path.join(output_directory, f"{i:03d}.png")
        final_image.save(output_path)

    return output_directory

def create_overlay_video(image_directory, output_video_path, fps):
    input_stream = ffmpeg.input(os.path.join(image_directory, "%03d.png"), framerate=fps)
    ffmpeg.output(input_stream, output_video_path).run()

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_directory = "output_images"
output_video_path = "output_video.mp4"
num_steps = 200
fps = 30

# Create the overlay images
overlay_images(base_image_path, overlay_image_path, output_directory, num_steps)

# Create the video from the overlay images
create_overlay_video(output_directory, output_video_path, fps)

output_directory = "output_images"
output_video_path = "output_video.mp4"
num_steps = 200
fps = 30

# Create the overlay images
overlay_images(base_image_path, overlay_image_path, output_directory, num_steps)

# Create the video from the overlay images
create_overlay_video(output_directory, output_video_path, fps)
overlay_image_path = "/home/jack/Desktop/learn_flask/static/screaming/00069.jpg"
output_video_path = "output_video.mp4"  # Change the extension to ".mp4"
num_steps = 100
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)


!vlc output_video.mp4



from moviepy.editor import ImageClip, CompositeVideoClip, concatenate_videoclips

def overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps):
    base_clip = ImageClip(base_image_path)

    # Print the attributes of the base clip
    print("Base Clip Attributes:", dir(base_clip))

    # Set the duration of the base clip to the number of steps
    base_duration = (num_steps / fps)*.05
    base_clip = base_clip.set_duration(base_duration)

    # Print the updated duration of the base clip
    print("Base Clip Duration (Updated):", base_clip.duration)

    overlay_clip = ImageClip(overlay_image_path, transparent=True)

    width, height = base_clip.w, base_clip.h
    step_size = 1.0 / num_steps

    clips = []
    for i in range(num_steps + 1):
        # Calculate current opacity and size
        opacity = i * step_size
        size = (int(width * (1.0 + i * step_size)), int(height * (1.0 + i * step_size)))

        # Resize the overlay image
        resized_overlay_clip = overlay_clip.resize(size)

        # Set the opacity of the overlay clip
        resized_overlay_clip = resized_overlay_clip.set_opacity(opacity)

        # Composite the overlay clip on the base clip
        composite_clip = CompositeVideoClip([base_clip.copy(), resized_overlay_clip], use_bgclip=True)

        # Set the duration of the composite clip to match the base clip
        composite_clip = composite_clip.set_duration(base_duration)

        clips.append(composite_clip)

    try:
        # Concatenate all clips to create the final video
        final_clip = concatenate_videoclips(clips, method="compose")
        print("Concatenation successful!")
    except Exception as e:
        print("Error while concatenating clips:", e)
        return

    try:
        # Write the video to a file in MP4 format
        final_clip.write_videofile(output_video_path, fps=fps, codec='libx264', audio=False)
        print("Video writing successful!")
    except Exception as e:
        print("Error while writing the output video:", e)
        return

    print("Overlay video created successfully!")

# Rest of the script remains the same

# Paths and parameters
base_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
overlay_image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
output_video_path = "zooms/output_video.mp4"  # Change the extension to ".mp4"
num_steps = 100
fps = 30

# Create the overlay video
overlay_images(base_image_path, overlay_image_path, num_steps, output_video_path, fps)


from PIL import Image

def overlay_images_with_opacity_and_zoom(image_path, num_images):
    original_image = Image.open(image_path).convert("RGBA")
    w, h = original_image.size

    for i in range(num_images):
        inc = (i + 1) / num_images
        opacity = int(255 * inc)

        # Create a new transparent image with increased size for each iteration
        new_size = (int(w + w * inc), int(h + h * inc))
        final_image = Image.new("RGBA", new_size, (0, 0, 0, 0))

        # Calculate the top-left coordinates to center the original image on the final image
        x_offset = (new_size[0] - w) // 2
        y_offset = (new_size[1] - h) // 2

        # Paste the original image onto the final image at the calculated coordinates
        final_image.paste(original_image, (x_offset, y_offset))

        # Resize the original image to the desired size
        resized_original = original_image.resize((int(w * inc), int(h * inc)), Image.BICUBIC)

        # Calculate the top-left coordinates to center the overlay image on the final image
        x_overlay_offset = (new_size[0] - resized_original.width) // 2
        y_overlay_offset = (new_size[1] - resized_original.height) // 2

        # Create a new transparent image to hold the overlay
        overlay_image = Image.new("RGBA", new_size, (0, 0, 0, 0))

        # Paste the resized overlay image onto the transparent image at the calculated coordinates
        overlay_image.paste(resized_original, (x_overlay_offset, y_overlay_offset))

        # Set the opacity for the overlay image
        overlay_image.putalpha(opacity)

        # Composite the overlay image with the final image
        final_image = Image.alpha_composite(final_image, overlay_image)
        DIR = "zooms/"
        file_number = str(i + 1).zfill(5)
        output_path = f"{DIR}{file_number}.png"
        final_image.save(output_path)

# Example usage
image_path = "/home/jack/Desktop/StoryMaker/static/images/Elektra-Weapons/00059.jpg"
num_images = 4  # Change this to the desired number of images in the sequence

overlay_images_with_opacity_and_zoom(image_path, num_images)


# %load /home/jack/hidden/MUSIC.py
import glob
import random
def music():
    MUSIC = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/Music/*.mp3"))
    return MUSIC



from PIL import Image, ImageSequence
import random
import glob
import uuid
def zoom_effect(bg_file, fg_file):
    bg = Image.open(bg_file).convert('RGBA')
    bg = bg.resize((512,1024), Image.BICUBIC)
    fg = Image.open(fg_file).convert('RGBA')
    fg = fg.resize((bg.size), Image.BICUBIC)
    fg_copy = fg.copy()
    fg_copy = fg_copy.resize((int(fg_copy.width), int(fg_copy.height)))
    result_images = []
    for i in range(100):
        size = (int(fg_copy.width * (i+1)/100), int(fg_copy.height * (i+1)/100))
        fg_copy_resized = fg_copy.resize(size)
        fg_copy_resized.putalpha(int((i+1)*255/100))
        fg_copy_resized = fg_copy_resized.convert('RGBA')
        fg_copy_resized.putalpha(int((i+1)*255/100))
        result = bg.copy()
        x = int((bg.width - fg_copy_resized.width)/2)
        y = int((bg.height - fg_copy_resized.height)/2)
        result.alpha_composite(fg_copy_resized, (x, y))
        #result.save("gifs/_"+str(i)+".png")
        result_images.append(result)
    # Save the resulting images as a GIF animation
    unique_filename = str(uuid.uuid4())+".gif"
    result_images[0].save('static/images/story/'+unique_filename, save_all=True, append_images=result_images[1:], optimize=False, duration=100, loop=0)
#zoom_effect(bg_file, fg_file)
for i in range(0,50):
    bg_file = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/images/story_nov28/*.jpg"))
    fg_file = random.choice(glob.glob("/home/jack/Desktop/HDD500/collections/images/story_nov28/*.jpg"))

    zoom_effect(bg_file, fg_file)

!mkdir -p static/images/story






==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Scrolling_Text.ipynb
Code Content:
ffmpeg -i static/audio_mp3/TheArcanianInfluenceandtheUnraveling.mp3 -f lavfi -i color='#470000'@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='FORMATTED.txt':y=(h-120)-12*t:x=550:fontcolor=orange:fontfile=/home/jack/Arimo-Regular.ttf:fontsize=26" -t 260 -y output04b.mp4
vlc output04b.mp4

cp /home/jack/Desktop/StoryMaker/static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.png BlackFORMATTED.png

imag="BlackFORMATTED.png"
logo="/home/jack/Desktop/StoryMaker/static/images/exotic_beautiful_Lat_9.jpg"

ffmpeg -f lavfi -i color=s=1280x720 -loop 1 -t 0.08 -i BlackFORMATTED.png -filter_complex "[1:v]scale=1280:-2,setpts=if(eq(N,0),0,1+1/0.02/TB),fps=25[fg]; [0:v][fg]overlay=y=-'t*h*0.02':eof_action=endall[v]" -map "[v]" -y output2.mp4


ffmpeg -f lavfi -i color=s=1280x720 -loop 1 -t 0.08 -i BlackFORMATTED.png -filter_complex "[1:v]scale=1280:-2,setpts='if(eq(N\,0)\,0\,1+1/0.02/TB)',fps=25[fg];[0:v][fg]overlay=y=(H-h)/2-'t*h*0.02':eof_action=endall[v]" -map "[v]" -y output2.mp4
vlc output2.mp4

ffmpeg -f lavfi -i color=s=1280x720 -loop 1 -t 0.08 -i BlackFORMATTED.png -filter_complex "[1:v]scale=1280:-2,setpts='if(eq(N\,0)\,0\,1+1/0.02/TB)',fps=25[fg];[0:v][fg]overlay=y=-'t*h*0.02':eof_action=endall[v]" -map "[v]" -y output2.mp4


vlc output2.mp4

ls /home/jack/Desktop/StoryMaker/static/images/exotic_beautiful_Lat_9.jpg

#echo ${imag}
echo ${img}

ffmpeg -hide_banner -y -loop 1 -r 25 -i /home/jack/Desktop/StoryMaker/static/images/exotic_beautiful_Lat_9.jpg -loop 1 -r 25 -i ${imag} -filter_complex "[0:v]crop=384:512:0:0[img1];[1:v]crop=384:512:0:0[img2];[img1][img2]hstack=inputs=2" -t 25 -y 768x512.mp4


vlc 768x512.mp4

ffmpeg -hide_banner -y -loop 1 -r 25 -i /home/jack/Desktop/StoryMaker/static/images/exotic_beautiful_Lat_9.jpg -loop 1 -r 25 -i ${imag} -filter_complex "[0:v]crop=270:257:360:55[c0];[c0][1:v]overlay=y='257-t*257*0.04':eof_action=endall[fg];[0:v][fg]overlay=x=360:y=55:eof_action=endall" -t 25 -y output.mp4

cp /home/jack/Desktop/StoryMaker/static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt FORMATTED.txt

TEXT="/home/jack/Desktop/StoryMaker/static/formatted_text/The-Arcanian-Influence-and-the-Unraveling-the-BlackFORMATTED.txt"

ffmpeg -f lavfi -i color=green@0.0:s=1280x720:rate=60,format=rgba -ss 00:00:00 -t 00:01:30 -vf "drawtext=fontfile=/home/jack/fonts/Impact.ttf:fontsize=30:fontcolor=green:x=(w-text_w)/2+20:y=h-40*t:line_spacing=80:textfile='FORMATTED.txt'" -t 20 -y output01.mp4

vlc output02.mp4

ls /home/jack/fonts/

ffmpeg -f lavfi -i color=white@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='FORMATTED.txt':y=h-20*t:x=520" -t 630 -y output04.mp4
vlc output04.mp4

ffmpeg -i static/audio_mp3/TheArcanianInfluenceandtheUnraveling.mp3 -f lavfi -i color=white@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='FORMATTED.txt':y=(h-120)-12*t:x=520:fontfile=/home/jack/Arimo-Regular.ttf:fontsize=26" -t 260 -y output04a.mp4
vlc output04a.mp4


ffmpeg -i static/audio_mp3/TheArcanianInfluenceandtheUnraveling.mp3 -f lavfi -i color=white@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='FORMATTED.txt':y=(h-120)-12*t:x=520:fontfile=/home/jack/Arimo-Regular.ttf:fontsize=26" -t 260 -y output04a.mp4
vlc output04a.mp4


ffmpeg -i static/audio_mp3/TheArcanianInfluenceandtheUnraveling.mp3 -f lavfi -i color=white@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='FORMATTED.txt':y=h-10*t:x=720:fontfile=/home/jack/Arimo-Regular.ttf:fontsize=26" -t 630 -y output04.mp4
vlc output04.mp4


vlc output04.mp4

#Working
ffmpeg -f lavfi -i color=white@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='FORMATTED.txt':y=h-10*t:x=720:fontfile=/home/jack/Arimo-Regular.ttf:fontsize=26" -t 120 -y output04.mp4
vlc output04.mp4

#WORKS 
ffmpeg -f lavfi -i color=white@0.0:s=1280x720:rate=60,format=rgba -vf drawtext="textfile='FORMATTED.txt':y=h-20*t" -t 40 -y output03.mp4
vlc output03.mp4

ffmpeg -f lavfi -i color=green@0.0:s=1280x720:rate=60,format=rgba -vf drawtext="text='Hello World':y=h-20*t" -t 20 -y output02.mp4


ffmpeg -f lavfi -i color=green@0.0:s=1280x720:rate=60,format=rgba -vf "drawtext=textfile='textfile.txt':y=h-20*t" -t 20 -y output02.mp4


ls *.txt

vlc output02.mp4

ffmpeg -i input.mp4 -vf drawtext="text='THANKS for watching':y=h-20*t" output01.mp4

ffmpeg -f lavfi -i color=green@0.0:s=1280x720:rate=60,format=rgba -ss 00:00:00 -t 00:01:30 -vf "drawtext=fontfile=/home/jack/fonts/Impact.ttf:fontsize=30:fontcolor=green:x=(w-text_w)/2+20:y=h-40*t:line_spacing=80:textfile=${TEXT}" -c:v libx264 -preset ultrafast -t 35 output.mp4





==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled6.ipynb
Code Content:
hair_colors = ['black hair', 'brown hair', 'blonde hair', 'red hair', 'brunette hair', 'gray hair', 'white hair', 'auburn hair', 'silver hair', 'blue hair']


hair = ['redhead', 'blonde hair', 'brown hair', 'black hair', 'green hair', 'blue hair', 'brown and blonde streaked hair', 'trait8', 'trait9', 'trait10']

import random

hair_colors = ['auburn hair','black hair', 'brown hair', 'blonde hair', 'red hair', 'brunette hair', 'gray hair', 'white hair', 'silver hair', 'blue hair']
probabilities = [0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]

selected_trait = random.choices(hair_colors, probabilities)[0]
selected_trait

for i in range(0,30):
    selected_trait = random.choices(hair_colors, probabilities)[0]
    print(selected_trait)

import re
from datetime import datetime
from g4f import ChatCompletion
from flask import request, Response, stream_with_context
from requests import get
from server.config import special_instructions
import logging
import json

from flask import request, Response
from requests import get

def fetch_search_results(query):
    """  
    Fetch search results for a given query.  

    :param query: Search query string  
    :return: List of search results  
    """
    search = get('https://ddg-api.herokuapp.com/search',
                 params={
                     'query': query,
                     'limit': 3,
                 })

    snippets = ""
    for index, result in enumerate(search.json()):
        snippet = f'[{index + 1}] "{result["snippet"]}" URL:{result["link"]}.'
        snippets += snippet

    response = "Here are some updated web searches. Use this to improve user response:"
    response += snippets

    return [{'role': 'system', 'content': response}]      
#----------------- 
query= " where is Grand Rapids"
info = fetch_search_results(query) 
info

from flask import request, Response
from requests import get
import json
def save_messages(info):
        """
        Save the messages to a file.
        :param messages: List of messages
        :param conversation_id: Conversation ID
        """
        filename = "info.json"
        with open(filename, 'a') as file:
            json.dump(info, file)
        return print("Info Saved")  
def fetch_search_results(query):
    """  
    Fetch search results for a given query.  

    :param query: Search query string  
    :return: List of search results  
    """
    search = get('https://ddg-api.herokuapp.com/search',
                 params={
                     'query': query,
                     'limit': 3,
                 })

    snippets = ""
    for index, result in enumerate(search.json()):
        snippet = f'[{index + 1}] "{result["snippet"]}" URL:{result["link"]}.'
        snippets += snippet

    response = "Here are some updated web searches. Use this to improve user response:"
    response += snippets

    return [{'role': 'system', 'content': response}]      
#----------------- 
query= input("Query: ")
info = fetch_search_results(query) 
save_messages(info)
info

import json
def save_messages(info):
        """
        Save the messages to a file.
        :param messages: List of messages
        :param conversation_id: Conversation ID
        """
        filename = "info.json"
        with open(filename, 'a') as file:
            json.dump(info, file)
        return print("Info Saved")    

save_messages(info)

# %load info.json
[{"role": "system", "content": "Here are some updated web searches. Use this to improve user response:[1] \"Grand Rapids is a city and county seat of Kent County in the U.S. state of Michigan. [4] At the 2020 census, the city had a population of 198,893 [5] which ranks it as the second most-populated city in the state after Detroit.\" URL:https://en.wikipedia.org/wiki/Grand_Rapids,_Michigan.[2] \"It's the most dazzling time of the year in Grand Rapids, as the world's most visited art event once again takes over the streets, sidewalks, bridges, parks and buildings of the city's downtown. Visit ArtPrize \u00ae (Sept. 14-Oct. 1) to see 650+ works of art from international artists and vote for your favorites to win $200,000 in prize money.\" URL:https://www.michigan.org/city/grand-rapids.[3] \"Michigan (MI) Kent County Grand Rapids Things to Do in Grand Rapids Things to Do in Grand Rapids, MI - Grand Rapids Attractions Things to Do in Grand Rapids Popular things to do Self-guided Tours Airport & Hotel Transfers Theaters Food, Wine & Nightlife Ways to tour Grand Rapids Book these experiences for a close-up look at Grand Rapids. See all\" URL:https://www.tripadvisor.com/Attractions-g42256-Activities-Grand_Rapids_Kent_County_Michigan.html."}][{"role": "system", "content": "Here are some updated web searches. Use this to improve user response:[1] \"FlaskAppArchitect a Self Editing Flask App Creator. Totally Insane Art. 1.03K subscribers. No views 1 minute ago. FlaskAppArchitect a Self Editing Flask App Creator FlaskAppArchitect was...\" URL:https://www.youtube.com/watch?v=QsXZbWrHDPo.[2] \"FlaskAppArchitect Flask App Creator | YouTube Video Maker #flask #art #aiimages #flaskappJoin us on an exciting journey as we build a powerful Flask web appl...\" URL:https://www.youtube.com/watch?v=oENOrJx8z-g.[3] \"FlaskAppArchitect with JavaScript Short #flask #javascript #flaskapp Join us on an exciting journey as we build a powerful Flask web application step by step, with the guidance of ChatGPT AI....\" URL:https://www.youtube.com/watch?v=_dMFoFzYO-w."}]




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled5.ipynb
Code Content:
huggingface:EleutherAI/gpt-neox-20b

from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast

model = GPTNeoXForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b")
tokenizer = GPTNeoXTokenizerFast.from_pretrained("EleutherAI/gpt-neox-20b")

prompt = "GPTNeoX20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI."

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens)[0]




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled7.ipynb
Code Content:
import glob
import nbformat
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def read_ipynb_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        try:
            notebook = nbformat.read(file, as_version=4)
            text_content = ""
            for cell in notebook['cells']:
                if cell['cell_type'] == 'markdown':
                    text_content += cell['source'] + '\n'
            return text_content
        except Exception as e:
            logging.error(f"Error reading {file_path}: {e}")
            return None

def search_and_extract_text(directory):
    ipynb_files = glob.glob(f"{directory}/*.ipynb")
    extracted_text = []

    for file_path in ipynb_files:
        logging.info(f"Processing file: {file_path}")
        text_content = read_ipynb_file(file_path)
        
        if text_content is not None:
            extracted_text.append({
                'file_path': file_path,
                'text_content': text_content
            })

    return extracted_text

def write_to_all_notebooks_file(extracted_text, output_file='/home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/ALLnotebooks.txt'):
    with open(output_file, 'w', encoding='utf-8') as output:
        for entry in extracted_text:
            output.write(f"File: {entry['file_path']}\n")
            output.write(f"Text Content:\n{entry['text_content']}\n")
            output.write("=" * 50 + "\n")

if __name__ == "__main__":
    directory_path = "/home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator"
    # Change this to the directory path you want to search
    result = search_and_extract_text(directory_path)

    if result:
        write_to_all_notebooks_file(result)
        logging.info("All text content has been written to ALLnotebooks.txt")
    else:
        logging.warning("No *.ipynb files found in the specified directory.")


import glob
import nbformat
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def read_ipynb_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        try:
            notebook = nbformat.read(file, as_version=4)
            code_content = ""
            for cell in notebook['cells']:
                if cell['cell_type'] == 'code':
                    code_content += cell['source'] + '\n\n'
            return code_content
        except Exception as e:
            logging.error(f"Error reading {file_path}: {e}")
            return None

def search_and_extract_code(directory):
    ipynb_files = glob.glob(f"{directory}/*.ipynb")
    extracted_code = []

    for file_path in ipynb_files:
        logging.info(f"Processing file: {file_path}")
        code_content = read_ipynb_file(file_path)
        
        if code_content is not None:
            extracted_code.append({
                'file_path': file_path,
                'code_content': code_content
            })

    return extracted_code

def write_to_all_code_file(extracted_code, output_file='ALLcode.txt'):
    with open(output_file, 'w', encoding='utf-8') as output:
        for entry in extracted_code:
            output.write(f"File: {entry['file_path']}\n")
            output.write(f"Code Content:\n{entry['code_content']}\n")
            output.write("=" * 50 + "\n")

if __name__ == "__main__":
    directory_path = "/path/to/your/directory"  # Change this to the directory path you want to search
    result = search_and_extract_code(directory_path)

    if result:
        write_to_all_code_file(result)
        logging.info("All code content has been written to ALLcode.txt")
    else:
        logging.warning("No *.ipynb files found in the specified directory.")



==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled2.ipynb
Code Content:
!ls TODO

import sqlite3
conn = sqlite3.connect("TODO/products.db")
conn.text_factory = str
c = conn.cursor()
rows = c.execute("SELECT ROWID, * from products" )
for row in rows:
    print(row)


import sqlite3
conn = sqlite3.connect("code.db")
conn.text_factory = str
c = conn.cursor()
rows = c.execute("SELECT ROWID, * from snippets" )
for row in rows:
    print(row)


import sqlite3

    conn.text_factory = str
    c = conn.cursor()
    c.execute("CREATE VIRTUAL TABLE PROJECT using FTS4 (input)")
    conn.commit()
    text = "Database Created"
    return text



==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled1.ipynb
Code Content:
from PIL import Image
import glob
import random
import uuid
# Load the images you want to stitch
def stitched(IMAGE1,IMAGE2):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)

    # Get the dimensions of the images
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Determine the dimensions of the stitched image
    stitched_width = width1 + width2
    stitched_height = max(height1, height2)

    # Create a new blank image with the stitched dimensions
    stitched_image = Image.new('RGB', (stitched_width, stitched_height))

    # Paste the images onto the stitched image
    stitched_image.paste(image1, (0, 0))
    stitched_image.paste(image2, (width1, 0))

    # Save the stitched image
    filename = "static/images/giger/stitched"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)
    print (filename)
    return filename

IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
FileName = stitched(IMAGE1,IMAGE2)

!ls static/images/giger/stitchedcd793035-fac6-48cb-8065-dd7c3281c8a3.jpg

im = Image.open(FileName)
im

from PIL import Image

def blend_images(image1, image2, alpha):
    blended_image = Image.new('RGB', image1.size)

    for x in range(image1.width):
        for y in range(image1.height):
            pixel1 = image1.getpixel((x, y))
            pixel2 = image2.getpixel((x, y))

            blended_pixel = (
                int((1 - alpha) * pixel1[0] + alpha * pixel2[0]),
                int((1 - alpha) * pixel1[1] + alpha * pixel2[1]),
                int((1 - alpha) * pixel1[2] + alpha * pixel2[2])
            )

            blended_image.putpixel((x, y), blended_pixel)

    return blended_image



# Load the images you want to stitch
def stitched(IMAGE1,IMAGE2):

    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)

    # Adjust the blending factor (0.0 to 1.0)
    alpha = 0.5

    # Blend the images
    stitched_image = blend_images(image1, image2, alpha)

    # Save the blended image
    filename = "static/images/giger/stitched/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)
    return filename
IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
FileName = stitched(IMAGE1,IMAGE2)

im = Image.open(FileName)
im

from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,250):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)
    # Save the stitched image
    filename = "static/images/giger/stitched/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)



from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,250):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/stitched/long/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/stitched/long/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)

    # Save the stitched image
    filename = "static/images/giger/stitched/long/longest/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)





from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,2):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)

    # Save the stitched image
    filename = "static/images/giger/superlong"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)





!ls static/images/giger/superlong*

import numpy as np
from moviepy.editor import VideoClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/superlongd7c865c1-4663-4d56-a348-f0d2f77b553e.jpg"))
image_path = "static/images/giger/superlong3889669f-bc39-40b7-9518-6ec8eb9bb46c.jpg"
#image_clip = VideoFileClip(image_path, audio=False)

image = np.array(Image.open(image_path))

# Set parameters
output_size = (768, 512)  # Output video dimensions
#scroll_speed = 50  # Pixels per second
scroll_speed = 10  # Pixels per second
# Function to generate frames for the scrolling video
def make_frame(t):
    x_offset = int(t * scroll_speed)
    frame = image[:, x_offset:x_offset + output_size[0], :]
    return frame

# Create the scrolling video clip
duration = image.shape[1] / scroll_speed  # Total duration to scroll the entire image
scrolling_video = VideoClip(make_frame, duration=duration)

# Resize the video to the desired output size
scrolling_video = scrolling_video.resize(output_size)

# Save the scrolling video as scrolling_video.mp4
output_path = 'scrolling_videolongest11w.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=30)


!vlc scrolling_videolongest11w.mp4

from moviepy.editor import VideoClip, clips_array
from moviepy.video.io.VideoFileClip import VideoFileClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
image_clip = VideoFileClip(image_path, audio=False)

# Set the desired output dimensions (512x512)
output_width, output_height = 512, 512

# Calculate the scrolling duration based on your requirement (e.g., 4 minutes)
scrolling_duration = 4 * 60  # 4 minutes in seconds

# Function to create the scrolling effect
def scrolling_frame(t):
    x_offset = int((image_clip.w - output_width) * t / scrolling_duration)
    frame = image_clip.get_frame(t)[x_offset:x_offset + output_width, :, :]
    return frame

# Create the scrolling video clip
scrolling_video = VideoClip(scrolling_frame, duration=scrolling_duration)

# Resize the video to 512x512
scrolling_video = scrolling_video.resize(newsize=(output_width, output_height))

# Save the video as scrolling_video.mp4
output_path = 'scrolling_video.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=24)


import numpy as np
from moviepy.editor import VideoClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
#image_clip = VideoFileClip(image_path, audio=False)

image = np.array(Image.open(image_path))

# Set parameters
output_size = (512, 512)  # Output video dimensions
scroll_speed = 50  # Pixels per second

# Function to generate frames for the scrolling video
def make_frame(t):
    x_offset = int(t * scroll_speed)
    frame = image[:, x_offset:x_offset + output_size[0], :]
    return frame

# Create the scrolling video clip
duration = image.shape[1] / scroll_speed  # Total duration to scroll the entire image
scrolling_video = VideoClip(make_frame, duration=duration)

# Resize the video to the desired output size
scrolling_video = scrolling_video.resize(output_size)

# Save the scrolling video as scrolling_video.mp4
output_path = 'scrolling_video1.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=30)


!vlc scrolling_video1.mp4

image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/.jpg"))



stitched_image

!cp /home/jack/Desktop/StoryMaker/downloadz/00135.jpg start.jpg

from PIL import Image, ImageFilter

# Open the image
img = Image.open("/home/jack/Desktop/StoryMaker/downloadz/00135.jpg")

# Create a new image with the same size as the original
new_img = Image.new("RGBA", img.size)

# Paste the original image onto the new image
new_img.paste(img, (0, 0))

# Create a mask with a gradient from 255 to 0
mask = Image.new("L", img.size, 255)
for x in range(img.width - 50, img.width):
    alpha = int(255 * (x - (img.width - 50)) / 50)
    for y in range(img.height):
        mask.putpixel((x, y), alpha)

# Blur the mask
mask = mask.filter(ImageFilter.GaussianBlur(10))

# Apply the mask to the new image
new_img.putalpha(mask)

# Create a new mask with a gradient from 255 to 0 over the rightmost edge of the image
mask2 = Image.new("L", img.size, 255)
for y in range(img.height):
    alpha = int(255 * (y - (img.height - 1)) / (img.height - 1))
    mask2.putpixel((img.width - 1, y), alpha)

# Apply the new mask to the new image
new_img.putalpha(mask2)

# Save the new image as a PNG file
new_img.save("feather_transparent.png")


from PIL import Image, ImageDraw, ImageOps

def feather_image(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering
        mask = Image.new("L", input_image.size, 255)
        draw = ImageDraw.Draw(mask)
        for y in range(input_image.height):
            alpha = int(255 * (1 - (y - 450) / feather_width))
            draw.line([(0, y), (input_image.width, y)], fill=alpha)
        
        # Apply the mask to the output image
        output_image.putalpha(mask)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered.png"
    feather_width = 52
    feather_image(input_path, output_path, feather_width)


im = Image.open("feathered.png")
im

from PIL import Image, ImageDraw, ImageOps

def feather_image(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering
        mask = Image.new("L", input_image.size, 255)
        draw = ImageDraw.Draw(mask)
        for x in range(input_image.width - feather_width, input_image.width):
            alpha = int(255 * (1 - (x - (input_image.width - feather_width)) / feather_width))
            draw.line([(x, 0), (x, input_image.height)], fill=alpha)
        
        # Apply the mask to the output image
        output_image.putalpha(mask)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered_right.png"
    feather_width = 52
    feather_image(input_path, output_path, feather_width)


im = Image.open("feathered_right.png")
im

from PIL import Image, ImageDraw, ImageOps

def feather_left_side(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering the left side
        mask_left = Image.new("L", input_image.size, 255)
        #mask_left = Image.new("L", input_image.size), 255)
        #draw = ImageDraw.Draw(mask)
        draw_left = ImageDraw.Draw(mask_left)
        #for x in range(input_image.width - feather_width, input_image.width):
        #    alpha = int(255 * (1 - (x - (input_image.width - feather_width)) / feather_width))
        #    draw.line([(x, 0), (x, input_image.height)], fill=alpha)               
        for x in range(feather_width):
            alpha = int(255 * (1 - x / feather_width))
            draw_left.line([(x, 0), (x, input_image.height)], fill=alpha)
        
        # Apply the left mask to the output image
        output_image.paste((0, 0, 0, 0), (0, 0, feather_width, input_image.height))
        output_image.paste(ImageOps.colorize(mask_left, (0, 0, 0, 0), (0, 0, 0, 255)), (0, 0), mask_left)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Left side feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered_left.png"
    feather_width = 52
    feather_left_side(input_path, output_path, feather_width)


im = Image.open("feathered_left.png")
im

import numpy as np
import cv2
import matplotlib.pyplot as plt
import uuid
input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
giger = cv2.imread(input_path)
l_row, l_col, nb_channel = giger.shape
rows, cols = np.mgrid[:l_row, :l_col]
radius = np.sqrt((rows - l_row/2)**2 + (cols - l_col/2)**2)
alpha_channel = np.zeros((l_row, l_col))
#change .8 to .6
r_min, r_max = 1./3 * radius.max(), 0.6 * radius.max()
alpha_channel[radius < r_min] = 1
alpha_channel[radius > r_max] = 0
gradient_zone = np.logical_and(radius >= r_min, radius <= r_max)
alpha_channel[gradient_zone] = (r_max - radius[gradient_zone])/(r_max - r_min)
alpha_channel *= 255
feathered = np.empty((l_row, l_col, nb_channel + 1), dtype=np.uint8)
feathered[..., :3] = astro[:]
feathered[..., -1] = alpha_channel[:]

# Display the image without plot elements
plt.axis('off')
plt.imshow(feathered)
plt.show()

# Save the image without plot elements
output_path = "feathered_image.png"
plt.imsave(output_path, feathered[..., :3], format="png")




import numpy as np
from skimage import data
import cv2
input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
astro = cv2.imread(input_path)
l_row, l_col, nb_channel = astro.shape
rows, cols = np.mgrid[:l_row, :l_col]
radius = np.sqrt((rows - l_row/2)**2 + (cols - l_col/2)**2)
alpha_channel = np.zeros((l_row, l_col))
r_min, r_max = 1./3 * radius.max(), 0.8 * radius.max()
alpha_channel[radius < r_min] = 1
alpha_channel[radius > r_max] = 0
gradient_zone = np.logical_and(radius >= r_min, radius <= r_max)
alpha_channel[gradient_zone] = (r_max - radius[gradient_zone])/(r_max - r_min)
alpha_channel *= 255
feathered = np.empty((l_row, l_col, nb_channel + 1), dtype=np.uint8)
feathered[..., :3] = astro[:]
feathered[..., -1] = alpha_channel[:]

import matplotlib.pyplot as plt
plt.imshow(feathered)
plt.show()




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Create_Ipython_Kernel_of_your_Flask_env.ipynb
Code Content:
I asked ChatGPT:
    How do I install a my flask_env notebook kernel in my existing jupyter notebook

Sure, Jack! To install a Jupyter notebook kernel within your flask_env virtual environment so that you can use it in Jupyter Notebook, follow these steps:

    Activate the Virtual Environment:
    Open your terminal and navigate to the directory where your flask_env is located. Then, activate the virtual environment using the appropriate command. Since you're using Linux, the command should be:

Teminal command:
source flask_env/bin/activate

Install IPython:
IPython is required for Jupyter to function properly. While your virtual environment is active, install IPython using pip:

pip install ipython

I could kip this I already have jupyter notebook
if you install jupyter notebook in different envs it can break the install.
so, if you already jupyter notebook DO NOT:
pip install jupyter

 
if you already have jupyter notebook installed:
Within your activated virtual environment, install an ipykernel.
pip install ipykernel        
Now, within your activated virtual environment, install a Jupyter kernel that's associated with the virtual environment. This allows you to select the environment as a kernel in Jupyter Notebook:

ipython kernel install --user --name=flask_env

Start Jupyter Notebook:
    
Again ! If you have jupyter notebook already installed, deactivate this environment.
source deactivate.

Then Jupyter Notebook from the envionment it was originally installed by simply typing:
jupyter notebook

    Select the Flask Environment Kernel:
    When you create or open a notebook in Jupyter, you'll be able to choose the flask_env kernel from the kernel dropdown menu. This way, the notebook will run in the context of your virtual environment.

ONLY IF THIS IS THE ONLY ENVIRONMENT THAT HAS JUPYTER NOTEBOOK INSTALLED:
    Each time you want to use Jupyter Notebook with your flask_env environment, you need to activate the environment first using the source command from step 1.

If you encounter any issues during this process, feel free to ask for assistance, Jack. Happy coding!

Now I have A notebook Using my flask_env


==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/SQLITE3_database.ipynb
Code Content:
!ls TODO

import sqlite3
conn = sqlite3.connect("TODO/products.db")

import sqlite3
conn = sqlite3.connect("TODO/products.db")
conn.text_factory = str
c = conn.cursor()
rows = c.execute("SELECT ROWID, * from products" )
for row in rows:
    print(row)


import sqlite3
conn = sqlite3.connect("code.db")
conn.text_factory = str
c = conn.cursor()
rows = c.execute("SELECT ROWID, * from snippets" )
for row in rows:
    print(row)


import sqlite3

    conn.text_factory = str
    c = conn.cursor()
    c.execute("CREATE VIRTUAL TABLE PROJECT using FTS4 (input)")
    conn.commit()
    text = "Database Created"
    return text



==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Untitled4.ipynb
Code Content:
import g4f
from g4f.Provider import HuggingChat
dir (HuggingChat)

response = g4f.ChatCompletion.create(
    model=g4f.models.HuggingChat,
    messages=[{"role": "user", "content": "Hello"}],
    auth=True
)

import g4f
from g4f.Provider import (
    HuggingChat,
)
# Usage:
response = g4f.ChatCompletion.create(
    model=g4f.models.default,
    messages=[{"role": "user", "content": "Hello"}],
    provider=HuggingChat,
    #cookies=g4f.get_cookies(".google.com"),
    cookies={"cookie_name": "value", "cookie_name2": "value2"},
    auth=True
)

import g4f
from g4f.Provider import (
    Bard,
    Bing,
    HuggingChat,
    OpenAssistant,
    OpenaiChat,
)
# Usage:
response = g4f.ChatCompletion.create(
    model=g4f.models.default,
    messages=[{"role": "user", "content": "Hello"}],
    provider=Bard,
    #cookies=g4f.get_cookies(".google.com"),
    cookies={"cookie_name": "value", "cookie_name2": "value2"},
    auth=True
)

!pip freeze

import g4f, asyncio

async def run_async():
  _providers = [
      g4f.Provider.AItianhu,
      g4f.Provider.Acytoo,
      g4f.Provider.Aichat,
      g4f.Provider.Ails,
      g4f.Provider.Aivvm,
      g4f.Provider.ChatBase,
      g4f.Provider.ChatgptAi,
      g4f.Provider.ChatgptLogin,
      g4f.Provider.CodeLinkAva,
      g4f.Provider.DeepAi,
      g4f.Provider.Opchatgpts,
      g4f.Provider.Vercel,
      g4f.Provider.Vitalentum,
      g4f.Provider.Wewordle,
      g4f.Provider.Ylokh,
      g4f.Provider.You,
      g4f.Provider.Yqcloud,
  ]
  responses = [
      provider.create_async(
          model=g4f.models.default,
          messages=[{"role": "user", "content": "Hello"}],
      )
      for provider in _providers
  ]
  responses = await asyncio.gather(*responses)
  for idx, provider in enumerate(_providers):
      print(f"{provider.__name__}:", responses[idx])

asyncio.run(run_async())




==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/Writing_Text_On_Images.ipynb
Code Content:
import PIL
print(PIL.__version__)

'''['BytesIO',
 'FreeTypeFont',
 'Image',
 'ImageFont',
 'IntEnum',
 'Layout',
 'MAX_STRING_LENGTH',
 'TransposedFont',
 '__builtins__',
 '__cached__',
 '__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__spec__',
 '_string_length_check',
 'base64',
 'core',
 'is_directory',
 'is_path',
 'load',
 'load_default',
 'load_path',
 'os',
 'sys',
 'truetype',
 'warnings']
 '''

from PIL import ImageFont
dir (ImageFont)


['__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__setstate__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 'font_variant',
 'get_variation_axes',
 'get_variation_names',
 'getbbox',
 'getlength',
 'getmask',
 'getmask2',
 'getmetrics',
 'getname',
 'set_variation_by_axes',
 'set_variation_by_name']

dir (ImageFont.FreeTypeFont)




from PIL import ImageFont, ImageDraw, Image
from datetime import datetime

def mkmeme(IMAGE, FONT, txt):
    image = Image.open(IMAGE).convert("RGB")
    draw = ImageDraw.Draw(image)
    
    fontsize = 1
    img_fraction = 0.50
    font = ImageFont.truetype(FONT, fontsize)
    
    while font.getsize(txt)[0] < img_fraction * image.size[0]:
        fontsize += 1
        font = ImageFont.truetype(FONT, fontsize)
    
    fontsize -= 1
    font = ImageFont.truetype(FONT, fontsize)
    
    print('final font size', fontsize)
    draw.text((10, 25), txt, font=font)
    
    now = datetime.now()
    DIR = "static/MemImages/"
    FileName = now.strftime("%Y-%m-%d_%H:%M:%S") + ".jpg"
    image.save(DIR + FileName)
    
    return image

IMAGE = "/home/jack/Desktop/StoryMaker/static/images/abstract_beauty/Deliberate_11_abstract_beauty_centered_looking_at_the_c_002.jpg"
FONT = "/home/jack/fonts/Biryani-Bold.ttf"
txt = "Hello World"
im = mkmeme(IMAGE, FONT, txt)
im.show()




from PIL import ImageFont, ImageDraw, Image
from datetime import datetime
from textwrap import TextWrapper

def mkmeme(IMAGE, FONT_PATH, txt):
    image = Image.open(IMAGE)
    draw = ImageDraw.Draw(image)
    
    # Set up the text wrapper for calculating font size
    wrapper = TextWrapper(width=int(image.size[0] * 0.50))
    fontsize = 20
    
    while fontsize < 200:  # Adjust the upper limit as needed
        font = ImageFont.truetype(FONT_PATH, fontsize)
        wrapped_text = wrapper.wrap(txt)
        text_height = sum(font.getsize(line)[1] for line in wrapped_text)
        if text_height < image.size[1] * 0.8:  # Adjust the threshold as needed
            break
        fontsize += 1
    
    print('final font size', fontsize)
    
    # Calculate the Y-coordinate for centering the text vertically
    y_coord = (image.size[1] - text_height) // 2
    
    # Draw the wrapped text on the image
    for line in wrapped_text:
        width, height = font.getsize(line)
        draw.text(((image.size[0] - width) // 2, y_coord), line, font=font, fill=(255, 255, 255, 255))
        y_coord += height
    
    now = datetime.now()
    DIR = "static/MemImages/"
    FileName = now.strftime("%Y-%m-%d_%H:%M:%S") + "XX.jpg"
    image.save(DIR + FileName)  # save it
    return image

IMAGE = "/home/jack/Desktop/StoryMaker/static/images/abstract_beauty/Deliberate_11_abstract_beauty_centered_looking_at_the_c_002.jpg"
FONT_PATH = "/home/jack/fonts/Biryani-Bold.ttf"
txt = "Hello World. This is a longer text"
im = mkmeme(IMAGE, FONT_PATH, txt)
im.show()  # Show the generated image


from PIL import ImageFont, ImageDraw, Image
from datetime import datetime
from textwrap import TextWrapper

def mkmeme(IMAGE, FONT_PATH, txt):
    image = Image.open(IMAGE)
    draw = ImageDraw.Draw(image)
    
    # Set up the text wrapper for calculating font size
    wrapper = TextWrapper(width=int(image.size[0] * 0.50))
    fontsize = 60
    
    while fontsize < 200:  # Adjust the upper limit as needed
        font = ImageFont.truetype(FONT_PATH, fontsize)
        wrapped_text = wrapper.wrap(txt)
        text_height = sum(font.getsize(line)[1] for line in wrapped_text)
        if text_height < image.size[1] * 0.8:  # Adjust the threshold as needed
            break
        fontsize += 1
    
    print('final font size', fontsize)
    #draw.text((10, (image.size[1] - text_height) // 2), "\n".join(wrapped_text), font=font, fill=(255,255, 0, 255))
    
    print ("TEXT: ",txt)
    draw.text((50, 50), txt, font=font, fill=(255,255, 0, 255))    
    print("LOCATION: ",(image.size[1] - text_height) // 2)
    now = datetime.now()
    DIR = "static/MemImages/"
    FileName = now.strftime("%Y-%m-%d_%H:%M:%S") + "XX.jpg"
    image.save(DIR + FileName)  # save it
    print(DIR + FileName)
    return image

IMAGE = "/home/jack/Desktop/StoryMaker/static/images/abstract_beauty/Deliberate_11_abstract_beauty_centered_looking_at_the_c_002.jpg"
FONT_PATH = "/home/jack/fonts/Biryani-Bold.ttf"
txt = "Hello World. This is a longer text"
im = mkmeme(IMAGE, FONT_PATH, txt)
im.show()  # Show the generated image








from PIL import Image,ImageFont,ImageDraw
from datetime import datetime
def mkmem(mem_text,FNTdir,IMG):
    writeimg = Image.open(IMG).convert("RGB")
    newimg = Image.new("RGB", writeimg.size)
    newimg.paste(writeimg)
    width_image = newimg.size[0]
    height_image = newimg.size[1]
    
    for font_size in range(50, 0, -1):
        font = ImageFont.truetype(FNTdir+"Biryani-Bold.ttf", font_size)
        if font.getsize(meme_text)[0] <= width_image:
            print (font_size)
            break
    else:
        print ('no fonts fit!')   
    
    
    
    draw = ImageDraw.Draw(newimg)
    # font = ImageFont.truetype(<font-file>, <font-size>)
    font = ImageFont.truetype(FNTdir+"Biryani-Black.ttf", 20)
    # draw.text((x, y),"Sample Text",(r,g,b))
    draw.text((int(0.05*width_image), int(0.7*height_image)),meme_text,(255,255,255),font=font)
    # current dateTime
    now = datetime.now()
    # convert to string
    DIR ="static/MemImages/"
    FileName = now.strftime("%Y-%m-%d_%H:%M:%S")+".jpg"
    newimg.save(DIR+FileName)
    return newimg
meme_text = "Sometimes I think that you are out of this world, do you know why"
FNTdir = "/home/jack/fonts/"
IMG="/home/jack/Desktop/StoryMaker/static/images/abstract_beauty/Deliberate_11_abstract_beauty_centered_looking_at_the_c_002.jpg"
im = mkmem(meme_text,FNTdir,IMG)    
im

!ls /home/jack/fonts/Biryani-Bold.ttf

for font_size in range(50, 0, -1):
    font = ImageFont.truetype("impact.ttf", font_size)
    if font.getsize(meme_text)[0] <= width_image:
        break
else:
    print 'no fonts fit!'


from PIL import Image,ImageFont,ImageDraw
from datetime import datetime

# current dateTime
now = datetime.now()
# convert to string
FileName = now.strftime("%Y-%m-%d_%H:%M:%S")+".jpg"
print(date_time_str)

!ls /home/jack/fonts/

from PIL import ImageFont, ImageDraw, Image

font = ImageFont.truetype("/home/jack/fonts/DejaVuSans-Bold.ttf", 41)
draw = ImageDraw.Draw(Image.new("RGB", (1, 1)))  # Create a temporary image to create an ImageDraw object
width, height = draw.textsize("pharetra. purus", font=font)

print(width, height)


from PIL import ImageFont
font = ImageFont.truetype("/home/jack/fonts/DejaVuSans-Bold.ttf", 41)
width, height = font.getsize("pharetra. purus")
print(width, height)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[11], line 3
      1 from PIL import ImageFont
      2 font = ImageFont.truetype("/home/jack/fonts/DejaVuSans-Bold.ttf", 41)
----> 3 width, height = font.getsize("pharetra. purus")
      4 print(width, height)

AttributeError: 'FreeTypeFont' object has no attribute 'getsize'


from PIL import ImageFont

FNTdir = "/home/jack/fonts/"
meme_text = "When friends say you're out of this world"
font_size = 20

text_size = get_text_size(FNTdir + "Biryani-Black.ttf", meme_text, font_size)

font = ImageFont.truetype(fontPath, fontSize)
sz = font.getsize(text)


from PIL import ImageFont

def get_text_size(font_path, text, font_size):
    font = ImageFont.truetype(font_path, font_size)
    text_size = font.getsize(text)
    return text_size

FNTdir = "/home/jack/fonts/"
meme_text = "When friends say you're out of this world"
font_size = 20

text_size = get_text_size(FNTdir + "Biryani-Black.ttf", meme_text, font_size)

print("Text size:", text_size)


from PIL import Image,ImageFont,ImageDraw
from datetime import datetime
Image_file ="/home/jack/Desktop/StoryMaker/static/images/abstract_beauty/Deliberate_11_abstract_beauty_centered_looking_at_the_c_002.jpg"
max_frame_int = 1
meme_text = "When friends say your out of this world"
FNTdir = "/home/jack/fonts/"
for font_size in range(50, 0, -1):
    font = ImageFont.truetype(FNTdir+"Biryani-Bold.ttf", font_size)
    if font.getsize(meme_text)[0] <= width_image:
        break
else:
    print ('no fonts fit!')

for i in range(0,max_frame_int + 1):
    writeimg = Image.open(Image_file)
    newimg = Image.new("RGB", writeimg.size)
    newimg.paste(writeimg)
    width_image = newimg.size[0]
    height_image = newimg.size[1]
    draw = ImageDraw.Draw(newimg)
    # font = ImageFont.truetype(<font-file>, <font-size>)
    font = ImageFont.truetype(FNTdir+"Biryani-Black.ttf", 20)

    # draw.text((x, y),"Sample Text",(r,g,b))
    draw.text((int(0.05*width_image), int(0.7*height_image)),meme_text,(255,255,255),font=font)
    # current dateTime
    now = datetime.now()
    # convert to string
    DIR ="static/MemImages/"
    FileName = now.strftime("%Y-%m-%d_%H:%M:%S")+".jpg"
    newimg.save(DIR+FileName)
    newimg
    
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[4], line 9
      7 for font_size in range(50, 0, -1):
      8     font = ImageFont.truetype(FNTdir+"Biryani-Bold.ttf", font_size)
----> 9     if font.getsize(meme_text)[0] <= width_image:
     10         break
     11 else:

AttributeError: 'FreeTypeFont' object has no attribute 'getsize'
    

from PIL import Image,ImageFont,ImageDraw
from datetime import datetime

max_frame_int = 1
meme_text = "When friends say your out of this world"
FNTdir = "/home/jack/fonts/"
writeimg = Image.open("/home/jack/Desktop/StoryMaker/static/goddess/1080_2023-07-13-06:231689200625.jpg")
width_image =writeimg.size[1]
for font_size in range(50, 0, -1):
    font = ImageFont.truetype(FNTdir+"Biryani-Bold.ttf", font_size)
    if font.getsize(meme_text)[0] <= width_image:
        break
else:
    print ('no fonts fit!')

def mkmem(mem_text,FNTdir,IMG):
    writeimg = Image.open(IMG)
    newimg = Image.new("RGB", writeimg.size)
    newimg.paste(writeimg)
    width_image = newimg.size[0]
    height_image = newimg.size[1]
    draw = ImageDraw.Draw(newimg)
    # font = ImageFont.truetype(<font-file>, <font-size>)
    font = ImageFont.truetype(FNTdir+"Biryani-Black.ttf", 20)
    # draw.text((x, y),"Sample Text",(r,g,b))
    draw.text((int(0.05*width_image), int(0.7*height_image)),meme_text,(255,255,255),font=font)
    # current dateTime
    now = datetime.now()
    # convert to string
    DIR ="static/MemImages/"
    FileName = now.strftime("%Y-%m-%d_%H:%M:%S")+".jpg"
    newimg.save(DIR+FileName)
    return newimg
meme_text = "When friends say your out of this world"
FNTdir = "/home/jack/fonts/"
IMG="/home/jack/Desktop/StoryMaker/static/goddess/1080_2023-07-13-06:231689200625.jpg"
im = mkmem(meme_text,FNTdir,IMG)    
im

!ls DIR+FileName

import time
file = time.stfmt(%d%f%m)
print (file)

!ls /home/jack/fonts


















==================================================
File: /home/jack/Desktop/FlaskAppArchitect_Flask_App_Creator/stitch.ipynb
Code Content:
from PIL import Image
import glob
import random
import uuid
# Load the images you want to stitch
def stitched(IMAGE1,IMAGE2):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)

    # Get the dimensions of the images
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Determine the dimensions of the stitched image
    stitched_width = width1 + width2
    stitched_height = max(height1, height2)

    # Create a new blank image with the stitched dimensions
    stitched_image = Image.new('RGB', (stitched_width, stitched_height))

    # Paste the images onto the stitched image
    stitched_image.paste(image1, (0, 0))
    stitched_image.paste(image2, (width1, 0))

    # Save the stitched image
    filename = "static/images/giger/stitched"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)
    print (filename)
    return filename

IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
FileName = stitched(IMAGE1,IMAGE2)

!ls static/images/giger/stitchedcd793035-fac6-48cb-8065-dd7c3281c8a3.jpg

im = Image.open(FileName)
im

from PIL import Image

def blend_images(image1, image2, alpha):
    blended_image = Image.new('RGB', image1.size)

    for x in range(image1.width):
        for y in range(image1.height):
            pixel1 = image1.getpixel((x, y))
            pixel2 = image2.getpixel((x, y))

            blended_pixel = (
                int((1 - alpha) * pixel1[0] + alpha * pixel2[0]),
                int((1 - alpha) * pixel1[1] + alpha * pixel2[1]),
                int((1 - alpha) * pixel1[2] + alpha * pixel2[2])
            )

            blended_image.putpixel((x, y), blended_pixel)

    return blended_image



# Load the images you want to stitch
def stitched(IMAGE1,IMAGE2):

    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)

    # Adjust the blending factor (0.0 to 1.0)
    alpha = 0.5

    # Blend the images
    stitched_image = blend_images(image1, image2, alpha)

    # Save the blended image
    filename = "static/images/giger/stitched/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)
    return filename
IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
FileName = stitched(IMAGE1,IMAGE2)

im = Image.open(FileName)
im

from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,250):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)
    # Save the stitched image
    filename = "static/images/giger/stitched/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)



from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,250):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/stitched/long/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/stitched/long/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)

    # Save the stitched image
    filename = "static/images/giger/stitched/long/longest/"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)





from PIL import Image

def stitch_images(IMAGE1,IMAGE2, overlap):
    image1 = Image.open(IMAGE1)
    image2 = Image.open(IMAGE2)
    width1, height1 = image1.size
    width2, height2 = image2.size

    # Create a new canvas large enough to accommodate both images
    new_width = width1 + width2 - overlap
    new_height = max(height1, height2)
    stitched_image = Image.new('RGB', (new_width, new_height))

    # Paste the first image onto the canvas
    stitched_image.paste(image1, (0, 0))

    # Calculate the region to blend and blend the images
    blend_region = (width1 - overlap, 0, width1, new_height)
    blended_region = Image.blend(image1.crop(blend_region), image2.crop(blend_region), alpha=0.5)

    # Paste the blended region of the second image onto the canvas
    stitched_image.paste(blended_region, (width1 - overlap, 0))

    # Paste the remaining portion of the second image onto the canvas
    stitched_image.paste(image2.crop((overlap, 0, width2, new_height)), (width1, 0))

    return stitched_image

for i in range(0,2):# Load the images you want to stitch
    IMAGE1 = random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
    IMAGE2 = random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
    # Define the overlap width (adjust as needed)
    overlap = 50

    # Stitch the images
    stitched_image = stitch_images(IMAGE1,IMAGE2, overlap)
    print(stitched_image.size)

    # Save the stitched image
    filename = "static/images/giger/superlong"+str(uuid.uuid4()) + ".jpg"
    stitched_image.save(filename)





!ls static/images/giger/superlong*

import numpy as np
from moviepy.editor import VideoClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/superlongd7c865c1-4663-4d56-a348-f0d2f77b553e.jpg"))
image_path = "static/images/giger/superlong3889669f-bc39-40b7-9518-6ec8eb9bb46c.jpg"
#image_clip = VideoFileClip(image_path, audio=False)

image = np.array(Image.open(image_path))

# Set parameters
output_size = (768, 512)  # Output video dimensions
#scroll_speed = 50  # Pixels per second
scroll_speed = 10  # Pixels per second
# Function to generate frames for the scrolling video
def make_frame(t):
    x_offset = int(t * scroll_speed)
    frame = image[:, x_offset:x_offset + output_size[0], :]
    return frame

# Create the scrolling video clip
duration = image.shape[1] / scroll_speed  # Total duration to scroll the entire image
scrolling_video = VideoClip(make_frame, duration=duration)

# Resize the video to the desired output size
scrolling_video = scrolling_video.resize(output_size)

# Save the scrolling video as scrolling_video.mp4
output_path = 'scrolling_videolongest11w.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=30)


!vlc scrolling_videolongest11w.mp4

from moviepy.editor import VideoClip, clips_array
from moviepy.video.io.VideoFileClip import VideoFileClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
image_clip = VideoFileClip(image_path, audio=False)

# Set the desired output dimensions (512x512)
output_width, output_height = 512, 512

# Calculate the scrolling duration based on your requirement (e.g., 4 minutes)
scrolling_duration = 4 * 60  # 4 minutes in seconds

# Function to create the scrolling effect
def scrolling_frame(t):
    x_offset = int((image_clip.w - output_width) * t / scrolling_duration)
    frame = image_clip.get_frame(t)[x_offset:x_offset + output_width, :, :]
    return frame

# Create the scrolling video clip
scrolling_video = VideoClip(scrolling_frame, duration=scrolling_duration)

# Resize the video to 512x512
scrolling_video = scrolling_video.resize(newsize=(output_width, output_height))

# Save the video as scrolling_video.mp4
output_path = 'scrolling_video.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=24)


import numpy as np
from moviepy.editor import VideoClip

# Load the wide image
image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/*.jpg"))
#image_clip = VideoFileClip(image_path, audio=False)

image = np.array(Image.open(image_path))

# Set parameters
output_size = (512, 512)  # Output video dimensions
scroll_speed = 50  # Pixels per second

# Function to generate frames for the scrolling video
def make_frame(t):
    x_offset = int(t * scroll_speed)
    frame = image[:, x_offset:x_offset + output_size[0], :]
    return frame

# Create the scrolling video clip
duration = image.shape[1] / scroll_speed  # Total duration to scroll the entire image
scrolling_video = VideoClip(make_frame, duration=duration)

# Resize the video to the desired output size
scrolling_video = scrolling_video.resize(output_size)

# Save the scrolling video as scrolling_video.mp4
output_path = 'scrolling_video1.mp4'
scrolling_video.write_videofile(output_path, codec='libx264', fps=30)


!vlc scrolling_video1.mp4

image_path =  random.choice(glob.glob("static/images/giger/stitched/long/longest/.jpg"))



stitched_image

!cp /home/jack/Desktop/StoryMaker/downloadz/00135.jpg start.jpg

from PIL import Image, ImageFilter

# Open the image
img = Image.open("/home/jack/Desktop/StoryMaker/downloadz/00135.jpg")

# Create a new image with the same size as the original
new_img = Image.new("RGBA", img.size)

# Paste the original image onto the new image
new_img.paste(img, (0, 0))

# Create a mask with a gradient from 255 to 0
mask = Image.new("L", img.size, 255)
for x in range(img.width - 50, img.width):
    alpha = int(255 * (x - (img.width - 50)) / 50)
    for y in range(img.height):
        mask.putpixel((x, y), alpha)

# Blur the mask
mask = mask.filter(ImageFilter.GaussianBlur(10))

# Apply the mask to the new image
new_img.putalpha(mask)

# Create a new mask with a gradient from 255 to 0 over the rightmost edge of the image
mask2 = Image.new("L", img.size, 255)
for y in range(img.height):
    alpha = int(255 * (y - (img.height - 1)) / (img.height - 1))
    mask2.putpixel((img.width - 1, y), alpha)

# Apply the new mask to the new image
new_img.putalpha(mask2)

# Save the new image as a PNG file
new_img.save("feather_transparent.png")


from PIL import Image, ImageDraw, ImageOps

def feather_image(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering
        mask = Image.new("L", input_image.size, 255)
        draw = ImageDraw.Draw(mask)
        for y in range(input_image.height):
            alpha = int(255 * (1 - (y - 450) / feather_width))
            draw.line([(0, y), (input_image.width, y)], fill=alpha)
        
        # Apply the mask to the output image
        output_image.putalpha(mask)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered.png"
    feather_width = 52
    feather_image(input_path, output_path, feather_width)


im = Image.open("feathered.png")
im

from PIL import Image, ImageDraw, ImageOps

def feather_image(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering
        mask = Image.new("L", input_image.size, 255)
        draw = ImageDraw.Draw(mask)
        for x in range(input_image.width - feather_width, input_image.width):
            alpha = int(255 * (1 - (x - (input_image.width - feather_width)) / feather_width))
            draw.line([(x, 0), (x, input_image.height)], fill=alpha)
        
        # Apply the mask to the output image
        output_image.putalpha(mask)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered_right.png"
    feather_width = 52
    feather_image(input_path, output_path, feather_width)


im = Image.open("feathered_right.png")
im

from PIL import Image, ImageDraw, ImageOps

def feather_left_side(input_path, output_path, feather_width):
    try:
        # Open the input image
        input_image = Image.open(input_path)
        
        # Create a new image with transparency
        output_image = Image.new("RGBA", input_image.size, (0, 0, 0, 0))
        
        # Copy the original image onto the new image
        output_image.paste(input_image, (0, 0))
        
        # Create a gradient mask for feathering the left side
        mask_left = Image.new("L", input_image.size, 255)
        #mask_left = Image.new("L", input_image.size), 255)
        #draw = ImageDraw.Draw(mask)
        draw_left = ImageDraw.Draw(mask_left)
        #for x in range(input_image.width - feather_width, input_image.width):
        #    alpha = int(255 * (1 - (x - (input_image.width - feather_width)) / feather_width))
        #    draw.line([(x, 0), (x, input_image.height)], fill=alpha)               
        for x in range(feather_width):
            alpha = int(255 * (1 - x / feather_width))
            draw_left.line([(x, 0), (x, input_image.height)], fill=alpha)
        
        # Apply the left mask to the output image
        output_image.paste((0, 0, 0, 0), (0, 0, feather_width, input_image.height))
        output_image.paste(ImageOps.colorize(mask_left, (0, 0, 0, 0), (0, 0, 0, 255)), (0, 0), mask_left)
        
        # Save the feathered image
        output_image.save(output_path)
        
        print("Left side feathering complete. Check the output image:", output_path)
        
    except Exception as e:
        print("An error occurred:", str(e))

if __name__ == "__main__":
    input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
    output_path = "feathered_left.png"
    feather_width = 52
    feather_left_side(input_path, output_path, feather_width)


im = Image.open("feathered_left.png")
im

import numpy as np
import cv2
import matplotlib.pyplot as plt

input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
giger = cv2.imread(input_path)
l_row, l_col, nb_channel = giger.shape
rows, cols = np.mgrid[:l_row, :l_col]
radius = np.sqrt((rows - l_row/2)**2 + (cols - l_col/2)**2)
alpha_channel = np.zeros((l_row, l_col))
#change .8 to .6
r_min, r_max = 1./3 * radius.max(), 0.6 * radius.max()
alpha_channel[radius < r_min] = 1
alpha_channel[radius > r_max] = 0
gradient_zone = np.logical_and(radius >= r_min, radius <= r_max)
alpha_channel[gradient_zone] = (r_max - radius[gradient_zone])/(r_max - r_min)
alpha_channel *= 255
feathered = np.empty((l_row, l_col, nb_channel + 1), dtype=np.uint8)
feathered[..., :3] = astro[:]
feathered[..., -1] = alpha_channel[:]

# Display the image without plot elements
plt.axis('off')
plt.imshow(feathered)
plt.show()

# Save the image without plot elements
output_path = "feathered_image.png"
#plt.imsave(output_path, feathered[..., :3], format="png")
plt.imsave(output_path, feathered, format="png")

im = Image.open("feathered_image.png")
im

import numpy as np
from skimage import data
import cv2
input_path = "/home/jack/Desktop/StoryMaker/downloadz/00135.jpg"
astro = cv2.imread(input_path)
l_row, l_col, nb_channel = astro.shape
rows, cols = np.mgrid[:l_row, :l_col]
radius = np.sqrt((rows - l_row/2)**2 + (cols - l_col/2)**2)
alpha_channel = np.zeros((l_row, l_col))
r_min, r_max = 1./3 * radius.max(), 0.8 * radius.max()
alpha_channel[radius < r_min] = 1
alpha_channel[radius > r_max] = 0
gradient_zone = np.logical_and(radius >= r_min, radius <= r_max)
alpha_channel[gradient_zone] = (r_max - radius[gradient_zone])/(r_max - r_min)
alpha_channel *= 255
feathered = np.empty((l_row, l_col, nb_channel + 1), dtype=np.uint8)
feathered[..., :3] = astro[:]
feathered[..., -1] = alpha_channel[:]

import matplotlib.pyplot as plt
plt.imshow(feathered)
plt.show()




==================================================
